I"º<p>goåœ¨ç¨‹åºå¯åŠ¨æ—¶ä¼šåˆ†é…ä¸€å—è™šæ‹Ÿå†…å­˜åœ°å€æ˜¯è¿ç»­çš„å†…å­˜, ç»“æ„å¦‚ä¸‹:
spans 512M  bitmap 16G arena  512G
è¿™ä¸€å—å†…å­˜åˆ†ä¸ºäº†3ä¸ªåŒºåŸŸ, åœ¨X64ä¸Šå¤§å°åˆ†åˆ«æ˜¯512M, 16Gå’Œ512G, å®ƒä»¬çš„ä½œç”¨å¦‚ä¸‹:</p>

<p>arena</p>

<p>arenaåŒºåŸŸå°±æ˜¯æˆ‘ä»¬é€šå¸¸è¯´çš„heap, goä»heapåˆ†é…çš„å†…å­˜éƒ½åœ¨è¿™ä¸ªåŒºåŸŸä¸­.</p>

<p>bitmap</p>

<p>bitmapåŒºåŸŸç”¨äºè¡¨ç¤ºarenaåŒºåŸŸä¸­å“ªäº›åœ°å€ä¿å­˜äº†å¯¹è±¡, å¹¶ä¸”å¯¹è±¡ä¸­å“ªäº›åœ°å€åŒ…å«äº†æŒ‡é’ˆ.
bitmapåŒºåŸŸä¸­ä¸€ä¸ªbyte(8 bit)å¯¹åº”äº†arenaåŒºåŸŸä¸­çš„å››ä¸ªæŒ‡é’ˆå¤§å°çš„å†…å­˜, ä¹Ÿå°±æ˜¯2 bitå¯¹åº”ä¸€ä¸ªæŒ‡é’ˆå¤§å°çš„å†…å­˜.
æ‰€ä»¥bitmapåŒºåŸŸçš„å¤§å°æ˜¯ 512GB / æŒ‡é’ˆå¤§å°(8 byte) / 4 = 16GB.</p>

<p>bitmapåŒºåŸŸä¸­çš„ä¸€ä¸ªbyteå¯¹åº”arenaåŒºåŸŸçš„å››ä¸ªæŒ‡é’ˆå¤§å°çš„å†…å­˜çš„ç»“æ„å¦‚ä¸‹,
æ¯ä¸€ä¸ªæŒ‡é’ˆå¤§å°çš„å†…å­˜éƒ½ä¼šæœ‰ä¸¤ä¸ªbitåˆ†åˆ«è¡¨ç¤ºæ˜¯å¦åº”è¯¥ç»§ç»­æ‰«æå’Œæ˜¯å¦åŒ…å«æŒ‡é’ˆ:
	<img src="https://xiazemin.github.io/MyBlog/img/bitmap.png" />
	bitmapä¸­çš„byteå’Œarenaçš„å¯¹åº”å…³ç³»ä»æœ«å°¾å¼€å§‹, ä¹Ÿå°±æ˜¯éšç€å†…å­˜åˆ†é…ä¼šå‘ä¸¤è¾¹æ‰©å±•:
		<img src="https://xiazemin.github.io/MyBlog/img/bitmap_arena.png" />
<!-- more -->
spans</p>

<p>spansåŒºåŸŸç”¨äºè¡¨ç¤ºarenaåŒºä¸­çš„æŸä¸€é¡µ(Page)å±äºå“ªä¸ªspan, ä»€ä¹ˆæ˜¯spanå°†åœ¨ä¸‹é¢ä»‹ç».
spansåŒºåŸŸä¸­ä¸€ä¸ªæŒ‡é’ˆ(8 byte)å¯¹åº”äº†arenaåŒºåŸŸä¸­çš„ä¸€é¡µ(åœ¨goä¸­ä¸€é¡µ=8KB).
æ‰€ä»¥spansçš„å¤§å°æ˜¯ 512GB / é¡µå¤§å°(8KB) * æŒ‡é’ˆå¤§å°(8 byte) = 512MB.</p>

<p>spansåŒºåŸŸçš„ä¸€ä¸ªæŒ‡é’ˆå¯¹åº”arenaåŒºåŸŸçš„ä¸€é¡µçš„ç»“æ„å¦‚ä¸‹, å’Œbitmapä¸ä¸€æ ·çš„æ˜¯å¯¹åº”å…³ç³»ä¼šä»å¼€å¤´å¼€å§‹:
<img src="https://xiazemin.github.io/MyBlog/img/span_arena.png" />
ä»€ä¹ˆæ—¶å€™ä»Heapåˆ†é…å¯¹è±¡
å¾ˆå¤šè®²è§£goçš„æ–‡ç« å’Œä¹¦ç±ä¸­éƒ½æåˆ°è¿‡, goä¼šè‡ªåŠ¨ç¡®å®šå“ªäº›å¯¹è±¡åº”è¯¥æ”¾åœ¨æ ˆä¸Š, å“ªäº›å¯¹è±¡åº”è¯¥æ”¾åœ¨å †ä¸Š.
ç®€å•çš„æ¥è¯´, å½“ä¸€ä¸ªå¯¹è±¡çš„å†…å®¹å¯èƒ½åœ¨ç”Ÿæˆè¯¥å¯¹è±¡çš„å‡½æ•°ç»“æŸåè¢«è®¿é—®, é‚£ä¹ˆè¿™ä¸ªå¯¹è±¡å°±ä¼šåˆ†é…åœ¨å †ä¸Š.
åœ¨å †ä¸Šåˆ†é…å¯¹è±¡çš„æƒ…å†µåŒ…æ‹¬:</p>

<p>è¿”å›å¯¹è±¡çš„æŒ‡é’ˆ
ä¼ é€’äº†å¯¹è±¡çš„æŒ‡é’ˆåˆ°å…¶ä»–å‡½æ•°
åœ¨é—­åŒ…ä¸­ä½¿ç”¨äº†å¯¹è±¡å¹¶ä¸”éœ€è¦ä¿®æ”¹å¯¹è±¡
ä½¿ç”¨new
åœ¨Cè¯­è¨€ä¸­å‡½æ•°è¿”å›åœ¨æ ˆä¸Šçš„å¯¹è±¡çš„æŒ‡é’ˆæ˜¯éå¸¸å±é™©çš„äº‹æƒ…, ä½†åœ¨goä¸­å´æ˜¯å®‰å…¨çš„, å› ä¸ºè¿™ä¸ªå¯¹è±¡ä¼šè‡ªåŠ¨åœ¨å †ä¸Šåˆ†é….
goå†³å®šæ˜¯å¦ä½¿ç”¨å †åˆ†é…å¯¹è±¡çš„è¿‡ç¨‹ä¹Ÿå«â€é€ƒé€¸åˆ†æâ€.</p>

<p>GC Bitmap
GCåœ¨æ ‡è®°æ—¶éœ€è¦çŸ¥é“å“ªäº›åœ°æ–¹åŒ…å«äº†æŒ‡é’ˆ, ä¾‹å¦‚ä¸Šé¢æåˆ°çš„bitmapåŒºåŸŸæ¶µç›–äº†arenaåŒºåŸŸä¸­çš„æŒ‡é’ˆä¿¡æ¯.
é™¤æ­¤ä¹‹å¤–, GCè¿˜éœ€è¦çŸ¥é“æ ˆç©ºé—´ä¸Šå“ªäº›åœ°æ–¹åŒ…å«äº†æŒ‡é’ˆ,
å› ä¸ºæ ˆç©ºé—´ä¸å±äºarenaåŒºåŸŸ, æ ˆç©ºé—´çš„æŒ‡é’ˆä¿¡æ¯å°†ä¼šåœ¨å‡½æ•°ä¿¡æ¯é‡Œé¢.
å¦å¤–, GCåœ¨åˆ†é…å¯¹è±¡æ—¶ä¹Ÿéœ€è¦æ ¹æ®å¯¹è±¡çš„ç±»å‹è®¾ç½®bitmapåŒºåŸŸ, æ¥æºçš„æŒ‡é’ˆä¿¡æ¯å°†ä¼šåœ¨ç±»å‹ä¿¡æ¯é‡Œé¢.</p>

<p>æ€»ç»“èµ·æ¥goä¸­æœ‰ä»¥ä¸‹çš„GC Bitmap:</p>

<p>bitmapåŒºåŸŸ: æ¶µç›–äº†arenaåŒºåŸŸ, ä½¿ç”¨2 bitè¡¨ç¤ºä¸€ä¸ªæŒ‡é’ˆå¤§å°çš„å†…å­˜
å‡½æ•°ä¿¡æ¯: æ¶µç›–äº†å‡½æ•°çš„æ ˆç©ºé—´, ä½¿ç”¨1 bitè¡¨ç¤ºä¸€ä¸ªæŒ‡é’ˆå¤§å°çš„å†…å­˜ (ä½äºstackmap.bytedata)
ç±»å‹ä¿¡æ¯: åœ¨åˆ†é…å¯¹è±¡æ—¶ä¼šå¤åˆ¶åˆ°bitmapåŒºåŸŸ, ä½¿ç”¨1 bitè¡¨ç¤ºä¸€ä¸ªæŒ‡é’ˆå¤§å°çš„å†…å­˜ (ä½äº_type.gcdata)
Span
spanæ˜¯ç”¨äºåˆ†é…å¯¹è±¡çš„åŒºå—, ä¸‹å›¾æ˜¯ç®€å•è¯´æ˜äº†Spançš„å†…éƒ¨ç»“æ„:
<img src="https://xiazemin.github.io/MyBlog/img/span_bit.png" />
é€šå¸¸ä¸€ä¸ªspanåŒ…å«äº†å¤šä¸ªå¤§å°ç›¸åŒçš„å…ƒç´ , ä¸€ä¸ªå…ƒç´ ä¼šä¿å­˜ä¸€ä¸ªå¯¹è±¡, é™¤é:</p>

<p>spanç”¨äºä¿å­˜å¤§å¯¹è±¡, è¿™ç§æƒ…å†µspanåªæœ‰ä¸€ä¸ªå…ƒç´ 
spanç”¨äºä¿å­˜æå°å¯¹è±¡ä¸”ä¸åŒ…å«æŒ‡é’ˆçš„å¯¹è±¡(tiny object), è¿™ç§æƒ…å†µspanä¼šç”¨ä¸€ä¸ªå…ƒç´ ä¿å­˜å¤šä¸ªå¯¹è±¡
spanä¸­æœ‰ä¸€ä¸ªfreeindexæ ‡è®°ä¸‹ä¸€æ¬¡åˆ†é…å¯¹è±¡æ—¶åº”è¯¥å¼€å§‹æœç´¢çš„åœ°å€, åˆ†é…åfreeindexä¼šå¢åŠ ,
åœ¨freeindexä¹‹å‰çš„å…ƒç´ éƒ½æ˜¯å·²åˆ†é…çš„, åœ¨freeindexä¹‹åçš„å…ƒç´ æœ‰å¯èƒ½å·²åˆ†é…, ä¹Ÿæœ‰å¯èƒ½æœªåˆ†é….</p>

<p>spanæ¯æ¬¡GCä»¥åéƒ½å¯èƒ½ä¼šå›æ”¶æ‰ä¸€äº›å…ƒç´ , allocBitsç”¨äºæ ‡è®°å“ªäº›å…ƒç´ æ˜¯å·²åˆ†é…çš„, å“ªäº›å…ƒç´ æ˜¯æœªåˆ†é…çš„.
ä½¿ç”¨freeindex + allocBitså¯ä»¥åœ¨åˆ†é…æ—¶è·³è¿‡å·²åˆ†é…çš„å…ƒç´ , æŠŠå¯¹è±¡è®¾ç½®åœ¨æœªåˆ†é…çš„å…ƒç´ ä¸­,
ä½†å› ä¸ºæ¯æ¬¡éƒ½å»è®¿é—®allocBitsæ•ˆç‡ä¼šæ¯”è¾ƒæ…¢, spanä¸­æœ‰ä¸€ä¸ªæ•´æ•°å‹çš„allocCacheç”¨äºç¼“å­˜freeindexå¼€å§‹çš„bitmap, ç¼“å­˜çš„bitå€¼ä¸åŸå€¼ç›¸å.</p>

<p>gcmarkBitsç”¨äºåœ¨gcæ—¶æ ‡è®°å“ªäº›å¯¹è±¡å­˜æ´», æ¯æ¬¡gcä»¥ågcmarkBitsä¼šå˜ä¸ºallocBits.
éœ€è¦æ³¨æ„çš„æ˜¯spanç»“æ„æœ¬èº«çš„å†…å­˜æ˜¯ä»ç³»ç»Ÿåˆ†é…çš„, ä¸Šé¢æåˆ°çš„spansåŒºåŸŸå’ŒbitmapåŒºåŸŸéƒ½åªæ˜¯ä¸€ä¸ªç´¢å¼•.</p>

<p>Spançš„ç±»å‹
spanæ ¹æ®å¤§å°å¯ä»¥åˆ†ä¸º67ä¸ªç±»å‹, å¦‚ä¸‹:</p>

<p>// class  bytes/obj  bytes/span  objects  tail waste  max waste
//     1          8        8192     1024           0     87.50%
//     2         16        8192      512           0     43.75%
//     3         32        8192      256           0     46.88%
//     4         48        8192      170          32     31.52%
//     5         64        8192      128           0     23.44%
//     6         80        8192      102          32     19.07%
//     7         96        8192       85          32     15.95%
//     8        112        8192       73          16     13.56%
//     9        128        8192       64           0     11.72%
//    10        144        8192       56         128     11.82%
//    11        160        8192       51          32      9.73%
//    12        176        8192       46          96      9.59%
//    13        192        8192       42         128      9.25%
//    14        208        8192       39          80      8.12%
//    15        224        8192       36         128      8.15%
//    16        240        8192       34          32      6.62%
//    17        256        8192       32           0      5.86%
//    18        288        8192       28         128     12.16%
//    19        320        8192       25         192     11.80%
//    20        352        8192       23          96      9.88%
//    21        384        8192       21         128      9.51%
//    22        416        8192       19         288     10.71%
//    23        448        8192       18         128      8.37%
//    24        480        8192       17          32      6.82%
//    25        512        8192       16           0      6.05%
//    26        576        8192       14         128     12.33%
//    27        640        8192       12         512     15.48%
//    28        704        8192       11         448     13.93%
//    29        768        8192       10         512     13.94%
//    30        896        8192        9         128     15.52%
//    31       1024        8192        8           0     12.40%
//    32       1152        8192        7         128     12.41%
//    33       1280        8192        6         512     15.55%
//    34       1408       16384       11         896     14.00%
//    35       1536        8192        5         512     14.00%
//    36       1792       16384        9         256     15.57%
//    37       2048        8192        4           0     12.45%
//    38       2304       16384        7         256     12.46%
//    39       2688        8192        3         128     15.59%
//    40       3072       24576        8           0     12.47%
//    41       3200       16384        5         384      6.22%
//    42       3456       24576        7         384      8.83%
//    43       4096        8192        2           0     15.60%
//    44       4864       24576        5         256     16.65%
//    45       5376       16384        3         256     10.92%
//    46       6144       24576        4           0     12.48%
//    47       6528       32768        5         128      6.23%
//    48       6784       40960        6         256      4.36%
//    49       6912       49152        7         768      3.37%
//    50       8192        8192        1           0     15.61%
//    51       9472       57344        6         512     14.28%
//    52       9728       49152        5         512      3.64%
//    53      10240       40960        4           0      4.99%
//    54      10880       32768        3         128      6.24%
//    55      12288       24576        2           0     11.45%
//    56      13568       40960        3         256      9.99%
//    57      14336       57344        4           0      5.35%
//    58      16384       16384        1           0     12.49%
//    59      18432       73728        4           0     11.11%
//    60      19072       57344        3         128      3.57%
//    61      20480       40960        2           0      6.87%
//    62      21760       65536        3         256      6.25%
//    63      24576       24576        1           0     11.45%
//    64      27264       81920        3         128     10.00%
//    65      28672       57344        2           0      4.91%
//    66      32768       32768        1           0     12.50%
ä»¥ç±»å‹(class)ä¸º1çš„spanä¸ºä¾‹,
spanä¸­çš„å…ƒç´ å¤§å°æ˜¯8 byte, spanæœ¬èº«å 1é¡µä¹Ÿå°±æ˜¯8K, ä¸€å…±å¯ä»¥ä¿å­˜1024ä¸ªå¯¹è±¡.</p>

<p>åœ¨åˆ†é…å¯¹è±¡æ—¶, ä¼šæ ¹æ®å¯¹è±¡çš„å¤§å°å†³å®šä½¿ç”¨ä»€ä¹ˆç±»å‹çš„span,
ä¾‹å¦‚16 byteçš„å¯¹è±¡ä¼šä½¿ç”¨span 2, 17 byteçš„å¯¹è±¡ä¼šä½¿ç”¨span 3, 32 byteçš„å¯¹è±¡ä¼šä½¿ç”¨span 3.
ä»è¿™ä¸ªä¾‹å­ä¹Ÿå¯ä»¥çœ‹åˆ°, åˆ†é…17å’Œ32 byteçš„å¯¹è±¡éƒ½ä¼šä½¿ç”¨span 3, ä¹Ÿå°±æ˜¯è¯´éƒ¨åˆ†å¤§å°çš„å¯¹è±¡åœ¨åˆ†é…æ—¶ä¼šæµªè´¹ä¸€å®šçš„ç©ºé—´.</p>

<p>æœ‰äººå¯èƒ½ä¼šæ³¨æ„åˆ°, ä¸Šé¢æœ€å¤§çš„spançš„å…ƒç´ å¤§å°æ˜¯32K, é‚£ä¹ˆåˆ†é…è¶…è¿‡32Kçš„å¯¹è±¡ä¼šåœ¨å“ªé‡Œåˆ†é…å‘¢?
è¶…è¿‡32Kçš„å¯¹è±¡ç§°ä¸ºâ€å¤§å¯¹è±¡â€, åˆ†é…å¤§å¯¹è±¡æ—¶, ä¼šç›´æ¥ä»heapåˆ†é…ä¸€ä¸ªç‰¹æ®Šçš„span,
è¿™ä¸ªç‰¹æ®Šçš„spançš„ç±»å‹(class)æ˜¯0, åªåŒ…å«äº†ä¸€ä¸ªå¤§å¯¹è±¡, spançš„å¤§å°ç”±å¯¹è±¡çš„å¤§å°å†³å®š.</p>

<p>ç‰¹æ®Šçš„spanåŠ ä¸Šçš„66ä¸ªæ ‡å‡†çš„span, ä¸€å…±ç»„æˆäº†67ä¸ªspanç±»å‹.</p>

<p>Spançš„ä½ç½®
åœ¨å‰ä¸€ç¯‡ä¸­æˆ‘æåˆ°äº†Pæ˜¯ä¸€ä¸ªè™šæ‹Ÿçš„èµ„æº, åŒä¸€æ—¶é—´åªèƒ½æœ‰ä¸€ä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ªP, æ‰€ä»¥Pä¸­çš„æ•°æ®ä¸éœ€è¦é”.
ä¸ºäº†åˆ†é…å¯¹è±¡æ—¶æœ‰æ›´å¥½çš„æ€§èƒ½, å„ä¸ªPä¸­éƒ½æœ‰spançš„ç¼“å­˜(ä¹Ÿå«mcache), ç¼“å­˜çš„ç»“æ„å¦‚ä¸‹:
<img src="https://xiazemin.github.io/MyBlog/img/span_p.png" />
å„ä¸ªPä¸­æŒ‰spanç±»å‹çš„ä¸åŒ, æœ‰67*2=134ä¸ªspançš„ç¼“å­˜,</p>

<p>å…¶ä¸­scanå’Œnoscançš„åŒºåˆ«åœ¨äº,
å¦‚æœå¯¹è±¡åŒ…å«äº†æŒ‡é’ˆ, åˆ†é…å¯¹è±¡æ—¶ä¼šä½¿ç”¨scançš„span,
å¦‚æœå¯¹è±¡ä¸åŒ…å«æŒ‡é’ˆ, åˆ†é…å¯¹è±¡æ—¶ä¼šä½¿ç”¨noscançš„span.
æŠŠspanåˆ†ä¸ºscanå’Œnoscançš„æ„ä¹‰åœ¨äº,
GCæ‰«æå¯¹è±¡çš„æ—¶å€™å¯¹äºnoscançš„spanå¯ä»¥ä¸å»æŸ¥çœ‹bitmapåŒºåŸŸæ¥æ ‡è®°å­å¯¹è±¡, è¿™æ ·å¯ä»¥å¤§å¹…æå‡æ ‡è®°çš„æ•ˆç‡.</p>

<p>åœ¨åˆ†é…å¯¹è±¡æ—¶å°†ä¼šä»ä»¥ä¸‹çš„ä½ç½®è·å–é€‚åˆçš„spanç”¨äºåˆ†é…:</p>

<p>é¦–å…ˆä»Pçš„ç¼“å­˜(mcache)è·å–, å¦‚æœæœ‰ç¼“å­˜çš„spanå¹¶ä¸”æœªæ»¡åˆ™ä½¿ç”¨, è¿™ä¸ªæ­¥éª¤ä¸éœ€è¦é”
ç„¶åä»å…¨å±€ç¼“å­˜(mcentral)è·å–, å¦‚æœè·å–æˆåŠŸåˆ™è®¾ç½®åˆ°P, è¿™ä¸ªæ­¥éª¤éœ€è¦é”
æœ€åä»mheapè·å–, è·å–åè®¾ç½®åˆ°å…¨å±€ç¼“å­˜, è¿™ä¸ªæ­¥éª¤éœ€è¦é”
åœ¨Pä¸­ç¼“å­˜spançš„åšæ³•è·ŸCoreCLRä¸­çº¿ç¨‹ç¼“å­˜åˆ†é…ä¸Šä¸‹æ–‡(Allocation Context)çš„åšæ³•ç›¸ä¼¼,
éƒ½å¯ä»¥è®©åˆ†é…å¯¹è±¡æ—¶å¤§éƒ¨åˆ†æ—¶å€™ä¸éœ€è¦çº¿ç¨‹é”, æ”¹è¿›åˆ†é…çš„æ€§èƒ½.</p>

<p>åˆ†é…å¯¹è±¡çš„å¤„ç†
åˆ†é…å¯¹è±¡çš„æµç¨‹
goä»å †åˆ†é…å¯¹è±¡æ—¶ä¼šè°ƒç”¨newobjectå‡½æ•°, è¿™ä¸ªå‡½æ•°çš„æµç¨‹å¤§è‡´å¦‚ä¸‹:
<img src="https://xiazemin.github.io/MyBlog/img/span_malloc.png" />
é¦–å…ˆä¼šæ£€æŸ¥GCæ˜¯å¦åœ¨å·¥ä½œä¸­, å¦‚æœGCåœ¨å·¥ä½œä¸­å¹¶ä¸”å½“å‰çš„Gåˆ†é…äº†ä¸€å®šå¤§å°çš„å†…å­˜åˆ™éœ€è¦ååŠ©GCåšä¸€å®šçš„å·¥ä½œ,
è¿™ä¸ªæœºåˆ¶å«GC Assist, ç”¨äºé˜²æ­¢åˆ†é…å†…å­˜å¤ªå¿«å¯¼è‡´GCå›æ”¶è·Ÿä¸ä¸Šçš„æƒ…å†µå‘ç”Ÿ.</p>

<p>ä¹‹åä¼šåˆ¤æ–­æ˜¯å°å¯¹è±¡è¿˜æ˜¯å¤§å¯¹è±¡, å¦‚æœæ˜¯å¤§å¯¹è±¡åˆ™ç›´æ¥è°ƒç”¨largeAllocä»å †ä¸­åˆ†é…,
å¦‚æœæ˜¯å°å¯¹è±¡åˆ†3ä¸ªé˜¶æ®µè·å–å¯ç”¨çš„span, ç„¶åä»spanä¸­åˆ†é…å¯¹è±¡:</p>

<p>é¦–å…ˆä»Pçš„ç¼“å­˜(mcache)è·å–
ç„¶åä»å…¨å±€ç¼“å­˜(mcentral)è·å–, å…¨å±€ç¼“å­˜ä¸­æœ‰å¯ç”¨çš„spançš„åˆ—è¡¨
æœ€åä»mheapè·å–, mheapä¸­ä¹Ÿæœ‰spançš„è‡ªç”±åˆ—è¡¨, å¦‚æœéƒ½è·å–å¤±è´¥åˆ™ä»arenaåŒºåŸŸåˆ†é…
è¿™ä¸‰ä¸ªé˜¶æ®µçš„è¯¦ç»†ç»“æ„å¦‚ä¸‹å›¾:
<img src="https://xiazemin.github.io/MyBlog/img/mcentral.png" />
æ•°æ®ç±»å‹çš„å®šä¹‰
åˆ†é…å¯¹è±¡æ¶‰åŠçš„æ•°æ®ç±»å‹åŒ…å«:</p>

<p>p: å‰ä¸€ç¯‡æåˆ°è¿‡, Pæ˜¯åç¨‹ä¸­çš„ç”¨äºè¿è¡Œgoä»£ç çš„è™šæ‹Ÿèµ„æº
m: å‰ä¸€ç¯‡æåˆ°è¿‡, Mç›®å‰ä»£è¡¨ç³»ç»Ÿçº¿ç¨‹
g: å‰ä¸€ç¯‡æåˆ°è¿‡, Gå°±æ˜¯goroutine
mspan: ç”¨äºåˆ†é…å¯¹è±¡çš„åŒºå—
mcentral: å…¨å±€çš„mspanç¼“å­˜, ä¸€å…±æœ‰67*2=134ä¸ª
mheap: ç”¨äºç®¡ç†heapçš„å¯¹è±¡, å…¨å±€åªæœ‰ä¸€ä¸ª</p>

<p>æºä»£ç åˆ†æ
goä»å †åˆ†é…å¯¹è±¡æ—¶ä¼šè°ƒç”¨newobjectå‡½æ•°, å…ˆä»è¿™ä¸ªå‡½æ•°çœ‹èµ·:</p>

<p>// implementation of new builtin
// compiler (both frontend and SSA backend) knows the signature
// of this function
func newobject(typ *_type) unsafe.Pointer {
    return mallocgc(typ.size, typ, true)
}
newobjectè°ƒç”¨äº†mallocgcå‡½æ•°:</p>

<p>// Allocate an object of size bytes.
// Small objects are allocated from the per-P cacheâ€™s free lists.
// Large objects (&gt; 32 kB) are allocated straight from the heap.
func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {
    if gcphase == _GCmarktermination {
        throw(â€œmallocgc called with gcphase == _GCmarkterminationâ€)
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if size == 0 {
    return unsafe.Pointer(&amp;zerobase)
}

if debug.sbrk != 0 {
    align := uintptr(16)
    if typ != nil {
        align = uintptr(typ.align)
    }
    return persistentalloc(size, align, &amp;memstats.other_sys)
}

// åˆ¤æ–­æ˜¯å¦è¦è¾…åŠ©GCå·¥ä½œ
// gcBlackenEnabledåœ¨GCçš„æ ‡è®°é˜¶æ®µä¼šå¼€å¯
// assistG is the G to charge for this allocation, or nil if
// GC is not currently active.
var assistG *g
if gcBlackenEnabled != 0 {
    // Charge the current user G for this allocation.
    assistG = getg()
    if assistG.m.curg != nil {
        assistG = assistG.m.curg
    }
    // Charge the allocation against the G. We'll account
    // for internal fragmentation at the end of mallocgc.
    assistG.gcAssistBytes -= int64(size)

    // ä¼šæŒ‰åˆ†é…çš„å¤§å°åˆ¤æ–­éœ€è¦ååŠ©GCå®Œæˆå¤šå°‘å·¥ä½œ
    // å…·ä½“çš„ç®—æ³•å°†åœ¨ä¸‹é¢è®²è§£æ”¶é›†å™¨æ—¶è¯´æ˜
    if assistG.gcAssistBytes &lt; 0 {
        // This G is in debt. Assist the GC to correct
        // this before allocating. This must happen
        // before disabling preemption.
        gcAssistAlloc(assistG)
    }
}

// å¢åŠ å½“å‰Gå¯¹åº”çš„Mçš„lockè®¡æ•°, é˜²æ­¢è¿™ä¸ªGè¢«æŠ¢å 
// Set mp.mallocing to keep from being preempted by GC.
mp := acquirem()
if mp.mallocing != 0 {
    throw("malloc deadlock")
}
if mp.gsignal == getg() {
    throw("malloc during signal")
}
mp.mallocing = 1

shouldhelpgc := false
dataSize := size
// è·å–å½“å‰Gå¯¹åº”çš„Må¯¹åº”çš„Pçš„æœ¬åœ°spanç¼“å­˜(mcache)
// å› ä¸ºMåœ¨æ‹¥æœ‰Påä¼šæŠŠPçš„mcacheè®¾åˆ°Mä¸­, è¿™é‡Œè¿”å›çš„æ˜¯getg().m.mcache
c := gomcache()
var x unsafe.Pointer
noscan := typ == nil || typ.kind&amp;kindNoPointers != 0
// åˆ¤æ–­æ˜¯å¦å°å¯¹è±¡, maxSmallSizeå½“å‰çš„å€¼æ˜¯32K
if size &lt;= maxSmallSize {
    // å¦‚æœå¯¹è±¡ä¸åŒ…å«æŒ‡é’ˆ, å¹¶ä¸”å¯¹è±¡çš„å¤§å°å°äº16 bytes, å¯ä»¥åšç‰¹æ®Šå¤„ç†
    // è¿™é‡Œæ˜¯é’ˆå¯¹éå¸¸å°çš„å¯¹è±¡çš„ä¼˜åŒ–, å› ä¸ºspançš„å…ƒç´ æœ€å°åªèƒ½æ˜¯8 byte, å¦‚æœå¯¹è±¡æ›´å°é‚£ä¹ˆå¾ˆå¤šç©ºé—´éƒ½ä¼šè¢«æµªè´¹æ‰
    // éå¸¸å°çš„å¯¹è±¡å¯ä»¥æ•´åˆåœ¨"class 2 noscan"çš„å…ƒç´ (å¤§å°ä¸º16 byte)ä¸­
    if noscan &amp;&amp; size &lt; maxTinySize {
        // Tiny allocator.
        //
        // Tiny allocator combines several tiny allocation requests
        // into a single memory block. The resulting memory block
        // is freed when all subobjects are unreachable. The subobjects
        // must be noscan (don't have pointers), this ensures that
        // the amount of potentially wasted memory is bounded.
        //
        // Size of the memory block used for combining (maxTinySize) is tunable.
        // Current setting is 16 bytes, which relates to 2x worst case memory
        // wastage (when all but one subobjects are unreachable).
        // 8 bytes would result in no wastage at all, but provides less
        // opportunities for combining.
        // 32 bytes provides more opportunities for combining,
        // but can lead to 4x worst case wastage.
        // The best case winning is 8x regardless of block size.
        //
        // Objects obtained from tiny allocator must not be freed explicitly.
        // So when an object will be freed explicitly, we ensure that
        // its size &gt;= maxTinySize.
        //
        // SetFinalizer has a special case for objects potentially coming
        // from tiny allocator, it such case it allows to set finalizers
        // for an inner byte of a memory block.
        //
        // The main targets of tiny allocator are small strings and
        // standalone escaping variables. On a json benchmark
        // the allocator reduces number of allocations by ~12% and
        // reduces heap size by ~20%.
        off := c.tinyoffset
        // Align tiny pointer for required (conservative) alignment.
        if size&amp;7 == 0 {
            off = round(off, 8)
        } else if size&amp;3 == 0 {
            off = round(off, 4)
        } else if size&amp;1 == 0 {
            off = round(off, 2)
        }
        if off+size &lt;= maxTinySize &amp;&amp; c.tiny != 0 {
            // The object fits into existing tiny block.
            x = unsafe.Pointer(c.tiny + off)
            c.tinyoffset = off + size
            c.local_tinyallocs++
            mp.mallocing = 0
            releasem(mp)
            return x
        }
        // Allocate a new maxTinySize block.
        span := c.alloc[tinySpanClass]
        v := nextFreeFast(span)
        if v == 0 {
            v, _, shouldhelpgc = c.nextFree(tinySpanClass)
        }
        x = unsafe.Pointer(v)
        (*[2]uint64)(x)[0] = 0
        (*[2]uint64)(x)[1] = 0
        // See if we need to replace the existing tiny block with the new one
        // based on amount of remaining free space.
        if size &lt; c.tinyoffset || c.tiny == 0 {
            c.tiny = uintptr(x)
            c.tinyoffset = size
        }
        size = maxTinySize
    } else {
        // å¦åˆ™æŒ‰æ™®é€šçš„å°å¯¹è±¡åˆ†é…
        // é¦–å…ˆè·å–å¯¹è±¡çš„å¤§å°åº”è¯¥ä½¿ç”¨å“ªä¸ªspanç±»å‹
        var sizeclass uint8
        if size &lt;= smallSizeMax-8 {
            sizeclass = size_to_class8[(size+smallSizeDiv-1)/smallSizeDiv]
        } else {
            sizeclass = size_to_class128[(size-smallSizeMax+largeSizeDiv-1)/largeSizeDiv]
        }
        size = uintptr(class_to_size[sizeclass])
        // ç­‰äºsizeclass * 2 + (noscan ? 1 : 0)
        spc := makeSpanClass(sizeclass, noscan)
        span := c.alloc[spc]
        // å°è¯•å¿«é€Ÿçš„ä»è¿™ä¸ªspanä¸­åˆ†é…
        v := nextFreeFast(span)
        if v == 0 {
            // åˆ†é…å¤±è´¥, å¯èƒ½éœ€è¦ä»mcentralæˆ–è€…mheapä¸­è·å–
            // å¦‚æœä»mcentralæˆ–è€…mheapè·å–äº†æ–°çš„span, åˆ™shouldhelpgcä¼šç­‰äºtrue
            // shouldhelpgcä¼šç­‰äºtrueæ—¶ä¼šåœ¨ä¸‹é¢åˆ¤æ–­æ˜¯å¦è¦è§¦å‘GC
            v, span, shouldhelpgc = c.nextFree(spc)
        }
        x = unsafe.Pointer(v)
        if needzero &amp;&amp; span.needzero != 0 {
            memclrNoHeapPointers(unsafe.Pointer(v), size)
        }
    }
} else {
    // å¤§å¯¹è±¡ç›´æ¥ä»mheapåˆ†é…, è¿™é‡Œçš„sæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„span, å®ƒçš„classæ˜¯0
    var s *mspan
    shouldhelpgc = true
    systemstack(func() {
        s = largeAlloc(size, needzero, noscan)
    })
    s.freeindex = 1
    s.allocCount = 1
    x = unsafe.Pointer(s.base())
    size = s.elemsize
}

// è®¾ç½®arenaå¯¹åº”çš„bitmap, è®°å½•å“ªäº›ä½ç½®åŒ…å«äº†æŒ‡é’ˆ, GCä¼šä½¿ç”¨bitmapæ‰«ææ‰€æœ‰å¯åˆ°è¾¾çš„å¯¹è±¡
var scanSize uintptr
if !noscan {
    // If allocating a defer+arg block, now that we've picked a malloc size
    // large enough to hold everything, cut the "asked for" size down to
    // just the defer header, so that the GC bitmap will record the arg block
    // as containing nothing at all (as if it were unused space at the end of
    // a malloc block caused by size rounding).
    // The defer arg areas are scanned as part of scanstack.
    if typ == deferType {
        dataSize = unsafe.Sizeof(_defer{})
    }
    // è¿™ä¸ªå‡½æ•°éå¸¸çš„é•¿, æœ‰å…´è¶£çš„å¯ä»¥çœ‹
    // https://github.com/golang/go/blob/go1.9.2/src/runtime/mbitmap.go#L855
    // è™½ç„¶ä»£ç å¾ˆé•¿ä½†æ˜¯è®¾ç½®çš„å†…å®¹è·Ÿä¸Šé¢è¯´è¿‡çš„bitmapåŒºåŸŸçš„ç»“æ„ä¸€æ ·
    // æ ¹æ®ç±»å‹ä¿¡æ¯è®¾ç½®scan bitè·Ÿpointer bit, scan bitæˆç«‹è¡¨ç¤ºåº”è¯¥ç»§ç»­æ‰«æ, pointer bitæˆç«‹è¡¨ç¤ºè¯¥ä½ç½®æ˜¯æŒ‡é’ˆ
    // éœ€è¦æ³¨æ„çš„åœ°æ–¹æœ‰
    // - å¦‚æœä¸€ä¸ªç±»å‹åªæœ‰å¼€å¤´çš„åœ°æ–¹åŒ…å«æŒ‡é’ˆ, ä¾‹å¦‚[ptr, ptr, large non-pointer data]
    //   é‚£ä¹ˆåé¢çš„éƒ¨åˆ†çš„scan bitå°†ä¼šä¸º0, è¿™æ ·å¯ä»¥å¤§å¹…æå‡æ ‡è®°çš„æ•ˆç‡
    // - ç¬¬äºŒä¸ªslotçš„scan bitç”¨é€”æ¯”è¾ƒç‰¹æ®Š, å®ƒå¹¶ä¸ç”¨äºæ ‡è®°æ˜¯å¦ç»§ç»­scan, è€Œæ˜¯æ ‡è®°checkmark
    // ä»€ä¹ˆæ˜¯checkmark
    // - å› ä¸ºgoçš„å¹¶è¡ŒGCæ¯”è¾ƒå¤æ‚, ä¸ºäº†æ£€æŸ¥å®ç°æ˜¯å¦æ­£ç¡®, goéœ€è¦åœ¨æœ‰ä¸€ä¸ªæ£€æŸ¥æ‰€æœ‰åº”è¯¥è¢«æ ‡è®°çš„å¯¹è±¡æ˜¯å¦è¢«æ ‡è®°çš„æœºåˆ¶
    //   è¿™ä¸ªæœºåˆ¶å°±æ˜¯checkmark, åœ¨å¼€å¯checkmarkæ—¶goä¼šåœ¨æ ‡è®°é˜¶æ®µçš„æœ€ååœæ­¢æ•´ä¸ªä¸–ç•Œç„¶åé‡æ–°æ‰§è¡Œä¸€æ¬¡æ ‡è®°
    //   ä¸Šé¢çš„ç¬¬äºŒä¸ªslotçš„scan bitå°±æ˜¯ç”¨äºæ ‡è®°å¯¹è±¡åœ¨checkmarkæ ‡è®°ä¸­æ˜¯å¦è¢«æ ‡è®°çš„
    // - æœ‰çš„äººå¯èƒ½ä¼šå‘ç°ç¬¬äºŒä¸ªslotè¦æ±‚å¯¹è±¡æœ€å°‘æœ‰ä¸¤ä¸ªæŒ‡é’ˆçš„å¤§å°, é‚£ä¹ˆåªæœ‰ä¸€ä¸ªæŒ‡é’ˆçš„å¤§å°çš„å¯¹è±¡å‘¢?
    //   åªæœ‰ä¸€ä¸ªæŒ‡é’ˆçš„å¤§å°çš„å¯¹è±¡å¯ä»¥åˆ†ä¸ºä¸¤ç§æƒ…å†µ
    //   å¯¹è±¡å°±æ˜¯æŒ‡é’ˆ, å› ä¸ºå¤§å°åˆšå¥½æ˜¯1ä¸ªæŒ‡é’ˆæ‰€ä»¥å¹¶ä¸éœ€è¦çœ‹bitmapåŒºåŸŸ, è¿™æ—¶ç¬¬ä¸€ä¸ªslotå°±æ˜¯checkmark
    //   å¯¹è±¡ä¸æ˜¯æŒ‡é’ˆ, å› ä¸ºæœ‰tiny allocçš„æœºåˆ¶, ä¸æ˜¯æŒ‡é’ˆä¸”åªæœ‰ä¸€ä¸ªæŒ‡é’ˆå¤§å°çš„å¯¹è±¡ä¼šåˆ†é…åœ¨ä¸¤ä¸ªæŒ‡é’ˆçš„spanä¸­
    //               è¿™æ—¶å€™ä¹Ÿä¸éœ€è¦çœ‹bitmapåŒºåŸŸ, æ‰€ä»¥å’Œä¸Šé¢ä¸€æ ·ç¬¬ä¸€ä¸ªslotå°±æ˜¯checkmark
    heapBitsSetType(uintptr(x), size, dataSize, typ)
    if dataSize &gt; typ.size {
        // Array allocation. If there are any
        // pointers, GC has to scan to the last
        // element.
        if typ.ptrdata != 0 {
            scanSize = dataSize - typ.size + typ.ptrdata
        }
    } else {
        scanSize = typ.ptrdata
    }
    c.local_scan += scanSize
}

// å†…å­˜å±éšœ, å› ä¸ºx86å’Œx64çš„storeä¸ä¼šä¹±åºæ‰€ä»¥è¿™é‡Œåªæ˜¯ä¸ªé’ˆå¯¹ç¼–è¯‘å™¨çš„å±éšœ, æ±‡ç¼–ä¸­æ˜¯ret
// Ensure that the stores above that initialize x to
// type-safe memory and set the heap bits occur before
// the caller can make x observable to the garbage
// collector. Otherwise, on weakly ordered machines,
// the garbage collector could follow a pointer to x,
// but see uninitialized memory or stale heap bits.
publicationBarrier()

// å¦‚æœå½“å‰åœ¨GCä¸­, éœ€è¦ç«‹åˆ»æ ‡è®°åˆ†é…åçš„å¯¹è±¡ä¸º"é»‘è‰²", é˜²æ­¢å®ƒè¢«å›æ”¶
// Allocate black during GC.
// All slots hold nil so no scanning is needed.
// This may be racing with GC so do it atomically if there can be
// a race marking the bit.
if gcphase != _GCoff {
    gcmarknewobject(uintptr(x), size, scanSize)
}

// Race Detectorçš„å¤„ç†(ç”¨äºæ£€æµ‹çº¿ç¨‹å†²çªé—®é¢˜)
if raceenabled {
    racemalloc(x, size)
}

// Memory Sanitizerçš„å¤„ç†(ç”¨äºæ£€æµ‹å±é™©æŒ‡é’ˆç­‰å†…å­˜é—®é¢˜)
if msanenabled {
    msanmalloc(x, size)
}

// é‡æ–°å…è®¸å½“å‰çš„Gè¢«æŠ¢å 
mp.mallocing = 0
releasem(mp)

// é™¤é”™è®°å½•
if debug.allocfreetrace != 0 {
    tracealloc(x, size, typ)
}

// Profilerè®°å½•
if rate := MemProfileRate; rate &gt; 0 {
    if size &lt; uintptr(rate) &amp;&amp; int32(size) &lt; c.next_sample {
        c.next_sample -= int32(size)
    } else {
        mp := acquirem()
        profilealloc(mp, x, size)
        releasem(mp)
    }
}

// gcAssistByteså‡å»"å®é™…åˆ†é…å¤§å° - è¦æ±‚åˆ†é…å¤§å°", è°ƒæ•´åˆ°å‡†ç¡®å€¼
if assistG != nil {
    // Account for internal fragmentation in the assist
    // debt now that we know it.
    assistG.gcAssistBytes -= int64(size - dataSize)
}

// å¦‚æœä¹‹å‰è·å–äº†æ–°çš„span, åˆ™åˆ¤æ–­æ˜¯å¦éœ€è¦åå°å¯åŠ¨GC
// è¿™é‡Œçš„åˆ¤æ–­é€»è¾‘(gcTrigger)ä¼šåœ¨ä¸‹é¢è¯¦ç»†è¯´æ˜
if shouldhelpgc {
    if t := (gcTrigger{kind: gcTriggerHeap}); t.test() {
        gcStart(gcBackgroundMode, t)
    }
}

return x } æ¥ä¸‹æ¥çœ‹çœ‹å¦‚ä½•ä»spané‡Œé¢åˆ†é…å¯¹è±¡, é¦–å…ˆä¼šè°ƒç”¨nextFreeFastå°è¯•å¿«é€Ÿåˆ†é…:
</code></pre></div></div>

<p>// nextFreeFast returns the next free object if one is quickly available.
// Otherwise it returns 0.
func nextFreeFast(s <em>mspan) gclinkptr {
    // è·å–ç¬¬ä¸€ä¸ªé0çš„bitæ˜¯ç¬¬å‡ ä¸ªbit, ä¹Ÿå°±æ˜¯å“ªä¸ªå…ƒç´ æ˜¯æœªåˆ†é…çš„
    theBit := sys.Ctz64(s.allocCache) // Is there a free object in the allocCache?
    // æ‰¾åˆ°æœªåˆ†é…çš„å…ƒç´ 
    if theBit &lt; 64 {
        result := s.freeindex + uintptr(theBit)
        // è¦æ±‚ç´¢å¼•å€¼å°äºå…ƒç´ æ•°é‡
        if result &lt; s.nelems {
            // ä¸‹ä¸€ä¸ªfreeindex
            freeidx := result + 1
            // å¯ä»¥è¢«64æ•´é™¤æ—¶éœ€è¦ç‰¹æ®Šå¤„ç†(å‚è€ƒnextFree)
            if freeidx%64 == 0 &amp;&amp; freeidx != s.nelems {
                return 0
            }
            // æ›´æ–°freeindexå’ŒallocCache(é«˜ä½éƒ½æ˜¯0, ç”¨å°½ä»¥åä¼šæ›´æ–°)
            s.allocCacheÂ Â»= uint(theBit + 1)
            s.freeindex = freeidx
            // è¿”å›å…ƒç´ æ‰€åœ¨çš„åœ°å€
            v := gclinkptr(result</em>s.elemsize + s.base())
            // æ·»åŠ å·²åˆ†é…çš„å…ƒç´ è®¡æ•°
            s.allocCount++
            return v
        }
    }
    return 0
}
å¦‚æœåœ¨freeindexåæ— æ³•å¿«é€Ÿæ‰¾åˆ°æœªåˆ†é…çš„å…ƒç´ , å°±éœ€è¦è°ƒç”¨nextFreeåšå‡ºæ›´å¤æ‚çš„å¤„ç†:</p>

<p>// nextFree returns the next free object from the cached span if one is available.
// Otherwise it refills the cache with a span with an available object and
// returns that object along with a flag indicating that this was a heavy
// weight allocation. If it is a heavy weight allocation the caller must
// determine whether a new GC cycle needs to be started or if the GC is active
// whether this goroutine needs to assist the GC.
func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool) {
    // æ‰¾åˆ°ä¸‹ä¸€ä¸ªfreeindexå’Œæ›´æ–°allocCache
    s = c.alloc[spc]
    shouldhelpgc = false
    freeIndex := s.nextFreeIndex()
    // å¦‚æœspané‡Œé¢æ‰€æœ‰å…ƒç´ éƒ½å·²åˆ†é…, åˆ™éœ€è¦è·å–æ–°çš„span
    if freeIndex == s.nelems {
        // The span is full.
        if uintptr(s.allocCount) != s.nelems {
            println(â€œruntime: s.allocCount=â€, s.allocCount, â€œs.nelems=â€, s.nelems)
            throw(â€œs.allocCount != s.nelems &amp;&amp; freeIndex == s.nelemsâ€)
        }
        // ç”³è¯·æ–°çš„span
        systemstack(func() {
            c.refill(spc)
        })
        // è·å–ç”³è¯·åçš„æ–°çš„span, å¹¶è®¾ç½®éœ€è¦æ£€æŸ¥æ˜¯å¦æ‰§è¡ŒGC
        shouldhelpgc = true
        s = c.alloc[spc]</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    freeIndex = s.nextFreeIndex()
}

if freeIndex &gt;= s.nelems {
    throw("freeIndex is not valid")
}

// è¿”å›å…ƒç´ æ‰€åœ¨çš„åœ°å€
v = gclinkptr(freeIndex*s.elemsize + s.base())
// æ·»åŠ å·²åˆ†é…çš„å…ƒç´ è®¡æ•°
s.allocCount++
if uintptr(s.allocCount) &gt; s.nelems {
    println("s.allocCount=", s.allocCount, "s.nelems=", s.nelems)
    throw("s.allocCount &gt; s.nelems")
}
return } å¦‚æœmcacheä¸­æŒ‡å®šç±»å‹çš„spanå·²æ»¡, å°±éœ€è¦è°ƒç”¨refillå‡½æ•°ç”³è¯·æ–°çš„span:
</code></pre></div></div>

<p>// Gets a span that has a free object in it and assigns it
// to be the cached span for the given sizeclass. Returns this span.
func (c *mcache) refill(spc spanClass) *mspan {
    <em>g</em> := getg()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// é˜²æ­¢Gè¢«æŠ¢å 
_g_.m.locks++
// Return the current cached span to the central lists.
s := c.alloc[spc]

// ç¡®ä¿å½“å‰çš„spanæ‰€æœ‰å…ƒç´ éƒ½å·²åˆ†é…
if uintptr(s.allocCount) != s.nelems {
    throw("refill of span with free space remaining")
}

// è®¾ç½®spançš„incacheå±æ€§, é™¤éæ˜¯å…¨å±€ä½¿ç”¨çš„ç©ºspan(ä¹Ÿå°±æ˜¯mcacheé‡Œé¢spanæŒ‡é’ˆçš„é»˜è®¤å€¼)
if s != &amp;emptymspan {
    s.incache = false
}

// å‘mcentralç”³è¯·ä¸€ä¸ªæ–°çš„span
// Get a new cached span from the central lists.
s = mheap_.central[spc].mcentral.cacheSpan()
if s == nil {
    throw("out of memory")
}

if uintptr(s.allocCount) == s.nelems {
    throw("span has no free space")
}

// è®¾ç½®æ–°çš„spanåˆ°mcacheä¸­
c.alloc[spc] = s
// å…è®¸Gè¢«æŠ¢å 
_g_.m.locks--
return s } å‘mcentralç”³è¯·ä¸€ä¸ªæ–°çš„spanä¼šé€šè¿‡cacheSpanå‡½æ•°: mcentralé¦–å…ˆå°è¯•ä»å†…éƒ¨çš„é“¾è¡¨å¤ç”¨åŸæœ‰çš„span, å¦‚æœå¤ç”¨å¤±è´¥åˆ™å‘mheapç”³è¯·.
</code></pre></div></div>

<p>// Allocate a span to use in an MCache.
func (c *mcentral) cacheSpan() *mspan {
    // è®©å½“å‰GååŠ©ä¸€éƒ¨åˆ†çš„sweepå·¥ä½œ
    // Deduct credit for this span allocation and sweep if necessary.
    spanBytes := uintptr(class_to_allocnpages[c.spanclass.sizeclass()]) * _PageSize
    deductSweepCredit(spanBytes, 0)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// å¯¹mcentralä¸Šé”, å› ä¸ºå¯èƒ½ä¼šæœ‰å¤šä¸ªM(P)åŒæ—¶è®¿é—®
lock(&amp;c.lock)
traceDone := false
if trace.enabled {
    traceGCSweepStart()
}
sg := mheap_.sweepgen retry:
// mcentralé‡Œé¢æœ‰ä¸¤ä¸ªspançš„é“¾è¡¨
// - nonemptyè¡¨ç¤ºç¡®å®šè¯¥spanæœ€å°‘æœ‰ä¸€ä¸ªæœªåˆ†é…çš„å…ƒç´ 
// - emptyè¡¨ç¤ºä¸ç¡®å®šè¯¥spanæœ€å°‘æœ‰ä¸€ä¸ªæœªåˆ†é…çš„å…ƒç´ 
// è¿™é‡Œä¼˜å…ˆæŸ¥æ‰¾nonemptyçš„é“¾è¡¨
// sweepgenæ¯æ¬¡GCéƒ½ä¼šå¢åŠ 2
// - sweepgen == å…¨å±€sweepgen, è¡¨ç¤ºspanå·²ç»sweepè¿‡
// - sweepgen == å…¨å±€sweepgen-1, è¡¨ç¤ºspanæ­£åœ¨sweep
// - sweepgen == å…¨å±€sweepgen-2, è¡¨ç¤ºspanç­‰å¾…sweep
var s *mspan
for s = c.nonempty.first; s != nil; s = s.next {
    // å¦‚æœspanç­‰å¾…sweep, å°è¯•åŸå­ä¿®æ”¹sweepgenä¸ºå…¨å±€sweepgen-1
    if s.sweepgen == sg-2 &amp;&amp; atomic.Cas(&amp;s.sweepgen, sg-2, sg-1) {
        // ä¿®æ”¹æˆåŠŸåˆ™æŠŠspanç§»åˆ°emptyé“¾è¡¨, sweepå®ƒç„¶åè·³åˆ°havespan
        c.nonempty.remove(s)
        c.empty.insertBack(s)
        unlock(&amp;c.lock)
        s.sweep(true)
        goto havespan
    }
    // å¦‚æœè¿™ä¸ªspanæ­£åœ¨è¢«å…¶ä»–çº¿ç¨‹sweep, å°±è·³è¿‡
    if s.sweepgen == sg-1 {
        // the span is being swept by background sweeper, skip
        continue
    }
    // spanå·²ç»sweepè¿‡
    // å› ä¸ºnonemptyé“¾è¡¨ä¸­çš„spanç¡®å®šæœ€å°‘æœ‰ä¸€ä¸ªæœªåˆ†é…çš„å…ƒç´ , è¿™é‡Œå¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒ
    // we have a nonempty span that does not require sweeping, allocate from it
    c.nonempty.remove(s)
    c.empty.insertBack(s)
    unlock(&amp;c.lock)
    goto havespan
}

// æŸ¥æ‰¾emptyçš„é“¾è¡¨
for s = c.empty.first; s != nil; s = s.next {
    // å¦‚æœspanç­‰å¾…sweep, å°è¯•åŸå­ä¿®æ”¹sweepgenä¸ºå…¨å±€sweepgen-1
    if s.sweepgen == sg-2 &amp;&amp; atomic.Cas(&amp;s.sweepgen, sg-2, sg-1) {
        // æŠŠspanæ”¾åˆ°emptyé“¾è¡¨çš„æœ€å
        // we have an empty span that requires sweeping,
        // sweep it and see if we can free some space in it
        c.empty.remove(s)
        // swept spans are at the end of the list
        c.empty.insertBack(s)
        unlock(&amp;c.lock)
        // å°è¯•sweep
        s.sweep(true)
        // sweepä»¥åè¿˜éœ€è¦æ£€æµ‹æ˜¯å¦æœ‰æœªåˆ†é…çš„å¯¹è±¡, å¦‚æœæœ‰åˆ™å¯ä»¥ä½¿ç”¨å®ƒ
        freeIndex := s.nextFreeIndex()
        if freeIndex != s.nelems {
            s.freeindex = freeIndex
            goto havespan
        }
        lock(&amp;c.lock)
        // the span is still empty after sweep
        // it is already in the empty list, so just retry
        goto retry
    }
    // å¦‚æœè¿™ä¸ªspanæ­£åœ¨è¢«å…¶ä»–çº¿ç¨‹sweep, å°±è·³è¿‡
    if s.sweepgen == sg-1 {
        // the span is being swept by background sweeper, skip
        continue
    }
    // æ‰¾ä¸åˆ°æœ‰æœªåˆ†é…å¯¹è±¡çš„span
    // already swept empty span,
    // all subsequent ones must also be either swept or in process of sweeping
    break
}
if trace.enabled {
    traceGCSweepDone()
    traceDone = true
}
unlock(&amp;c.lock)

// æ‰¾ä¸åˆ°æœ‰æœªåˆ†é…å¯¹è±¡çš„span, éœ€è¦ä»mheapåˆ†é…
// åˆ†é…å®ŒæˆååŠ åˆ°emptyé“¾è¡¨ä¸­
// Replenish central list if empty.
s = c.grow()
if s == nil {
    return nil
}
lock(&amp;c.lock)
c.empty.insertBack(s)
unlock(&amp;c.lock)

// At this point s is a non-empty span, queued at the end of the empty list,
// c is unlocked. havespan:
if trace.enabled &amp;&amp; !traceDone {
    traceGCSweepDone()
}
// ç»Ÿè®¡spanä¸­æœªåˆ†é…çš„å…ƒç´ æ•°é‡, åŠ åˆ°mcentral.nmallocä¸­
// ç»Ÿè®¡spanä¸­æœªåˆ†é…çš„å…ƒç´ æ€»å¤§å°, åŠ åˆ°memstats.heap_liveä¸­
cap := int32((s.npages &lt;&lt; _PageShift) / s.elemsize)
n := cap - int32(s.allocCount)
if n == 0 || s.freeindex == s.nelems || uintptr(s.allocCount) == s.nelems {
    throw("span has no free objects")
}
// Assume all objects from this span will be allocated in the
// mcache. If it gets uncached, we'll adjust this.
atomic.Xadd64(&amp;c.nmalloc, int64(n))
usedBytes := uintptr(s.allocCount) * s.elemsize
atomic.Xadd64(&amp;memstats.heap_live, int64(spanBytes)-int64(usedBytes))
// è·Ÿè¸ªå¤„ç†
if trace.enabled {
    // heap_live changed.
    traceHeapAlloc()
}
// å¦‚æœå½“å‰åœ¨GCä¸­, å› ä¸ºheap_liveæ”¹å˜äº†, é‡æ–°è°ƒæ•´Gè¾…åŠ©æ ‡è®°å·¥ä½œçš„å€¼
// è¯¦ç»†è¯·å‚è€ƒä¸‹é¢å¯¹reviseå‡½æ•°çš„è§£æ
if gcBlackenEnabled != 0 {
    // heap_live changed.
    gcController.revise()
}
// è®¾ç½®spançš„incacheå±æ€§, è¡¨ç¤ºspanæ­£åœ¨mcacheä¸­
s.incache = true
// æ ¹æ®freeindexæ›´æ–°allocCache
freeByteBase := s.freeindex &amp;^ (64 - 1)
whichByte := freeByteBase / 8
// Init alloc bits cache.
s.refillAllocCache(whichByte)

// Adjust the allocCache so that s.freeindex corresponds to the low bit in
// s.allocCache.
s.allocCache &gt;&gt;= s.freeindex % 64

return s } mcentralå‘mheapç”³è¯·ä¸€ä¸ªæ–°çš„spanä¼šä½¿ç”¨growå‡½æ•°:
</code></pre></div></div>

<p>// grow allocates a new empty span from the heap and initializes it for câ€™s size class.
func (c *mcentral) grow() *mspan {
    // æ ¹æ®mcentralçš„ç±»å‹è®¡ç®—éœ€è¦ç”³è¯·çš„spançš„å¤§å°(é™¤ä»¥8K = æœ‰å¤šå°‘é¡µ)å’Œå¯ä»¥ä¿å­˜å¤šå°‘ä¸ªå…ƒç´ 
    npages := uintptr(class_to_allocnpages[c.spanclass.sizeclass()])
    size := uintptr(class_to_size[c.spanclass.sizeclass()])
    n := (npages Â«Â _PageShift) / size</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// å‘mheapç”³è¯·ä¸€ä¸ªæ–°çš„span, ä»¥é¡µ(8K)ä¸ºå•ä½
s := mheap_.alloc(npages, c.spanclass, false, true)
if s == nil {
    return nil
}

p := s.base()
s.limit = p + size*n

// åˆ†é…å¹¶åˆå§‹åŒ–spançš„allocBitså’ŒgcmarkBits
heapBitsForSpan(s.base()).initSpan(s)
return s } mheapåˆ†é…spançš„å‡½æ•°æ˜¯alloc:
</code></pre></div></div>

<p>func (h *mheap) alloc(npage uintptr, spanclass spanClass, large bool, needzero bool) *mspan {
    // åœ¨g0çš„æ ˆç©ºé—´ä¸­è°ƒç”¨alloc_må‡½æ•°
    // å…³äºsystemstackçš„è¯´æ˜è¯·çœ‹å‰ä¸€ç¯‡æ–‡ç« 
    // Donâ€™t do any operations that lock the heap on the G stack.
    // It might trigger stack growth, and the stack growth code needs
    // to be able to allocate heap.
    var s *mspan
    systemstack(func() {
        s = h.alloc_m(npage, spanclass, large)
    })</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if s != nil {
    if needzero &amp;&amp; s.needzero != 0 {
        memclrNoHeapPointers(unsafe.Pointer(s.base()), s.npages&lt;&lt;_PageShift)
    }
    s.needzero = 0
}
return s } allocå‡½æ•°ä¼šåœ¨g0çš„æ ˆç©ºé—´ä¸­è°ƒç”¨alloc_må‡½æ•°:
</code></pre></div></div>

<p>// Allocate a new span of npage pages from the heap for GCâ€™d memory
// and record its size class in the HeapMap and HeapMapCache.
func (h *mheap) alloc_m(npage uintptr, spanclass spanClass, large bool) *mspan {
    <em>g</em> := getg()
    if <em>g</em> != <em>g</em>.m.g0 {
        throw(â€œ_mheap_alloc not on g0 stackâ€)
    }
    // å¯¹mheapä¸Šé”, è¿™é‡Œçš„é”æ˜¯å…¨å±€é”
    lock(&amp;h.lock)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// ä¸ºäº†é˜²æ­¢heapå¢é€Ÿå¤ªå¿«, åœ¨åˆ†é…né¡µä¹‹å‰è¦å…ˆsweepå’Œå›æ”¶né¡µ
// ä¼šå…ˆæšä¸¾busyåˆ—è¡¨ç„¶åå†æšä¸¾busyLargeåˆ—è¡¨è¿›è¡Œsweep, å…·ä½“å‚è€ƒreclaimå’ŒreclaimListå‡½æ•°
// To prevent excessive heap growth, before allocating n pages
// we need to sweep and reclaim at least n pages.
if h.sweepdone == 0 {
    // TODO(austin): This tends to sweep a large number of
    // spans in order to find a few completely free spans
    // (for example, in the garbage benchmark, this sweeps
    // ~30x the number of pages its trying to allocate).
    // If GC kept a bit for whether there were any marks
    // in a span, we could release these free spans
    // at the end of GC and eliminate this entirely.
    if trace.enabled {
        traceGCSweepStart()
    }
    h.reclaim(npage)
    if trace.enabled {
        traceGCSweepDone()
    }
}

// æŠŠmcacheä¸­çš„æœ¬åœ°ç»Ÿè®¡æ•°æ®åŠ åˆ°å…¨å±€
// transfer stats from cache to global
memstats.heap_scan += uint64(_g_.m.mcache.local_scan)
_g_.m.mcache.local_scan = 0
memstats.tinyallocs += uint64(_g_.m.mcache.local_tinyallocs)
_g_.m.mcache.local_tinyallocs = 0

// è°ƒç”¨allocSpanLockedåˆ†é…span, allocSpanLockedå‡½æ•°è¦æ±‚å½“å‰å·²ç»å¯¹mheapä¸Šé”
s := h.allocSpanLocked(npage, &amp;memstats.heap_inuse)
if s != nil {
    // Record span info, because gc needs to be
    // able to map interior pointer to containing span.
    // è®¾ç½®spançš„sweepgen = å…¨å±€sweepgen
    atomic.Store(&amp;s.sweepgen, h.sweepgen)
    // æ”¾åˆ°å…¨å±€spanåˆ—è¡¨ä¸­, è¿™é‡Œçš„sweepSpansçš„é•¿åº¦æ˜¯2
    // sweepSpans[h.sweepgen/2%2]ä¿å­˜å½“å‰æ­£åœ¨ä½¿ç”¨çš„spanåˆ—è¡¨
    // sweepSpans[1-h.sweepgen/2%2]ä¿å­˜ç­‰å¾…sweepçš„spanåˆ—è¡¨
    // å› ä¸ºæ¯æ¬¡gcsweepgenéƒ½ä¼šåŠ 2, æ¯æ¬¡gcè¿™ä¸¤ä¸ªåˆ—è¡¨éƒ½ä¼šäº¤æ¢
    h.sweepSpans[h.sweepgen/2%2].push(s) // Add to swept in-use list.
    // åˆå§‹åŒ–spanæˆå‘˜
    s.state = _MSpanInUse
    s.allocCount = 0
    s.spanclass = spanclass
    if sizeclass := spanclass.sizeclass(); sizeclass == 0 {
        s.elemsize = s.npages &lt;&lt; _PageShift
        s.divShift = 0
        s.divMul = 0
        s.divShift2 = 0
        s.baseMask = 0
    } else {
        s.elemsize = uintptr(class_to_size[sizeclass])
        m := &amp;class_to_divmagic[sizeclass]
        s.divShift = m.shift
        s.divMul = m.mul
        s.divShift2 = m.shift2
        s.baseMask = m.baseMask
    }

    // update stats, sweep lists
    h.pagesInUse += uint64(npage)
    // ä¸Šé¢growå‡½æ•°ä¼šä¼ å…¥true, ä¹Ÿå°±æ˜¯é€šè¿‡growè°ƒç”¨åˆ°è¿™é‡Œlargeä¼šç­‰äºtrue
    // æ·»åŠ å·²åˆ†é…çš„spanåˆ°busyåˆ—è¡¨, å¦‚æœé¡µæ•°è¶…è¿‡_MaxMHeapList(128é¡µ=8K*128=1M)åˆ™æ”¾åˆ°busylargeåˆ—è¡¨
    if large {
        memstats.heap_objects++
        mheap_.largealloc += uint64(s.elemsize)
        mheap_.nlargealloc++
        atomic.Xadd64(&amp;memstats.heap_live, int64(npage&lt;&lt;_PageShift))
        // Swept spans are at the end of lists.
        if s.npages &lt; uintptr(len(h.busy)) {
            h.busy[s.npages].insertBack(s)
        } else {
            h.busylarge.insertBack(s)
        }
    }
}
// å¦‚æœå½“å‰åœ¨GCä¸­, å› ä¸ºheap_liveæ”¹å˜äº†, é‡æ–°è°ƒæ•´Gè¾…åŠ©æ ‡è®°å·¥ä½œçš„å€¼
// è¯¦ç»†è¯·å‚è€ƒä¸‹é¢å¯¹reviseå‡½æ•°çš„è§£æ
// heap_scan and heap_live were updated.
if gcBlackenEnabled != 0 {
    gcController.revise()
}

// è·Ÿè¸ªå¤„ç†
if trace.enabled {
    traceHeapAlloc()
}

// h.spans is accessed concurrently without synchronization
// from other threads. Hence, there must be a store/store
// barrier here to ensure the writes to h.spans above happen
// before the caller can publish a pointer p to an object
// allocated from s. As soon as this happens, the garbage
// collector running on another processor could read p and
// look up s in h.spans. The unlock acts as the barrier to
// order these writes. On the read side, the data dependency
// between p and the index in h.spans orders the reads.
unlock(&amp;h.lock)
return s } ç»§ç»­æŸ¥çœ‹allocSpanLockedå‡½æ•°:
</code></pre></div></div>

<p>// Allocates a span of the given size.  h must be locked.
// The returned span has been removed from the
// free list, but its state is still MSpanFree.
func (h *mheap) allocSpanLocked(npage uintptr, stat *uint64) *mspan {
    var list *mSpanList
    var s *mspan</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// å°è¯•åœ¨mheapä¸­çš„è‡ªç”±åˆ—è¡¨åˆ†é…
// é¡µæ•°å°äº_MaxMHeapList(128é¡µ=1M)çš„è‡ªç”±spanéƒ½ä¼šåœ¨freeåˆ—è¡¨ä¸­
// é¡µæ•°å¤§äº_MaxMHeapListçš„è‡ªç”±spanéƒ½ä¼šåœ¨freelargeåˆ—è¡¨ä¸­
// Try in fixed-size lists up to max.
for i := int(npage); i &lt; len(h.free); i++ {
    list = &amp;h.free[i]
    if !list.isEmpty() {
        s = list.first
        list.remove(s)
        goto HaveSpan
    }
}
// freeåˆ—è¡¨æ‰¾ä¸åˆ°åˆ™æŸ¥æ‰¾freelargeåˆ—è¡¨
// æŸ¥æ‰¾ä¸åˆ°å°±å‘arenaåŒºåŸŸç”³è¯·ä¸€ä¸ªæ–°çš„spanåŠ åˆ°freelargeä¸­, ç„¶åå†æŸ¥æ‰¾freelargeåˆ—è¡¨
// Best fit in list of large spans.
s = h.allocLarge(npage) // allocLarge removed s from h.freelarge for us
if s == nil {
    if !h.grow(npage) {
        return nil
    }
    s = h.allocLarge(npage)
    if s == nil {
        return nil
    }
}
</code></pre></div></div>

<p>HaveSpan:
    // Mark span in use.
    if s.state != _MSpanFree {
        throw(â€œMHeap_AllocLocked - MSpan not freeâ€)
    }
    if s.npages &lt; npage {
        throw(â€œMHeap_AllocLocked - bad npagesâ€)
    }
    // å¦‚æœspanæœ‰å·²é‡Šæ”¾(è§£é™¤è™šæ‹Ÿå†…å­˜å’Œç‰©ç†å†…å­˜å…³ç³»)çš„é¡µ, æé†’è¿™äº›é¡µä¼šè¢«ä½¿ç”¨ç„¶åæ›´æ–°ç»Ÿè®¡æ•°æ®
    if s.npreleased &gt; 0 {
        sysUsed(unsafe.Pointer(s.base()), s.npagesÂ«_PageShift)
        memstats.heap_released -= uint64(s.npreleased Â«Â _PageShift)
        s.npreleased = 0
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// å¦‚æœè·å–åˆ°çš„spané¡µæ•°æ¯”è¦æ±‚çš„é¡µæ•°å¤š
// åˆ†å‰²å‰©ä½™çš„é¡µæ•°åˆ°å¦ä¸€ä¸ªspanå¹¶ä¸”æ”¾åˆ°è‡ªç”±åˆ—è¡¨ä¸­
if s.npages &gt; npage {
    // Trim extra and put it back in the heap.
    t := (*mspan)(h.spanalloc.alloc())
    t.init(s.base()+npage&lt;&lt;_PageShift, s.npages-npage)
    s.npages = npage
    p := (t.base() - h.arena_start) &gt;&gt; _PageShift
    if p &gt; 0 {
        h.spans[p-1] = s
    }
    h.spans[p] = t
    h.spans[p+t.npages-1] = t
    t.needzero = s.needzero
    s.state = _MSpanManual // prevent coalescing with s
    t.state = _MSpanManual
    h.freeSpanLocked(t, false, false, s.unusedsince)
    s.state = _MSpanFree
}
s.unusedsince = 0

// è®¾ç½®spansåŒºåŸŸ, å“ªäº›åœ°å€å¯¹åº”å“ªä¸ªmspanå¯¹è±¡
p := (s.base() - h.arena_start) &gt;&gt; _PageShift
for n := uintptr(0); n &lt; npage; n++ {
    h.spans[p+n] = s
}

// æ›´æ–°ç»Ÿè®¡æ•°æ®
*stat += uint64(npage &lt;&lt; _PageShift)
memstats.heap_idle -= uint64(npage &lt;&lt; _PageShift)

//println("spanalloc", hex(s.start&lt;&lt;_PageShift))
if s.inList() {
    throw("still in list")
}
return s } ç»§ç»­æŸ¥çœ‹allocLargeå‡½æ•°:
</code></pre></div></div>

<p>// allocLarge allocates a span of at least npage pages from the treap of large spans.
// Returns nil if no such span currently exists.
func (h *mheap) allocLarge(npage uintptr) *mspan {
    // Search treap for smallest span with &gt;= npage pages.
    return h.freelarge.remove(npage)
}
freelargeçš„ç±»å‹æ˜¯mTreap, è°ƒç”¨removeå‡½æ•°ä¼šåœ¨æ ‘é‡Œé¢æœç´¢ä¸€ä¸ªè‡³å°‘npageä¸”åœ¨æ ‘ä¸­çš„æœ€å°çš„spanè¿”å›:</p>

<p>// remove searches for, finds, removes from the treap, and returns the smallest
// span that can hold npages. If no span has at least npages return nil.
// This is slightly more complicated than a simple binary tree search
// since if an exact match is not found the next larger node is
// returned.
// If the last node inspected &gt; npagesKey not holding
// a left node (a smaller npages) is the â€œbest fitâ€ node.
func (root *mTreap) remove(npages uintptr) *mspan {
    t := root.treap
    for t != nil {
        if t.spanKey == nil {
            throw(â€œtreap node with nil spanKey foundâ€)
        }
        if t.npagesKey &lt; npages {
            t = t.right
        } else if t.left != nil &amp;&amp; t.left.npagesKey &gt;= npages {
            t = t.left
        } else {
            result := t.spanKey
            root.removeNode(t)
            return result
        }
    }
    return nil
}
å‘arenaåŒºåŸŸç”³è¯·æ–°spançš„å‡½æ•°æ˜¯mheapç±»çš„growå‡½æ•°:</p>

<p>// Try to add at least npage pages of memory to the heap,
// returning whether it worked.
//
// h must be locked.
func (h *mheap) grow(npage uintptr) bool {
    // Ask for a big chunk, to reduce the number of mappings
    // the operating system needs to track; also amortizes
    // the overhead of an operating system mapping.
    // Allocate a multiple of 64kB.
    npage = round(npage, (64Â«10)/_PageSize)
    ask := npage Â«Â _PageShift
    if ask &lt; _HeapAllocChunk {
        ask = _HeapAllocChunk
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// è°ƒç”¨mheap.sysAllocå‡½æ•°ç”³è¯·
v := h.sysAlloc(ask)
if v == nil {
    if ask &gt; npage&lt;&lt;_PageShift {
        ask = npage &lt;&lt; _PageShift
        v = h.sysAlloc(ask)
    }
    if v == nil {
        print("runtime: out of memory: cannot allocate ", ask, "-byte block (", memstats.heap_sys, " in use)\n")
        return false
    }
}

// åˆ›å»ºä¸€ä¸ªæ–°çš„spanå¹¶åŠ åˆ°è‡ªç”±åˆ—è¡¨ä¸­
// Create a fake "in use" span and free it, so that the
// right coalescing happens.
s := (*mspan)(h.spanalloc.alloc())
s.init(uintptr(v), ask&gt;&gt;_PageShift)
p := (s.base() - h.arena_start) &gt;&gt; _PageShift
for i := p; i &lt; p+s.npages; i++ {
    h.spans[i] = s
}
atomic.Store(&amp;s.sweepgen, h.sweepgen)
s.state = _MSpanInUse
h.pagesInUse += uint64(s.npages)
h.freeSpanLocked(s, false, true, 0)
return true } ç»§ç»­æŸ¥çœ‹mheapçš„sysAllocå‡½æ•°:
</code></pre></div></div>

<p>// sysAlloc allocates the next n bytes from the heap arena. The
// returned pointer is always _PageSize aligned and between
// h.arena_start and h.arena_end. sysAlloc returns nil on failure.
// There is no corresponding free function.
func (h *mheap) sysAlloc(n uintptr) unsafe.Pointer {
    // strandLimit is the maximum number of bytes to strand from
    // the current arena block. If we would need to strand more
    // than this, we fall back to sysAllocâ€™ing just enough for
    // this allocation.
    const strandLimit = 16 Â«Â 20</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// å¦‚æœarenaåŒºåŸŸå½“å‰å·²æäº¤çš„åŒºåŸŸä¸è¶³, åˆ™è°ƒç”¨sysReserveé¢„ç•™æ›´å¤šçš„ç©ºé—´, ç„¶åæ›´æ–°arena_end
// sysReserveåœ¨linuxä¸Šè°ƒç”¨çš„æ˜¯mmapå‡½æ•°
// mmap(v, n, _PROT_NONE, _MAP_ANON|_MAP_PRIVATE, -1, 0)
if n &gt; h.arena_end-h.arena_alloc {
    // If we haven't grown the arena to _MaxMem yet, try
    // to reserve some more address space.
    p_size := round(n+_PageSize, 256&lt;&lt;20)
    new_end := h.arena_end + p_size // Careful: can overflow
    if h.arena_end &lt;= new_end &amp;&amp; new_end-h.arena_start-1 &lt;= _MaxMem {
        // TODO: It would be bad if part of the arena
        // is reserved and part is not.
        var reserved bool
        p := uintptr(sysReserve(unsafe.Pointer(h.arena_end), p_size, &amp;reserved))
        if p == 0 {
            // TODO: Try smaller reservation
            // growths in case we're in a crowded
            // 32-bit address space.
            goto reservationFailed
        }
        // p can be just about anywhere in the address
        // space, including before arena_end.
        if p == h.arena_end {
            // The new block is contiguous with
            // the current block. Extend the
            // current arena block.
            h.arena_end = new_end
            h.arena_reserved = reserved
        } else if h.arena_start &lt;= p &amp;&amp; p+p_size-h.arena_start-1 &lt;= _MaxMem &amp;&amp; h.arena_end-h.arena_alloc &lt; strandLimit {
            // We were able to reserve more memory
            // within the arena space, but it's
            // not contiguous with our previous
            // reservation. It could be before or
            // after our current arena_used.
            //
            // Keep everything page-aligned.
            // Our pages are bigger than hardware pages.
            h.arena_end = p + p_size
            p = round(p, _PageSize)
            h.arena_alloc = p
            h.arena_reserved = reserved
        } else {
            // We got a mapping, but either
            //
            // 1) It's not in the arena, so we
            // can't use it. (This should never
            // happen on 32-bit.)
            //
            // 2) We would need to discard too
            // much of our current arena block to
            // use it.
            //
            // We haven't added this allocation to
            // the stats, so subtract it from a
            // fake stat (but avoid underflow).
            //
            // We'll fall back to a small sysAlloc.
            stat := uint64(p_size)
            sysFree(unsafe.Pointer(p), p_size, &amp;stat)
        }
    }
}

// é¢„ç•™çš„ç©ºé—´è¶³å¤Ÿæ—¶åªéœ€è¦å¢åŠ arena_alloc
if n &lt;= h.arena_end-h.arena_alloc {
    // Keep taking from our reservation.
    p := h.arena_alloc
    sysMap(unsafe.Pointer(p), n, h.arena_reserved, &amp;memstats.heap_sys)
    h.arena_alloc += n
    if h.arena_alloc &gt; h.arena_used {
        h.setArenaUsed(h.arena_alloc, true)
    }

    if p&amp;(_PageSize-1) != 0 {
        throw("misrounded allocation in MHeap_SysAlloc")
    }
    return unsafe.Pointer(p)
}

// é¢„ç•™ç©ºé—´å¤±è´¥åçš„å¤„ç† reservationFailed:
// If using 64-bit, our reservation is all we have.
if sys.PtrSize != 4 {
    return nil
}

// On 32-bit, once the reservation is gone we can
// try to get memory at a location chosen by the OS.
p_size := round(n, _PageSize) + _PageSize
p := uintptr(sysAlloc(p_size, &amp;memstats.heap_sys))
if p == 0 {
    return nil
}

if p &lt; h.arena_start || p+p_size-h.arena_start &gt; _MaxMem {
    // This shouldn't be possible because _MaxMem is the
    // whole address space on 32-bit.
    top := uint64(h.arena_start) + _MaxMem
    print("runtime: memory allocated by OS (", hex(p), ") not in usable range [", hex(h.arena_start), ",", hex(top), ")\n")
    sysFree(unsafe.Pointer(p), p_size, &amp;memstats.heap_sys)
    return nil
}

p += -p &amp; (_PageSize - 1)
if p+n &gt; h.arena_used {
    h.setArenaUsed(p+n, true)
}

if p&amp;(_PageSize-1) != 0 {
    throw("misrounded allocation in MHeap_SysAlloc")
}
return unsafe.Pointer(p) } ä»¥ä¸Šå°±æ˜¯åˆ†é…å¯¹è±¡çš„å®Œæ•´æµç¨‹äº†, æ¥ä¸‹æ¥åˆ†æGCæ ‡è®°å’Œå›æ”¶å¯¹è±¡çš„å¤„ç†.
</code></pre></div></div>

<p>å›æ”¶å¯¹è±¡çš„å¤„ç†
å›æ”¶å¯¹è±¡çš„æµç¨‹
GOçš„GCæ˜¯å¹¶è¡ŒGC, ä¹Ÿå°±æ˜¯GCçš„å¤§éƒ¨åˆ†å¤„ç†å’Œæ™®é€šçš„goä»£ç æ˜¯åŒæ—¶è¿è¡Œçš„, è¿™è®©GOçš„GCæµç¨‹æ¯”è¾ƒå¤æ‚.
é¦–å…ˆGCæœ‰å››ä¸ªé˜¶æ®µ, å®ƒä»¬åˆ†åˆ«æ˜¯:</p>

<p>Sweep Termination: å¯¹æœªæ¸…æ‰«çš„spanè¿›è¡Œæ¸…æ‰«, åªæœ‰ä¸Šä¸€è½®çš„GCçš„æ¸…æ‰«å·¥ä½œå®Œæˆæ‰å¯ä»¥å¼€å§‹æ–°ä¸€è½®çš„GC
Mark: æ‰«ææ‰€æœ‰æ ¹å¯¹è±¡, å’Œæ ¹å¯¹è±¡å¯ä»¥åˆ°è¾¾çš„æ‰€æœ‰å¯¹è±¡, æ ‡è®°å®ƒä»¬ä¸è¢«å›æ”¶
Mark Termination: å®Œæˆæ ‡è®°å·¥ä½œ, é‡æ–°æ‰«æéƒ¨åˆ†æ ¹å¯¹è±¡(è¦æ±‚STW)
Sweep: æŒ‰æ ‡è®°ç»“æœæ¸…æ‰«span
ä¸‹å›¾æ˜¯æ¯”è¾ƒå®Œæ•´çš„GCæµç¨‹, å¹¶æŒ‰é¢œè‰²å¯¹è¿™å››ä¸ªé˜¶æ®µè¿›è¡Œäº†åˆ†ç±»:
<img src="https://xiazemin.github.io/MyBlog/img/gc_stw.png" />
åœ¨GCè¿‡ç¨‹ä¸­ä¼šæœ‰ä¸¤ç§åå°ä»»åŠ¡(G), ä¸€ç§æ˜¯æ ‡è®°ç”¨çš„åå°ä»»åŠ¡, ä¸€ç§æ˜¯æ¸…æ‰«ç”¨çš„åå°ä»»åŠ¡.
æ ‡è®°ç”¨çš„åå°ä»»åŠ¡ä¼šåœ¨éœ€è¦æ—¶å¯åŠ¨, å¯ä»¥åŒæ—¶å·¥ä½œçš„åå°ä»»åŠ¡æ•°é‡å¤§çº¦æ˜¯Pçš„æ•°é‡çš„25%, ä¹Ÿå°±æ˜¯goæ‰€è®²çš„è®©25%çš„cpuç”¨åœ¨GCä¸Šçš„æ ¹æ®.
æ¸…æ‰«ç”¨çš„åå°ä»»åŠ¡åœ¨ç¨‹åºå¯åŠ¨æ—¶ä¼šå¯åŠ¨ä¸€ä¸ª, è¿›å…¥æ¸…æ‰«é˜¶æ®µæ—¶å”¤é†’.</p>

<p>ç›®å‰æ•´ä¸ªGCæµç¨‹ä¼šè¿›è¡Œä¸¤æ¬¡STW(Stop The World), ç¬¬ä¸€æ¬¡æ˜¯Marké˜¶æ®µçš„å¼€å§‹, ç¬¬äºŒæ¬¡æ˜¯Mark Terminationé˜¶æ®µ.
ç¬¬ä¸€æ¬¡STWä¼šå‡†å¤‡æ ¹å¯¹è±¡çš„æ‰«æ, å¯åŠ¨å†™å±éšœ(Write Barrier)å’Œè¾…åŠ©GC(mutator assist).
ç¬¬äºŒæ¬¡STWä¼šé‡æ–°æ‰«æéƒ¨åˆ†æ ¹å¯¹è±¡, ç¦ç”¨å†™å±éšœ(Write Barrier)å’Œè¾…åŠ©GC(mutator assist).
éœ€è¦æ³¨æ„çš„æ˜¯, ä¸æ˜¯æ‰€æœ‰æ ¹å¯¹è±¡çš„æ‰«æéƒ½éœ€è¦STW, ä¾‹å¦‚æ‰«ææ ˆä¸Šçš„å¯¹è±¡åªéœ€è¦åœæ­¢æ‹¥æœ‰è¯¥æ ˆçš„G.
ä»go 1.9å¼€å§‹, å†™å±éšœçš„å®ç°ä½¿ç”¨äº†Hybrid Write Barrier, å¤§å¹…å‡å°‘äº†ç¬¬äºŒæ¬¡STWçš„æ—¶é—´.</p>

<p>GCçš„è§¦å‘æ¡ä»¶
GCåœ¨æ»¡è¶³ä¸€å®šæ¡ä»¶åä¼šè¢«è§¦å‘, è§¦å‘æ¡ä»¶æœ‰ä»¥ä¸‹å‡ ç§:</p>

<p>gcTriggerAlways: å¼ºåˆ¶è§¦å‘GC
gcTriggerHeap: å½“å‰åˆ†é…çš„å†…å­˜è¾¾åˆ°ä¸€å®šå€¼å°±è§¦å‘GC
gcTriggerTime: å½“ä¸€å®šæ—¶é—´æ²¡æœ‰æ‰§è¡Œè¿‡GCå°±è§¦å‘GC
gcTriggerCycle: è¦æ±‚å¯åŠ¨æ–°ä¸€è½®çš„GC, å·²å¯åŠ¨åˆ™è·³è¿‡, æ‰‹åŠ¨è§¦å‘GCçš„runtime.GC()ä¼šä½¿ç”¨è¿™ä¸ªæ¡ä»¶
è§¦å‘æ¡ä»¶çš„åˆ¤æ–­åœ¨gctriggerçš„testå‡½æ•°.
å…¶ä¸­gcTriggerHeapå’ŒgcTriggerTimeè¿™ä¸¤ä¸ªæ¡ä»¶æ˜¯è‡ªç„¶è§¦å‘çš„, gcTriggerHeapçš„åˆ¤æ–­ä»£ç å¦‚ä¸‹:</p>

<p>return memstats.heap_live &gt;= memstats.gc_trigger
heap_liveçš„å¢åŠ åœ¨ä¸Šé¢å¯¹åˆ†é…å™¨çš„ä»£ç åˆ†æä¸­å¯ä»¥çœ‹åˆ°, å½“å€¼è¾¾åˆ°gc_triggerå°±ä¼šè§¦å‘GC, é‚£ä¹ˆgc_triggeræ˜¯å¦‚ä½•å†³å®šçš„?
gc_triggerçš„è®¡ç®—åœ¨gcSetTriggerRatioå‡½æ•°ä¸­, å…¬å¼æ˜¯:</p>

<p>trigger = uint64(float64(memstats.heap_marked) * (1 + triggerRatio))
å½“å‰æ ‡è®°å­˜æ´»çš„å¤§å°ä¹˜ä»¥1+ç³»æ•°triggerRatio, å°±æ˜¯ä¸‹æ¬¡å‡ºå‘GCéœ€è¦çš„åˆ†é…é‡.
triggerRatioåœ¨æ¯æ¬¡GCåéƒ½ä¼šè°ƒæ•´, è®¡ç®—triggerRatioçš„å‡½æ•°æ˜¯encCycle, å…¬å¼æ˜¯:</p>

<p>const triggerGain = 0.5
// ç›®æ ‡Heapå¢é•¿ç‡, é»˜è®¤æ˜¯1.0
goalGrowthRatio := float64(gcpercent) / 100
// å®é™…Heapå¢é•¿ç‡, ç­‰äºæ€»å¤§å°/å­˜æ´»å¤§å°-1
actualGrowthRatio := float64(memstats.heap_live)/float64(memstats.heap_marked) - 1
// GCæ ‡è®°é˜¶æ®µçš„ä½¿ç”¨æ—¶é—´(å› ä¸ºendCycleæ˜¯åœ¨Mark Terminationé˜¶æ®µè°ƒç”¨çš„)
assistDuration := nanotime() - c.markStartTime
// GCæ ‡è®°é˜¶æ®µçš„CPUå ç”¨ç‡, ç›®æ ‡å€¼æ˜¯0.25
utilization := gcGoalUtilization
if assistDuration &gt; 0 {
    // assistTimeæ˜¯Gè¾…åŠ©GCæ ‡è®°å¯¹è±¡æ‰€ä½¿ç”¨çš„æ—¶é—´åˆè®¡
    // (nanosecnds spent in mutator assists during this cycle)
    // é¢å¤–çš„CPUå ç”¨ç‡ = è¾…åŠ©GCæ ‡è®°å¯¹è±¡çš„æ€»æ—¶é—´ / (GCæ ‡è®°ä½¿ç”¨æ—¶é—´ * Pçš„æ•°é‡)
    utilization += float64(c.assistTime) / float64(assistDuration<em>int64(gomaxprocs))
}
// è§¦å‘ç³»æ•°åç§»å€¼ = ç›®æ ‡å¢é•¿ç‡ - åŸè§¦å‘ç³»æ•° - CPUå ç”¨ç‡ / ç›®æ ‡CPUå ç”¨ç‡ * (å®é™…å¢é•¿ç‡ - åŸè§¦å‘ç³»æ•°)
// å‚æ•°çš„åˆ†æ:
// å®é™…å¢é•¿ç‡è¶Šå¤§, è§¦å‘ç³»æ•°åç§»å€¼è¶Šå°, å°äº0æ—¶ä¸‹æ¬¡è§¦å‘GCä¼šææ—©
// CPUå ç”¨ç‡è¶Šå¤§, è§¦å‘ç³»æ•°åç§»å€¼è¶Šå°, å°äº0æ—¶ä¸‹æ¬¡è§¦å‘GCä¼šææ—©
// åŸè§¦å‘ç³»æ•°è¶Šå¤§, è§¦å‘ç³»æ•°åç§»å€¼è¶Šå°, å°äº0æ—¶ä¸‹æ¬¡è§¦å‘GCä¼šææ—©
triggerError := goalGrowthRatio - memstats.triggerRatio - utilization/gcGoalUtilization</em>(actualGrowthRatio-memstats.triggerRatio)
// æ ¹æ®åç§»å€¼è°ƒæ•´è§¦å‘ç³»æ•°, æ¯æ¬¡åªè°ƒæ•´åç§»å€¼çš„ä¸€åŠ(æ¸è¿›å¼è°ƒæ•´)
triggerRatio := memstats.triggerRatio + triggerGain*triggerError
å…¬å¼ä¸­çš„â€ç›®æ ‡Heapå¢é•¿ç‡â€å¯ä»¥é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡â€GOGCâ€è°ƒæ•´, é»˜è®¤å€¼æ˜¯100, å¢åŠ å®ƒçš„å€¼å¯ä»¥å‡å°‘GCçš„è§¦å‘.
è®¾ç½®â€GOGC=offâ€å¯ä»¥å½»åº•å…³æ‰GC.</p>

<p>gcTriggerTimeçš„åˆ¤æ–­ä»£ç å¦‚ä¸‹:</p>

<p>lastgc := int64(atomic.Load64(&amp;memstats.last_gc_nanotime))
return lastgc != 0 &amp;&amp; t.now-lastgc &gt; forcegcperiod
forcegcperiodçš„å®šä¹‰æ˜¯2åˆ†é’Ÿ, ä¹Ÿå°±æ˜¯2åˆ†é’Ÿå†…æ²¡æœ‰æ‰§è¡Œè¿‡GCå°±ä¼šå¼ºåˆ¶è§¦å‘.</p>

<p>ä¸‰è‰²çš„å®šä¹‰(é»‘, ç°, ç™½)
æˆ‘çœ‹è¿‡çš„å¯¹ä¸‰è‰²GCçš„â€ä¸‰è‰²â€è¿™ä¸ªæ¦‚å¿µè§£é‡Šçš„æœ€å¥½çš„æ–‡ç« å°±æ˜¯è¿™ä¸€ç¯‡äº†, å¼ºçƒˆå»ºè®®å…ˆçœ‹è¿™ä¸€ç¯‡ä¸­çš„è®²è§£.
â€œä¸‰è‰²â€çš„æ¦‚å¿µå¯ä»¥ç®€å•çš„ç†è§£ä¸º:</p>

<p>é»‘è‰²: å¯¹è±¡åœ¨è¿™æ¬¡GCä¸­å·²æ ‡è®°, ä¸”è¿™ä¸ªå¯¹è±¡åŒ…å«çš„å­å¯¹è±¡ä¹Ÿå·²æ ‡è®°
ç°è‰²: å¯¹è±¡åœ¨è¿™æ¬¡GCä¸­å·²æ ‡è®°, ä½†è¿™ä¸ªå¯¹è±¡åŒ…å«çš„å­å¯¹è±¡æœªæ ‡è®°
ç™½è‰²: å¯¹è±¡åœ¨è¿™æ¬¡GCä¸­æœªæ ‡è®°
åœ¨goå†…éƒ¨å¯¹è±¡å¹¶æ²¡æœ‰ä¿å­˜é¢œè‰²çš„å±æ€§, ä¸‰è‰²åªæ˜¯å¯¹å®ƒä»¬çš„çŠ¶æ€çš„æè¿°,
ç™½è‰²çš„å¯¹è±¡åœ¨å®ƒæ‰€åœ¨çš„spançš„gcmarkBitsä¸­å¯¹åº”çš„bitä¸º0,
ç°è‰²çš„å¯¹è±¡åœ¨å®ƒæ‰€åœ¨çš„spançš„gcmarkBitsä¸­å¯¹åº”çš„bitä¸º1, å¹¶ä¸”å¯¹è±¡åœ¨æ ‡è®°é˜Ÿåˆ—ä¸­,
é»‘è‰²çš„å¯¹è±¡åœ¨å®ƒæ‰€åœ¨çš„spançš„gcmarkBitsä¸­å¯¹åº”çš„bitä¸º1, å¹¶ä¸”å¯¹è±¡å·²ç»ä»æ ‡è®°é˜Ÿåˆ—ä¸­å–å‡ºå¹¶å¤„ç†.
gcå®Œæˆå, gcmarkBitsä¼šç§»åŠ¨åˆ°allocBitsç„¶åé‡æ–°åˆ†é…ä¸€ä¸ªå…¨éƒ¨ä¸º0çš„bitmap, è¿™æ ·é»‘è‰²çš„å¯¹è±¡å°±å˜ä¸ºäº†ç™½è‰².</p>

<p>å†™å±éšœ(Write Barrier)
å› ä¸ºgoæ”¯æŒå¹¶è¡ŒGC, GCçš„æ‰«æå’Œgoä»£ç å¯ä»¥åŒæ—¶è¿è¡Œ, è¿™æ ·å¸¦æ¥çš„é—®é¢˜æ˜¯GCæ‰«æçš„è¿‡ç¨‹ä¸­goä»£ç æœ‰å¯èƒ½æ”¹å˜äº†å¯¹è±¡çš„ä¾èµ–æ ‘,
ä¾‹å¦‚å¼€å§‹æ‰«ææ—¶å‘ç°æ ¹å¯¹è±¡Aå’ŒB, Bæ‹¥æœ‰Cçš„æŒ‡é’ˆ, GCå…ˆæ‰«æA, ç„¶åBæŠŠCçš„æŒ‡é’ˆäº¤ç»™A, GCå†æ‰«æB, è¿™æ—¶Cå°±ä¸ä¼šè¢«æ‰«æåˆ°.
ä¸ºäº†é¿å…è¿™ä¸ªé—®é¢˜, goåœ¨GCçš„æ ‡è®°é˜¶æ®µä¼šå¯ç”¨å†™å±éšœ(Write Barrier).</p>

<p>å¯ç”¨äº†å†™å±éšœ(Write Barrier)å, å½“BæŠŠCçš„æŒ‡é’ˆäº¤ç»™Aæ—¶, GCä¼šè®¤ä¸ºåœ¨è¿™ä¸€è½®çš„æ‰«æä¸­Cçš„æŒ‡é’ˆæ˜¯å­˜æ´»çš„,
å³ä½¿Aå¯èƒ½ä¼šåœ¨ç¨åä¸¢æ‰C, é‚£ä¹ˆCå°±åœ¨ä¸‹ä¸€è½®å›æ”¶.
å†™å±éšœåªé’ˆå¯¹æŒ‡é’ˆå¯ç”¨, è€Œä¸”åªåœ¨GCçš„æ ‡è®°é˜¶æ®µå¯ç”¨, å¹³æ—¶ä¼šç›´æ¥æŠŠå€¼å†™å…¥åˆ°ç›®æ ‡åœ°å€.</p>

<p>goåœ¨1.9å¼€å§‹å¯ç”¨äº†æ··åˆå†™å±éšœ(Hybrid Write Barrier), ä¼ªä»£ç å¦‚ä¸‹:</p>

<p>writePointer(slot, ptr):
    shade(*slot)
    if any stack is grey:
        shade(ptr)
    *slot = ptr
æ··åˆå†™å±éšœä¼šåŒæ—¶æ ‡è®°æŒ‡é’ˆå†™å…¥ç›®æ ‡çš„â€åŸæŒ‡é’ˆâ€å’Œâ€œæ–°æŒ‡é’ˆâ€.</p>

<p>æ ‡è®°åŸæŒ‡é’ˆçš„åŸå› æ˜¯, å…¶ä»–è¿è¡Œä¸­çš„çº¿ç¨‹æœ‰å¯èƒ½ä¼šåŒæ—¶æŠŠè¿™ä¸ªæŒ‡é’ˆçš„å€¼å¤åˆ¶åˆ°å¯„å­˜å™¨æˆ–è€…æ ˆä¸Šçš„æœ¬åœ°å˜é‡,
å› ä¸ºå¤åˆ¶æŒ‡é’ˆåˆ°å¯„å­˜å™¨æˆ–è€…æ ˆä¸Šçš„æœ¬åœ°å˜é‡ä¸ä¼šç»è¿‡å†™å±éšœ, æ‰€ä»¥æœ‰å¯èƒ½ä¼šå¯¼è‡´æŒ‡é’ˆä¸è¢«æ ‡è®°, è¯•æƒ³ä¸‹é¢çš„æƒ…å†µ:</p>

<p>[go] b = obj
[go] oldx = nil
[gc] scan oldxâ€¦
[go] oldx = b.x // å¤åˆ¶b.xåˆ°æœ¬åœ°å˜é‡, ä¸è¿›è¿‡å†™å±éšœ
[go] b.x = ptr // å†™å±éšœåº”è¯¥æ ‡è®°b.xçš„åŸå€¼
[gc] scan bâ€¦
å¦‚æœå†™å±éšœä¸æ ‡è®°åŸå€¼, é‚£ä¹ˆoldxå°±ä¸ä¼šè¢«æ‰«æåˆ°.
æ ‡è®°æ–°æŒ‡é’ˆçš„åŸå› æ˜¯, å…¶ä»–è¿è¡Œä¸­çš„çº¿ç¨‹æœ‰å¯èƒ½ä¼šè½¬ç§»æŒ‡é’ˆçš„ä½ç½®, è¯•æƒ³ä¸‹é¢çš„æƒ…å†µ:</p>

<p>[go] a = ptr
[go] b = obj
[gc] scan bâ€¦
[go] b.x = a // å†™å±éšœåº”è¯¥æ ‡è®°b.xçš„æ–°å€¼
[go] a = nil
[gc] scan aâ€¦
å¦‚æœå†™å±éšœä¸æ ‡è®°æ–°å€¼, é‚£ä¹ˆptrå°±ä¸ä¼šè¢«æ‰«æåˆ°.
æ··åˆå†™å±éšœå¯ä»¥è®©GCåœ¨å¹¶è¡Œæ ‡è®°ç»“æŸåä¸éœ€è¦é‡æ–°æ‰«æå„ä¸ªGçš„å †æ ˆ, å¯ä»¥å‡å°‘Mark Terminationä¸­çš„STWæ—¶é—´.
é™¤äº†å†™å±éšœå¤–, åœ¨GCçš„è¿‡ç¨‹ä¸­æ‰€æœ‰æ–°åˆ†é…çš„å¯¹è±¡éƒ½ä¼šç«‹åˆ»å˜ä¸ºé»‘è‰², åœ¨ä¸Šé¢çš„mallocgcå‡½æ•°ä¸­å¯ä»¥çœ‹åˆ°.</p>

<p>è¾…åŠ©GC(mutator assist)
ä¸ºäº†é˜²æ­¢heapå¢é€Ÿå¤ªå¿«, åœ¨GCæ‰§è¡Œçš„è¿‡ç¨‹ä¸­å¦‚æœåŒæ—¶è¿è¡Œçš„Gåˆ†é…äº†å†…å­˜, é‚£ä¹ˆè¿™ä¸ªGä¼šè¢«è¦æ±‚è¾…åŠ©GCåšä¸€éƒ¨åˆ†çš„å·¥ä½œ.
åœ¨GCçš„è¿‡ç¨‹ä¸­åŒæ—¶è¿è¡Œçš„Gç§°ä¸ºâ€mutatorâ€, â€œmutator assistâ€æœºåˆ¶å°±æ˜¯Gè¾…åŠ©GCåšä¸€éƒ¨åˆ†å·¥ä½œçš„æœºåˆ¶.</p>

<p>è¾…åŠ©GCåšçš„å·¥ä½œæœ‰ä¸¤ç§ç±»å‹, ä¸€ç§æ˜¯æ ‡è®°(Mark), å¦ä¸€ç§æ˜¯æ¸…æ‰«(Sweep).
è¾…åŠ©æ ‡è®°çš„è§¦å‘å¯ä»¥æŸ¥çœ‹ä¸Šé¢çš„mallocgcå‡½æ•°, è§¦å‘æ—¶Gä¼šå¸®åŠ©æ‰«æâ€å·¥ä½œé‡â€ä¸ªå¯¹è±¡, å·¥ä½œé‡çš„è®¡ç®—å…¬å¼æ˜¯:</p>

<p>debtBytes * assistWorkPerByte
æ„æ€æ˜¯åˆ†é…çš„å¤§å°ä¹˜ä»¥ç³»æ•°assistWorkPerByte, assistWorkPerByteçš„è®¡ç®—åœ¨å‡½æ•°reviseä¸­, å…¬å¼æ˜¯:</p>

<p>// ç­‰å¾…æ‰«æçš„å¯¹è±¡æ•°é‡ = æœªæ‰«æçš„å¯¹è±¡æ•°é‡ - å·²æ‰«æçš„å¯¹è±¡æ•°é‡
scanWorkExpected := int64(memstats.heap_scan) - c.scanWork
if scanWorkExpected &lt; 1000 {
    scanWorkExpected = 1000
}
// è·ç¦»è§¦å‘GCçš„Heapå¤§å° = æœŸå¾…è§¦å‘GCçš„Heapå¤§å° - å½“å‰çš„Heapå¤§å°
// æ³¨æ„next_gcçš„è®¡ç®—è·Ÿgc_triggerä¸ä¸€æ ·, next_gcç­‰äºheap_marked * (1 + gcpercent / 100)
heapDistance := int64(memstats.next_gc) - int64(atomic.Load64(&amp;memstats.heap_live))
if heapDistance &lt;= 0 {
    heapDistance = 1
}
// æ¯åˆ†é…1 byteéœ€è¦è¾…åŠ©æ‰«æçš„å¯¹è±¡æ•°é‡ = ç­‰å¾…æ‰«æçš„å¯¹è±¡æ•°é‡ / è·ç¦»è§¦å‘GCçš„Heapå¤§å°
c.assistWorkPerByte = float64(scanWorkExpected) / float64(heapDistance)
c.assistBytesPerWork = float64(heapDistance) / float64(scanWorkExpected)
å’Œè¾…åŠ©æ ‡è®°ä¸ä¸€æ ·çš„æ˜¯, è¾…åŠ©æ¸…æ‰«ç”³è¯·æ–°spanæ—¶æ‰ä¼šæ£€æŸ¥, è€Œè¾…åŠ©æ ‡è®°æ˜¯æ¯æ¬¡åˆ†é…å¯¹è±¡æ—¶éƒ½ä¼šæ£€æŸ¥.
è¾…åŠ©æ¸…æ‰«çš„è§¦å‘å¯ä»¥çœ‹ä¸Šé¢çš„cacheSpanå‡½æ•°, è§¦å‘æ—¶Gä¼šå¸®åŠ©å›æ”¶â€å·¥ä½œé‡â€é¡µçš„å¯¹è±¡, å·¥ä½œé‡çš„è®¡ç®—å…¬å¼æ˜¯:</p>

<p>spanBytes * sweepPagesPerByte // ä¸å®Œå…¨ç›¸åŒ, å…·ä½“çœ‹deductSweepCreditå‡½æ•°
æ„æ€æ˜¯åˆ†é…çš„å¤§å°ä¹˜ä»¥ç³»æ•°sweepPagesPerByte, sweepPagesPerByteçš„è®¡ç®—åœ¨å‡½æ•°gcSetTriggerRatioä¸­, å…¬å¼æ˜¯:</p>

<p>// å½“å‰çš„Heapå¤§å°
heapLiveBasis := atomic.Load64(&amp;memstats.heap_live)
// è·ç¦»è§¦å‘GCçš„Heapå¤§å° = ä¸‹æ¬¡è§¦å‘GCçš„Heapå¤§å° - å½“å‰çš„Heapå¤§å°
heapDistance := int64(trigger) - int64(heapLiveBasis)
heapDistance -= 1024 * 1024
if heapDistance &lt; <em>PageSize {
    heapDistance = _PageSize
}
// å·²æ¸…æ‰«çš„é¡µæ•°
pagesSwept := atomic.Load64(&amp;mheap</em>.pagesSwept)
// æœªæ¸…æ‰«çš„é¡µæ•° = ä½¿ç”¨ä¸­çš„é¡µæ•° - å·²æ¸…æ‰«çš„é¡µæ•°
sweepDistancePages := int64(mheap_.pagesInUse) - int64(pagesSwept)
if sweepDistancePages &lt;= 0 {
    mheap_.sweepPagesPerByte = 0
} else {
    // æ¯åˆ†é…1 byte(çš„span)éœ€è¦è¾…åŠ©æ¸…æ‰«çš„é¡µæ•° = æœªæ¸…æ‰«çš„é¡µæ•° / è·ç¦»è§¦å‘GCçš„Heapå¤§å°
    mheap_.sweepPagesPerByte = float64(sweepDistancePages) / float64(heapDistance)
}
æ ¹å¯¹è±¡
åœ¨GCçš„æ ‡è®°é˜¶æ®µé¦–å…ˆéœ€è¦æ ‡è®°çš„å°±æ˜¯â€æ ¹å¯¹è±¡â€, ä»æ ¹å¯¹è±¡å¼€å§‹å¯åˆ°è¾¾çš„æ‰€æœ‰å¯¹è±¡éƒ½ä¼šè¢«è®¤ä¸ºæ˜¯å­˜æ´»çš„.
æ ¹å¯¹è±¡åŒ…å«äº†å…¨å±€å˜é‡, å„ä¸ªGçš„æ ˆä¸Šçš„å˜é‡ç­‰, GCä¼šå…ˆæ‰«ææ ¹å¯¹è±¡ç„¶åå†æ‰«ææ ¹å¯¹è±¡å¯åˆ°è¾¾çš„æ‰€æœ‰å¯¹è±¡.
æ‰«ææ ¹å¯¹è±¡åŒ…å«äº†ä¸€ç³»åˆ—çš„å·¥ä½œ, å®ƒä»¬å®šä¹‰åœ¨[https://github.com/golang/go/blob/go1.9.2/src/runtime/mgcmark.go#L54]å‡½æ•°:</p>

<p>Fixed Roots: ç‰¹æ®Šçš„æ‰«æå·¥ä½œ
fixedRootFinalizers: æ‰«æææ„å™¨é˜Ÿåˆ—
fixedRootFreeGStacks: é‡Šæ”¾å·²ä¸­æ­¢çš„Gçš„æ ˆ
Flush Cache Roots: é‡Šæ”¾mcacheä¸­çš„æ‰€æœ‰span, è¦æ±‚STW
Data Roots: æ‰«æå¯è¯»å†™çš„å…¨å±€å˜é‡
BSS Roots: æ‰«æåªè¯»çš„å…¨å±€å˜é‡
Span Roots: æ‰«æå„ä¸ªspanä¸­ç‰¹æ®Šå¯¹è±¡(ææ„å™¨åˆ—è¡¨)
Stack Roots: æ‰«æå„ä¸ªGçš„æ ˆ
æ ‡è®°é˜¶æ®µ(Mark)ä¼šåšå…¶ä¸­çš„â€Fixed Rootsâ€, â€œData Rootsâ€, â€œBSS Rootsâ€, â€œSpan Rootsâ€, â€œStack Rootsâ€.
å®Œæˆæ ‡è®°é˜¶æ®µ(Mark Termination)ä¼šåšå…¶ä¸­çš„â€Fixed Rootsâ€, â€œFlush Cache Rootsâ€.</p>

<p>æ ‡è®°é˜Ÿåˆ—
GCçš„æ ‡è®°é˜¶æ®µä¼šä½¿ç”¨â€æ ‡è®°é˜Ÿåˆ—â€æ¥ç¡®å®šæ‰€æœ‰å¯ä»æ ¹å¯¹è±¡åˆ°è¾¾çš„å¯¹è±¡éƒ½å·²æ ‡è®°, ä¸Šé¢æåˆ°çš„â€ç°è‰²â€çš„å¯¹è±¡å°±æ˜¯åœ¨æ ‡è®°é˜Ÿåˆ—ä¸­çš„å¯¹è±¡.
ä¸¾ä¾‹æ¥è¯´, å¦‚æœå½“å‰æœ‰[A, B, C]è¿™ä¸‰ä¸ªæ ¹å¯¹è±¡, é‚£ä¹ˆæ‰«ææ ¹å¯¹è±¡æ—¶å°±ä¼šæŠŠå®ƒä»¬æ”¾åˆ°æ ‡è®°é˜Ÿåˆ—:</p>

<p>work queue: [A, B, C]
åå°æ ‡è®°ä»»åŠ¡ä»æ ‡è®°é˜Ÿåˆ—ä¸­å–å‡ºA, å¦‚æœAå¼•ç”¨äº†D, åˆ™æŠŠDæ”¾å…¥æ ‡è®°é˜Ÿåˆ—:</p>

<p>work queue: [B, C, D]
åå°æ ‡è®°ä»»åŠ¡ä»æ ‡è®°é˜Ÿåˆ—å–å‡ºB, å¦‚æœBä¹Ÿå¼•ç”¨äº†D, è¿™æ—¶å› ä¸ºDåœ¨gcmarkBitsä¸­å¯¹åº”çš„bitå·²ç»æ˜¯1æ‰€ä»¥ä¼šè·³è¿‡:</p>

<p>work queue: [C, D]
å¦‚æœå¹¶è¡Œè¿è¡Œçš„goä»£ç åˆ†é…äº†ä¸€ä¸ªå¯¹è±¡E, å¯¹è±¡Eä¼šè¢«ç«‹åˆ»æ ‡è®°, ä½†ä¸ä¼šè¿›å…¥æ ‡è®°é˜Ÿåˆ—(å› ä¸ºç¡®å®šEæ²¡æœ‰å¼•ç”¨å…¶ä»–å¯¹è±¡).
ç„¶åå¹¶è¡Œè¿è¡Œçš„goä»£ç æŠŠå¯¹è±¡Fè®¾ç½®ç»™å¯¹è±¡Eçš„æˆå‘˜, å†™å±éšœä¼šæ ‡è®°å¯¹è±¡Fç„¶åæŠŠå¯¹è±¡FåŠ åˆ°è¿è¡Œé˜Ÿåˆ—:</p>

<p>work queue: [C, D, F]
åå°æ ‡è®°ä»»åŠ¡ä»æ ‡è®°é˜Ÿåˆ—å–å‡ºC, å¦‚æœCæ²¡æœ‰å¼•ç”¨å…¶ä»–å¯¹è±¡, åˆ™ä¸éœ€è¦å¤„ç†:</p>

<p>work queue: [D, F]
åå°æ ‡è®°ä»»åŠ¡ä»æ ‡è®°é˜Ÿåˆ—å–å‡ºD, å¦‚æœDå¼•ç”¨äº†X, åˆ™æŠŠXæ”¾å…¥æ ‡è®°é˜Ÿåˆ—:</p>

<p>work queue: [F, X]
åå°æ ‡è®°ä»»åŠ¡ä»æ ‡è®°é˜Ÿåˆ—å–å‡ºF, å¦‚æœFæ²¡æœ‰å¼•ç”¨å…¶ä»–å¯¹è±¡, åˆ™ä¸éœ€è¦å¤„ç†.
åå°æ ‡è®°ä»»åŠ¡ä»æ ‡è®°é˜Ÿåˆ—å–å‡ºX, å¦‚æœXæ²¡æœ‰å¼•ç”¨å…¶ä»–å¯¹è±¡, åˆ™ä¸éœ€è¦å¤„ç†.
æœ€åæ ‡è®°é˜Ÿåˆ—ä¸ºç©º, æ ‡è®°å®Œæˆ, å­˜æ´»çš„å¯¹è±¡æœ‰[A, B, C, D, E, F, X].</p>

<p>å®é™…çš„çŠ¶å†µä¼šæ¯”ä¸Šé¢ä»‹ç»çš„çŠ¶å†µç¨å¾®å¤æ‚ä¸€ç‚¹.
æ ‡è®°é˜Ÿåˆ—ä¼šåˆ†ä¸ºå…¨å±€æ ‡è®°é˜Ÿåˆ—å’Œå„ä¸ªPçš„æœ¬åœ°æ ‡è®°é˜Ÿåˆ—, è¿™ç‚¹å’Œåç¨‹ä¸­çš„è¿è¡Œé˜Ÿåˆ—ç›¸ä¼¼.
å¹¶ä¸”æ ‡è®°é˜Ÿåˆ—ä¸ºç©ºä»¥å, è¿˜éœ€è¦åœæ­¢æ•´ä¸ªä¸–ç•Œå¹¶ç¦æ­¢å†™å±éšœ, ç„¶åå†æ¬¡æ£€æŸ¥æ˜¯å¦ä¸ºç©º.</p>

<p>æºä»£ç åˆ†æ
goè§¦å‘gcä¼šä»gcStartå‡½æ•°å¼€å§‹:</p>

<p>// gcStart transitions the GC from _GCoff to _GCmark (if
// !mode.stwMark) or _GCmarktermination (if mode.stwMark) by
// performing sweep termination and GC initialization.
//
// This may return without performing this transition in some cases,
// such as when called on a system stack or with locks held.
func gcStart(mode gcMode, trigger gcTrigger) {
    // åˆ¤æ–­å½“å‰Gæ˜¯å¦å¯æŠ¢å , ä¸å¯æŠ¢å æ—¶ä¸è§¦å‘GC
    // Since this is called from malloc and malloc is called in
    // the guts of a number of libraries that might be holding
    // locks, donâ€™t attempt to start GC in non-preemptible or
    // potentially unstable situations.
    mp := acquirem()
    if gp := getg(); gp == mp.g0 || mp.locks &gt; 1 || mp.preemptoff != â€œâ€ {
        releasem(mp)
        return
    }
    releasem(mp)
    mp = nil</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// å¹¶è¡Œæ¸…æ‰«ä¸Šä¸€è½®GCæœªæ¸…æ‰«çš„span
// Pick up the remaining unswept/not being swept spans concurrently
//
// This shouldn't happen if we're being invoked in background
// mode since proportional sweep should have just finished
// sweeping everything, but rounding errors, etc, may leave a
// few spans unswept. In forced mode, this is necessary since
// GC can be forced at any point in the sweeping cycle.
//
// We check the transition condition continuously here in case
// this G gets delayed in to the next GC cycle.
for trigger.test() &amp;&amp; gosweepone() != ^uintptr(0) {
    sweep.nbgsweep++
}

// ä¸Šé”, ç„¶åé‡æ–°æ£€æŸ¥gcTriggerçš„æ¡ä»¶æ˜¯å¦æˆç«‹, ä¸æˆç«‹æ—¶ä¸è§¦å‘GC
// Perform GC initialization and the sweep termination
// transition.
semacquire(&amp;work.startSema)
// Re-check transition condition under transition lock.
if !trigger.test() {
    semrelease(&amp;work.startSema)
    return
}

// è®°å½•æ˜¯å¦å¼ºåˆ¶è§¦å‘, gcTriggerCycleæ˜¯runtime.GCç”¨çš„
// For stats, check if this GC was forced by the user.
work.userForced = trigger.kind == gcTriggerAlways || trigger.kind == gcTriggerCycle

// åˆ¤æ–­æ˜¯å¦æŒ‡å®šäº†ç¦æ­¢å¹¶è¡ŒGCçš„å‚æ•°
// In gcstoptheworld debug mode, upgrade the mode accordingly.
// We do this after re-checking the transition condition so
// that multiple goroutines that detect the heap trigger don't
// start multiple STW GCs.
if mode == gcBackgroundMode {
    if debug.gcstoptheworld == 1 {
        mode = gcForceMode
    } else if debug.gcstoptheworld == 2 {
        mode = gcForceBlockMode
    }
}

// Ok, we're doing it!  Stop everybody else
semacquire(&amp;worldsema)

// è·Ÿè¸ªå¤„ç†
if trace.enabled {
    traceGCStart()
}

// å¯åŠ¨åå°æ‰«æä»»åŠ¡(G)
if mode == gcBackgroundMode {
    gcBgMarkStartWorkers()
}

// é‡ç½®æ ‡è®°ç›¸å…³çš„çŠ¶æ€
gcResetMarkState()

// é‡ç½®å‚æ•°
work.stwprocs, work.maxprocs = gcprocs(), gomaxprocs
work.heap0 = atomic.Load64(&amp;memstats.heap_live)
work.pauseNS = 0
work.mode = mode

// è®°å½•å¼€å§‹æ—¶é—´
now := nanotime()
work.tSweepTerm = now
work.pauseStart = now

// åœæ­¢æ‰€æœ‰è¿è¡Œä¸­çš„G, å¹¶ç¦æ­¢å®ƒä»¬è¿è¡Œ
systemstack(stopTheWorldWithSema)

// !!!!!!!!!!!!!!!!
// ä¸–ç•Œå·²åœæ­¢(STW)...
// !!!!!!!!!!!!!!!!

// æ¸…æ‰«ä¸Šä¸€è½®GCæœªæ¸…æ‰«çš„span, ç¡®ä¿ä¸Šä¸€è½®GCå·²å®Œæˆ
// Finish sweep before we start concurrent scan.
systemstack(func() {
    finishsweep_m()
})
// æ¸…æ‰«sched.sudogcacheå’Œsched.deferpool
// clearpools before we start the GC. If we wait they memory will not be
// reclaimed until the next GC cycle.
clearpools()

// å¢åŠ GCè®¡æ•°
work.cycles++

// åˆ¤æ–­æ˜¯å¦å¹¶è¡ŒGCæ¨¡å¼
if mode == gcBackgroundMode { // Do as much work concurrently as possible
    // æ ‡è®°æ–°ä¸€è½®GCå·²å¼€å§‹
    gcController.startCycle()
    work.heapGoal = memstats.next_gc

    // è®¾ç½®å…¨å±€å˜é‡ä¸­çš„GCçŠ¶æ€ä¸º_GCmark
    // ç„¶åå¯ç”¨å†™å±éšœ
    // Enter concurrent mark phase and enable
    // write barriers.
    //
    // Because the world is stopped, all Ps will
    // observe that write barriers are enabled by
    // the time we start the world and begin
    // scanning.
    //
    // Write barriers must be enabled before assists are
    // enabled because they must be enabled before
    // any non-leaf heap objects are marked. Since
    // allocations are blocked until assists can
    // happen, we want enable assists as early as
    // possible.
    setGCPhase(_GCmark)

    // é‡ç½®åå°æ ‡è®°ä»»åŠ¡çš„è®¡æ•°
    gcBgMarkPrepare() // Must happen before assist enable.

    // è®¡ç®—æ‰«ææ ¹å¯¹è±¡çš„ä»»åŠ¡æ•°é‡
    gcMarkRootPrepare()

    // æ ‡è®°æ‰€æœ‰tiny allocç­‰å¾…åˆå¹¶çš„å¯¹è±¡
    // Mark all active tinyalloc blocks. Since we're
    // allocating from these, they need to be black like
    // other allocations. The alternative is to blacken
    // the tiny block on every allocation from it, which
    // would slow down the tiny allocator.
    gcMarkTinyAllocs()

    // å¯ç”¨è¾…åŠ©GC
    // At this point all Ps have enabled the write
    // barrier, thus maintaining the no white to
    // black invariant. Enable mutator assists to
    // put back-pressure on fast allocating
    // mutators.
    atomic.Store(&amp;gcBlackenEnabled, 1)

    // è®°å½•æ ‡è®°å¼€å§‹çš„æ—¶é—´
    // Assists and workers can start the moment we start
    // the world.
    gcController.markStartTime = now

    // é‡æ–°å¯åŠ¨ä¸–ç•Œ
    // å‰é¢åˆ›å»ºçš„åå°æ ‡è®°ä»»åŠ¡ä¼šå¼€å§‹å·¥ä½œ, æ‰€æœ‰åå°æ ‡è®°ä»»åŠ¡éƒ½å®Œæˆå·¥ä½œå, è¿›å…¥å®Œæˆæ ‡è®°é˜¶æ®µ
    // Concurrent mark.
    systemstack(startTheWorldWithSema)
    
    // !!!!!!!!!!!!!!!
    // ä¸–ç•Œå·²é‡æ–°å¯åŠ¨...
    // !!!!!!!!!!!!!!!
    
    // è®°å½•åœæ­¢äº†å¤šä¹…, å’Œæ ‡è®°é˜¶æ®µå¼€å§‹çš„æ—¶é—´
    now = nanotime()
    work.pauseNS += now - work.pauseStart
    work.tMark = now
} else {
    // ä¸æ˜¯å¹¶è¡ŒGCæ¨¡å¼
    // è®°å½•å®Œæˆæ ‡è®°é˜¶æ®µå¼€å§‹çš„æ—¶é—´
    t := nanotime()
    work.tMark, work.tMarkTerm = t, t
    work.heapGoal = work.heap0

    // è·³è¿‡æ ‡è®°é˜¶æ®µ, æ‰§è¡Œå®Œæˆæ ‡è®°é˜¶æ®µ
    // æ‰€æœ‰æ ‡è®°å·¥ä½œéƒ½ä¼šåœ¨ä¸–ç•Œå·²åœæ­¢çš„çŠ¶æ€æ‰§è¡Œ
    // (æ ‡è®°é˜¶æ®µä¼šè®¾ç½®work.markrootDone=true, å¦‚æœè·³è¿‡åˆ™å®ƒçš„å€¼æ˜¯false, å®Œæˆæ ‡è®°é˜¶æ®µä¼šæ‰§è¡Œæ‰€æœ‰å·¥ä½œ)
    // å®Œæˆæ ‡è®°é˜¶æ®µä¼šé‡æ–°å¯åŠ¨ä¸–ç•Œ
    // Perform mark termination. This will restart the world.
    gcMarkTermination(memstats.triggerRatio)
}

semrelease(&amp;work.startSema) } æ¥ä¸‹æ¥ä¸€ä¸ªä¸ªåˆ†ægcStartè°ƒç”¨çš„å‡½æ•°, å»ºè®®é…åˆä¸Šé¢çš„"å›æ”¶å¯¹è±¡çš„æµç¨‹"ä¸­çš„å›¾ç†è§£.
</code></pre></div></div>

<p>å‡½æ•°gcBgMarkStartWorkersç”¨äºå¯åŠ¨åå°æ ‡è®°ä»»åŠ¡, å…ˆåˆ†åˆ«å¯¹æ¯ä¸ªPå¯åŠ¨ä¸€ä¸ª:</p>

<p>// gcBgMarkStartWorkers prepares background mark worker goroutines.
// These goroutines will not run until the mark phase, but they must
// be started while the work is not stopped and from a regular G
// stack. The caller must hold worldsema.
func gcBgMarkStartWorkers() {
    // Background marking is performed by per-P Gâ€™s. Ensure that
    // each P has a background GC G.
    for _, p := range &amp;allp {
        if p == nil || p.status == _Pdead {
            break
        }
        // å¦‚æœå·²å¯åŠ¨åˆ™ä¸é‡å¤å¯åŠ¨
        if p.gcBgMarkWorker == 0 {
            go gcBgMarkWorker(p)
            // å¯åŠ¨åç­‰å¾…è¯¥ä»»åŠ¡é€šçŸ¥ä¿¡å·é‡bgMarkReadyå†ç»§ç»­
            notetsleepg(&amp;work.bgMarkReady, -1)
            noteclear(&amp;work.bgMarkReady)
        }
    }
}
è¿™é‡Œè™½ç„¶ä¸ºæ¯ä¸ªPå¯åŠ¨äº†ä¸€ä¸ªåå°æ ‡è®°ä»»åŠ¡, ä½†æ˜¯å¯ä»¥åŒæ—¶å·¥ä½œçš„åªæœ‰25%, è¿™ä¸ªé€»è¾‘åœ¨åç¨‹Mè·å–Gæ—¶è°ƒç”¨çš„findRunnableGCWorkerä¸­:</p>

<p>// findRunnableGCWorker returns the background mark worker for <em>p</em> if it
// should be run. This must only be called when gcBlackenEnabled != 0.
func (c *gcControllerState) findRunnableGCWorker(<em>p</em> *p) *g {
    if gcBlackenEnabled == 0 {
        throw(â€œgcControllerState.findRunnable: blackening not enabledâ€)
    }
    if <em>p</em>.gcBgMarkWorker == 0 {
        // The mark worker associated with this P is blocked
        // performing a mark transition. We canâ€™t run it
        // because it may be on some other run or wait queue.
        return nil
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if !gcMarkWorkAvailable(_p_) {
    // No work to be done right now. This can happen at
    // the end of the mark phase when there are still
    // assists tapering off. Don't bother running a worker
    // now because it'll just return immediately.
    return nil
}

// åŸå­å‡å°‘å¯¹åº”çš„å€¼, å¦‚æœå‡å°‘åå¤§äºç­‰äº0åˆ™è¿”å›true, å¦åˆ™è¿”å›false
decIfPositive := func(ptr *int64) bool {
    if *ptr &gt; 0 {
        if atomic.Xaddint64(ptr, -1) &gt;= 0 {
            return true
        }
        // We lost a race
        atomic.Xaddint64(ptr, +1)
    }
    return false
}

// å‡å°‘dedicatedMarkWorkersNeeded, æˆåŠŸæ—¶åå°æ ‡è®°ä»»åŠ¡çš„æ¨¡å¼æ˜¯Dedicated
// dedicatedMarkWorkersNeededæ˜¯å½“å‰Pçš„æ•°é‡çš„25%å»é™¤å°æ•°ç‚¹
// è¯¦è§startCycleå‡½æ•°
if decIfPositive(&amp;c.dedicatedMarkWorkersNeeded) {
    // This P is now dedicated to marking until the end of
    // the concurrent mark phase.
    _p_.gcMarkWorkerMode = gcMarkWorkerDedicatedMode
} else {
    // å‡å°‘fractionalMarkWorkersNeeded, æˆåŠŸæ˜¯åå°æ ‡è®°ä»»åŠ¡çš„æ¨¡å¼æ˜¯Fractional
    // ä¸Šé¢çš„è®¡ç®—å¦‚æœå°æ•°ç‚¹åæœ‰æ•°å€¼(ä¸èƒ½å¤Ÿæ•´é™¤)åˆ™fractionalMarkWorkersNeededä¸º1, å¦åˆ™ä¸º0
    // è¯¦è§startCycleå‡½æ•°
    // ä¸¾ä¾‹æ¥è¯´, 4ä¸ªPæ—¶ä¼šæ‰§è¡Œ1ä¸ªDedicatedæ¨¡å¼çš„ä»»åŠ¡, 5ä¸ªPæ—¶ä¼šæ‰§è¡Œ1ä¸ªDedicatedæ¨¡å¼å’Œ1ä¸ªFractionalæ¨¡å¼çš„ä»»åŠ¡
    if !decIfPositive(&amp;c.fractionalMarkWorkersNeeded) {
        // No more workers are need right now.
        return nil
    }

    // æŒ‰Dedicatedæ¨¡å¼çš„ä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´åˆ¤æ–­cpuå ç”¨ç‡æ˜¯å¦è¶…è¿‡é¢„ç®—å€¼, è¶…è¿‡æ—¶ä¸å¯åŠ¨
    // This P has picked the token for the fractional worker.
    // Is the GC currently under or at the utilization goal?
    // If so, do more work.
    //
    // We used to check whether doing one time slice of work
    // would remain under the utilization goal, but that has the
    // effect of delaying work until the mutator has run for
    // enough time slices to pay for the work. During those time
    // slices, write barriers are enabled, so the mutator is running slower.
    // Now instead we do the work whenever we're under or at the
    // utilization work and pay for it by letting the mutator run later.
    // This doesn't change the overall utilization averages, but it
    // front loads the GC work so that the GC finishes earlier and
    // write barriers can be turned off sooner, effectively giving
    // the mutator a faster machine.
    //
    // The old, slower behavior can be restored by setting
    //  gcForcePreemptNS = forcePreemptNS.
    const gcForcePreemptNS = 0

    // TODO(austin): We could fast path this and basically
    // eliminate contention on c.fractionalMarkWorkersNeeded by
    // precomputing the minimum time at which it's worth
    // next scheduling the fractional worker. Then Ps
    // don't have to fight in the window where we've
    // passed that deadline and no one has started the
    // worker yet.
    //
    // TODO(austin): Shorter preemption interval for mark
    // worker to improve fairness and give this
    // finer-grained control over schedule?
    now := nanotime() - gcController.markStartTime
    then := now + gcForcePreemptNS
    timeUsed := c.fractionalMarkTime + gcForcePreemptNS
    if then &gt; 0 &amp;&amp; float64(timeUsed)/float64(then) &gt; c.fractionalUtilizationGoal {
        // Nope, we'd overshoot the utilization goal
        atomic.Xaddint64(&amp;c.fractionalMarkWorkersNeeded, +1)
        return nil
    }
    _p_.gcMarkWorkerMode = gcMarkWorkerFractionalMode
}

// å®‰æ’åå°æ ‡è®°ä»»åŠ¡æ‰§è¡Œ
// Run the background mark worker
gp := _p_.gcBgMarkWorker.ptr()
casgstatus(gp, _Gwaiting, _Grunnable)
if trace.enabled {
    traceGoUnpark(gp, 0)
}
return gp } gcResetMarkStateå‡½æ•°ä¼šé‡ç½®æ ‡è®°ç›¸å…³çš„çŠ¶æ€:
</code></pre></div></div>

<p>// gcResetMarkState resets global state prior to marking (concurrent
// or STW) and resets the stack scan state of all Gs.
//
// This is safe to do without the world stopped because any Gs created
// during or after this will start out in the reset state.
func gcResetMarkState() {
    // This may be called during a concurrent phase, so make sure
    // allgs doesnâ€™t change.
    lock(&amp;allglock)
    for _, gp := range allgs {
        gp.gcscandone = false  // set to true in gcphasework
        gp.gcscanvalid = false // stack has not been scanned
        gp.gcAssistBytes = 0
    }
    unlock(&amp;allglock)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>work.bytesMarked = 0
work.initialHeapLive = atomic.Load64(&amp;memstats.heap_live)
work.markrootDone = false } stopTheWorldWithSemaå‡½æ•°ä¼šåœæ­¢æ•´ä¸ªä¸–ç•Œ, è¿™ä¸ªå‡½æ•°å¿…é¡»åœ¨g0ä¸­è¿è¡Œ:
</code></pre></div></div>

<p>// stopTheWorldWithSema is the core implementation of stopTheWorld.
// The caller is responsible for acquiring worldsema and disabling
// preemption first and then should stopTheWorldWithSema on the system
// stack:
//
//  semacquire(&amp;worldsema, 0)
//  m.preemptoff = â€œreasonâ€
//  systemstack(stopTheWorldWithSema)
//
// When finished, the caller must either call startTheWorld or undo
// these three operations separately:
//
//  m.preemptoff = â€œâ€
//  systemstack(startTheWorldWithSema)
//  semrelease(&amp;worldsema)
//
// It is allowed to acquire worldsema once and then execute multiple
// startTheWorldWithSema/stopTheWorldWithSema pairs.
// Other Pâ€™s are able to execute between successive calls to
// startTheWorldWithSema and stopTheWorldWithSema.
// Holding worldsema causes any other goroutines invoking
// stopTheWorld to block.
func stopTheWorldWithSema() {
    <em>g</em> := getg()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// If we hold a lock, then we won't be able to stop another M
// that is blocked trying to acquire the lock.
if _g_.m.locks &gt; 0 {
    throw("stopTheWorld: holding locks")
}

lock(&amp;sched.lock)

// éœ€è¦åœæ­¢çš„Pæ•°é‡
sched.stopwait = gomaxprocs

// è®¾ç½®gcç­‰å¾…æ ‡è®°, è°ƒåº¦æ—¶çœ‹è§æ­¤æ ‡è®°ä¼šè¿›å…¥ç­‰å¾…
atomic.Store(&amp;sched.gcwaiting, 1)

// æŠ¢å æ‰€æœ‰è¿è¡Œä¸­çš„G
preemptall()

// åœæ­¢å½“å‰çš„P
// stop current P
_g_.m.p.ptr().status = _Pgcstop // Pgcstop is only diagnostic.

// å‡å°‘éœ€è¦åœæ­¢çš„Pæ•°é‡(å½“å‰çš„Pç®—ä¸€ä¸ª)
sched.stopwait--

// æŠ¢å æ‰€æœ‰åœ¨PsyscallçŠ¶æ€çš„P, é˜²æ­¢å®ƒä»¬é‡æ–°å‚ä¸è°ƒåº¦
// try to retake all P's in Psyscall status
for i := 0; i &lt; int(gomaxprocs); i++ {
    p := allp[i]
    s := p.status
    if s == _Psyscall &amp;&amp; atomic.Cas(&amp;p.status, s, _Pgcstop) {
        if trace.enabled {
            traceGoSysBlock(p)
            traceProcStop(p)
        }
        p.syscalltick++
        sched.stopwait--
    }
}

// é˜²æ­¢æ‰€æœ‰ç©ºé—²çš„Pé‡æ–°å‚ä¸è°ƒåº¦
// stop idle P's
for {
    p := pidleget()
    if p == nil {
        break
    }
    p.status = _Pgcstop
    sched.stopwait--
}
wait := sched.stopwait &gt; 0
unlock(&amp;sched.lock)

// å¦‚æœä»æœ‰éœ€è¦åœæ­¢çš„P, åˆ™ç­‰å¾…å®ƒä»¬åœæ­¢
// wait for remaining P's to stop voluntarily
if wait {
    for {
        // å¾ªç¯ç­‰å¾… + æŠ¢å æ‰€æœ‰è¿è¡Œä¸­çš„G
        // wait for 100us, then try to re-preempt in case of any races
        if notetsleep(&amp;sched.stopnote, 100*1000) {
            noteclear(&amp;sched.stopnote)
            break
        }
        preemptall()
    }
}

// é€»è¾‘æ­£ç¡®æ€§æ£€æŸ¥
// sanity checks
bad := ""
if sched.stopwait != 0 {
    bad = "stopTheWorld: not stopped (stopwait != 0)"
} else {
    for i := 0; i &lt; int(gomaxprocs); i++ {
        p := allp[i]
        if p.status != _Pgcstop {
            bad = "stopTheWorld: not stopped (status != _Pgcstop)"
        }
    }
}
if atomic.Load(&amp;freezing) != 0 {
    // Some other thread is panicking. This can cause the
    // sanity checks above to fail if the panic happens in
    // the signal handler on a stopped thread. Either way,
    // we should halt this thread.
    lock(&amp;deadlock)
    lock(&amp;deadlock)
}
if bad != "" {
    throw(bad)
}

// åˆ°è¿™é‡Œæ‰€æœ‰è¿è¡Œä¸­çš„Géƒ½ä¼šå˜ä¸ºå¾…è¿è¡Œ, å¹¶ä¸”æ‰€æœ‰çš„Péƒ½ä¸èƒ½è¢«Mè·å–
// ä¹Ÿå°±æ˜¯è¯´æ‰€æœ‰çš„goä»£ç (é™¤äº†å½“å‰çš„)éƒ½ä¼šåœæ­¢è¿è¡Œ, å¹¶ä¸”ä¸èƒ½è¿è¡Œæ–°çš„goä»£ç  } finishsweep_må‡½æ•°ä¼šæ¸…æ‰«ä¸Šä¸€è½®GCæœªæ¸…æ‰«çš„span, ç¡®ä¿ä¸Šä¸€è½®GCå·²å®Œæˆ:
</code></pre></div></div>

<p>// finishsweep_m ensures that all spans are swept.
//
// The world must be stopped. This ensures there are no sweeps in
// progress.
//
//go:nowritebarrier
func finishsweep_m() {
    // sweeponeä¼šå–å‡ºä¸€ä¸ªæœªsweepçš„spanç„¶åæ‰§è¡Œsweep
    // è¯¦ç»†å°†åœ¨ä¸‹é¢sweepé˜¶æ®µæ—¶åˆ†æ
    // Sweeping must be complete before marking commences, so
    // sweep any unswept spans. If this is a concurrent GC, there
    // shouldnâ€™t be any spans left to sweep, so this should finish
    // instantly. If GC was forced before the concurrent sweep
    // finished, there may be spans to sweep.
    for sweepone() != ^uintptr(0) {
        sweep.npausesweep++
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// æ‰€æœ‰spanéƒ½sweepå®Œæˆå, å¯åŠ¨ä¸€ä¸ªæ–°çš„markbitæ—¶ä»£
// è¿™ä¸ªå‡½æ•°æ˜¯å®ç°spançš„gcmarkBitså’ŒallocBitsçš„åˆ†é…å’Œå¤ç”¨çš„å…³é”®, æµç¨‹å¦‚ä¸‹
// - spanåˆ†é…gcmarkBitså’ŒallocBits
// - spanå®Œæˆsweep
//   - åŸallocBitsä¸å†è¢«ä½¿ç”¨
//   - gcmarkBitså˜ä¸ºallocBits
//   - åˆ†é…æ–°çš„gcmarkBits
// - å¼€å¯æ–°çš„markbitæ—¶ä»£
// - spanå®Œæˆsweep, åŒä¸Š
// - å¼€å¯æ–°çš„markbitæ—¶ä»£
//   - 2ä¸ªæ—¶ä»£ä¹‹å‰çš„bitmapå°†ä¸å†è¢«ä½¿ç”¨, å¯ä»¥å¤ç”¨è¿™äº›bitmap
nextMarkBitArenaEpoch() } clearpoolså‡½æ•°ä¼šæ¸…ç†sched.sudogcacheå’Œsched.deferpool, è®©å®ƒä»¬çš„å†…å­˜å¯ä»¥è¢«å›æ”¶:
</code></pre></div></div>

<p>func clearpools() {
    // clear sync.Pools
    if poolcleanup != nil {
        poolcleanup()
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Clear central sudog cache.
// Leave per-P caches alone, they have strictly bounded size.
// Disconnect cached list before dropping it on the floor,
// so that a dangling ref to one entry does not pin all of them.
lock(&amp;sched.sudoglock)
var sg, sgnext *sudog
for sg = sched.sudogcache; sg != nil; sg = sgnext {
    sgnext = sg.next
    sg.next = nil
}
sched.sudogcache = nil
unlock(&amp;sched.sudoglock)

// Clear central defer pools.
// Leave per-P pools alone, they have strictly bounded size.
lock(&amp;sched.deferlock)
for i := range sched.deferpool {
    // disconnect cached list before dropping it on the floor,
    // so that a dangling ref to one entry does not pin all of them.
    var d, dlink *_defer
    for d = sched.deferpool[i]; d != nil; d = dlink {
        dlink = d.link
        d.link = nil
    }
    sched.deferpool[i] = nil
}
unlock(&amp;sched.deferlock) } startCycleæ ‡è®°å¼€å§‹äº†æ–°ä¸€è½®çš„GC:
</code></pre></div></div>

<p>// startCycle resets the GC controllerâ€™s state and computes estimates
// for a new GC cycle. The caller must hold worldsema.
func (c *gcControllerState) startCycle() {
    c.scanWork = 0
    c.bgScanCredit = 0
    c.assistTime = 0
    c.dedicatedMarkTime = 0
    c.fractionalMarkTime = 0
    c.idleMarkTime = 0</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// ä¼ªè£…heap_markedçš„å€¼å¦‚æœgc_triggerçš„å€¼å¾ˆå°, é˜²æ­¢åé¢å¯¹triggerRatioåšå‡ºé”™è¯¯çš„è°ƒæ•´
// If this is the first GC cycle or we're operating on a very
// small heap, fake heap_marked so it looks like gc_trigger is
// the appropriate growth from heap_marked, even though the
// real heap_marked may not have a meaningful value (on the
// first cycle) or may be much smaller (resulting in a large
// error response).
if memstats.gc_trigger &lt;= heapminimum {
    memstats.heap_marked = uint64(float64(memstats.gc_trigger) / (1 + memstats.triggerRatio))
}

// é‡æ–°è®¡ç®—next_gc, æ³¨æ„next_gcçš„è®¡ç®—è·Ÿgc_triggerä¸ä¸€æ ·
// Re-compute the heap goal for this cycle in case something
// changed. This is the same calculation we use elsewhere.
memstats.next_gc = memstats.heap_marked + memstats.heap_marked*uint64(gcpercent)/100
if gcpercent &lt; 0 {
    memstats.next_gc = ^uint64(0)
}

// ç¡®ä¿next_gcå’Œheap_liveä¹‹é—´æœ€å°‘æœ‰1MB
// Ensure that the heap goal is at least a little larger than
// the current live heap size. This may not be the case if GC
// start is delayed or if the allocation that pushed heap_live
// over gc_trigger is large or if the trigger is really close to
// GOGC. Assist is proportional to this distance, so enforce a
// minimum distance, even if it means going over the GOGC goal
// by a tiny bit.
if memstats.next_gc &lt; memstats.heap_live+1024*1024 {
    memstats.next_gc = memstats.heap_live + 1024*1024
}

// è®¡ç®—å¯ä»¥åŒæ—¶æ‰§è¡Œçš„åå°æ ‡è®°ä»»åŠ¡çš„æ•°é‡
// dedicatedMarkWorkersNeededç­‰äºPçš„æ•°é‡çš„25%å»é™¤å°æ•°ç‚¹
// å¦‚æœå¯ä»¥æ•´é™¤åˆ™fractionalMarkWorkersNeededç­‰äº0å¦åˆ™ç­‰äº1
// totalUtilizationGoalæ˜¯GCæ‰€å çš„Pçš„ç›®æ ‡å€¼(ä¾‹å¦‚Pä¸€å…±æœ‰5ä¸ªæ—¶ç›®æ ‡æ˜¯1.25ä¸ªP)
// fractionalUtilizationGoalæ˜¯Fractionaæ¨¡å¼çš„ä»»åŠ¡æ‰€å çš„Pçš„ç›®æ ‡å€¼(ä¾‹å¦‚Pä¸€å…±æœ‰5ä¸ªæ—¶ç›®æ ‡æ˜¯0.25ä¸ªP)
// Compute the total mark utilization goal and divide it among
// dedicated and fractional workers.
totalUtilizationGoal := float64(gomaxprocs) * gcGoalUtilization
c.dedicatedMarkWorkersNeeded = int64(totalUtilizationGoal)
c.fractionalUtilizationGoal = totalUtilizationGoal - float64(c.dedicatedMarkWorkersNeeded)
if c.fractionalUtilizationGoal &gt; 0 {
    c.fractionalMarkWorkersNeeded = 1
} else {
    c.fractionalMarkWorkersNeeded = 0
}

// é‡ç½®Pä¸­çš„è¾…åŠ©GCæ‰€ç”¨çš„æ—¶é—´ç»Ÿè®¡
// Clear per-P state
for _, p := range &amp;allp {
    if p == nil {
        break
    }
    p.gcAssistTime = 0
}

// è®¡ç®—è¾…åŠ©GCçš„å‚æ•°
// å‚è€ƒä¸Šé¢å¯¹è®¡ç®—assistWorkPerByteçš„å…¬å¼çš„åˆ†æ
// Compute initial values for controls that are updated
// throughout the cycle.
c.revise()

if debug.gcpacertrace &gt; 0 {
    print("pacer: assist ratio=", c.assistWorkPerByte,
        " (scan ", memstats.heap_scan&gt;&gt;20, " MB in ",
        work.initialHeapLive&gt;&gt;20, "-&gt;",
        memstats.next_gc&gt;&gt;20, " MB)",
        " workers=", c.dedicatedMarkWorkersNeeded,
        "+", c.fractionalMarkWorkersNeeded, "\n")
} } setGCPhaseå‡½æ•°ä¼šä¿®æ”¹è¡¨ç¤ºå½“å‰GCé˜¶æ®µçš„å…¨å±€å˜é‡å’Œæ˜¯å¦å¼€å¯å†™å±éšœçš„å…¨å±€å˜é‡:
</code></pre></div></div>

<p>//go:nosplit
func setGCPhase(x uint32) {
    atomic.Store(&amp;gcphase, x)
    writeBarrier.needed = gcphase == _GCmark || gcphase == _GCmarktermination
    writeBarrier.enabled = writeBarrier.needed || writeBarrier.cgo
}
gcBgMarkPrepareå‡½æ•°ä¼šé‡ç½®åå°æ ‡è®°ä»»åŠ¡çš„è®¡æ•°:</p>

<p>// gcBgMarkPrepare sets up state for background marking.
// Mutator assists must not yet be enabled.
func gcBgMarkPrepare() {
    // Background marking will stop when the work queues are empty
    // and there are no more workers (note that, since this is
    // concurrent, this may be a transient state, but mark
    // termination will clean it up). Between background workers
    // and assists, we donâ€™t really know how many workers there
    // will be, so we pretend to have an arbitrarily large number
    // of workers, almost all of which are â€œwaitingâ€. While a
    // worker is working it decrements nwait. If nproc == nwait,
    // there are no workers.
    work.nproc = ^uint32(0)
    work.nwait = ^uint32(0)
}
gcMarkRootPrepareå‡½æ•°ä¼šè®¡ç®—æ‰«ææ ¹å¯¹è±¡çš„ä»»åŠ¡æ•°é‡:</p>

<p>// gcMarkRootPrepare queues root scanning jobs (stacks, globals, and
// some miscellany) and initializes scanning-related state.
//
// The caller must have call gcCopySpans().
//
// The world must be stopped.
//
//go:nowritebarrier
func gcMarkRootPrepare() {
    // é‡Šæ”¾mcacheä¸­çš„æ‰€æœ‰spançš„ä»»åŠ¡, åªåœ¨å®Œæˆæ ‡è®°é˜¶æ®µ(mark termination)ä¸­æ‰§è¡Œ
    if gcphase == _GCmarktermination {
        work.nFlushCacheRoots = int(gomaxprocs)
    } else {
        work.nFlushCacheRoots = 0
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// è®¡ç®—blockæ•°é‡çš„å‡½æ•°, rootBlockBytesæ˜¯256KB
// Compute how many data and BSS root blocks there are.
nBlocks := func(bytes uintptr) int {
    return int((bytes + rootBlockBytes - 1) / rootBlockBytes)
}

work.nDataRoots = 0
work.nBSSRoots = 0

// dataå’Œbssæ¯ä¸€è½®GCåªæ‰«æä¸€æ¬¡
// å¹¶è¡ŒGCä¸­ä¼šåœ¨åå°æ ‡è®°ä»»åŠ¡ä¸­æ‰«æ, å®Œæˆæ ‡è®°é˜¶æ®µ(mark termination)ä¸­ä¸æ‰«æ
// éå¹¶è¡ŒGCä¼šåœ¨å®Œæˆæ ‡è®°é˜¶æ®µ(mark termination)ä¸­æ‰«æ
// Only scan globals once per cycle; preferably concurrently.
if !work.markrootDone {
    // è®¡ç®—æ‰«æå¯è¯»å†™çš„å…¨å±€å˜é‡çš„ä»»åŠ¡æ•°é‡
    for _, datap := range activeModules() {
        nDataRoots := nBlocks(datap.edata - datap.data)
        if nDataRoots &gt; work.nDataRoots {
            work.nDataRoots = nDataRoots
        }
    }

    // è®¡ç®—æ‰«æåªè¯»çš„å…¨å±€å˜é‡çš„ä»»åŠ¡æ•°é‡
    for _, datap := range activeModules() {
        nBSSRoots := nBlocks(datap.ebss - datap.bss)
        if nBSSRoots &gt; work.nBSSRoots {
            work.nBSSRoots = nBSSRoots
        }
    }
}

// spanä¸­çš„finalizerå’Œå„ä¸ªGçš„æ ˆæ¯ä¸€è½®GCåªæ‰«æä¸€æ¬¡
// åŒä¸Š
if !work.markrootDone {
    // è®¡ç®—æ‰«æspanä¸­çš„finalizerçš„ä»»åŠ¡æ•°é‡
    // On the first markroot, we need to scan span roots.
    // In concurrent GC, this happens during concurrent
    // mark and we depend on addfinalizer to ensure the
    // above invariants for objects that get finalizers
    // after concurrent mark. In STW GC, this will happen
    // during mark termination.
    //
    // We're only interested in scanning the in-use spans,
    // which will all be swept at this point. More spans
    // may be added to this list during concurrent GC, but
    // we only care about spans that were allocated before
    // this mark phase.
    work.nSpanRoots = mheap_.sweepSpans[mheap_.sweepgen/2%2].numBlocks()

    // è®¡ç®—æ‰«æå„ä¸ªGçš„æ ˆçš„ä»»åŠ¡æ•°é‡
    // On the first markroot, we need to scan all Gs. Gs
    // may be created after this point, but it's okay that
    // we ignore them because they begin life without any
    // roots, so there's nothing to scan, and any roots
    // they create during the concurrent phase will be
    // scanned during mark termination. During mark
    // termination, allglen isn't changing, so we'll scan
    // all Gs.
    work.nStackRoots = int(atomic.Loaduintptr(&amp;allglen))
} else {
    // We've already scanned span roots and kept the scan
    // up-to-date during concurrent mark.
    work.nSpanRoots = 0

    // The hybrid barrier ensures that stacks can't
    // contain pointers to unmarked objects, so on the
    // second markroot, there's no need to scan stacks.
    work.nStackRoots = 0

    if debug.gcrescanstacks &gt; 0 {
        // Scan stacks anyway for debugging.
        work.nStackRoots = int(atomic.Loaduintptr(&amp;allglen))
    }
}

// è®¡ç®—æ€»ä»»åŠ¡æ•°é‡
// åå°æ ‡è®°ä»»åŠ¡ä¼šå¯¹markrootNextè¿›è¡ŒåŸå­é€’å¢, æ¥å†³å®šåšå“ªä¸ªä»»åŠ¡
// è¿™ç§ç”¨æ•°å€¼æ¥å®ç°é”è‡ªç”±é˜Ÿåˆ—çš„åŠæ³•æŒºèªæ˜çš„, å°½ç®¡googleå·¥ç¨‹å¸ˆè§‰å¾—ä¸å¥½(çœ‹åé¢markrootå‡½æ•°çš„åˆ†æ)
work.markrootNext = 0
work.markrootJobs = uint32(fixedRootCount + work.nFlushCacheRoots + work.nDataRoots + work.nBSSRoots + work.nSpanRoots + work.nStackRoots) } gcMarkTinyAllocså‡½æ•°ä¼šæ ‡è®°æ‰€æœ‰tiny allocç­‰å¾…åˆå¹¶çš„å¯¹è±¡:
</code></pre></div></div>

<p>// gcMarkTinyAllocs greys all active tiny alloc blocks.
//
// The world must be stopped.
func gcMarkTinyAllocs() {
    for _, p := range &amp;allp {
        if p == nil || p.status == _Pdead {
            break
        }
        c := p.mcache
        if c == nil || c.tiny == 0 {
            continue
        }
        // æ ‡è®°å„ä¸ªPä¸­çš„mcacheä¸­çš„tiny
        // åœ¨ä¸Šé¢çš„mallocgcå‡½æ•°ä¸­å¯ä»¥çœ‹åˆ°tinyæ˜¯å½“å‰ç­‰å¾…åˆå¹¶çš„å¯¹è±¡
        _, hbits, span, objIndex := heapBitsForObject(c.tiny, 0, 0)
        gcw := &amp;p.gcw
        // æ ‡è®°ä¸€ä¸ªå¯¹è±¡å­˜æ´», å¹¶æŠŠå®ƒåŠ åˆ°æ ‡è®°é˜Ÿåˆ—(è¯¥å¯¹è±¡å˜ä¸ºç°è‰²)
        greyobject(c.tiny, 0, 0, hbits, span, gcw, objIndex)
        // gcBlackenPromptlyå˜é‡è¡¨ç¤ºå½“å‰æ˜¯å¦ç¦æ­¢æœ¬åœ°é˜Ÿåˆ—, å¦‚æœå·²ç¦æ­¢åˆ™æŠŠæ ‡è®°ä»»åŠ¡flushåˆ°å…¨å±€é˜Ÿåˆ—
        if gcBlackenPromptly {
            gcw.dispose()
        }
    }
}
startTheWorldWithSemaå‡½æ•°ä¼šé‡æ–°å¯åŠ¨ä¸–ç•Œ:</p>

<p>func startTheWorldWithSema() {
    <em>g</em> := getg()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// ç¦æ­¢Gè¢«æŠ¢å 
_g_.m.locks++        // disable preemption because it can be holding p in a local var

// åˆ¤æ–­æ”¶åˆ°çš„ç½‘ç»œäº‹ä»¶(fdå¯è¯»å¯å†™æˆ–é”™è¯¯)å¹¶æ·»åŠ å¯¹åº”çš„Gåˆ°å¾…è¿è¡Œé˜Ÿåˆ—
gp := netpoll(false) // non-blocking
injectglist(gp)

// åˆ¤æ–­æ˜¯å¦è¦å¯åŠ¨gc helper
add := needaddgcproc()
lock(&amp;sched.lock)

// å¦‚æœè¦æ±‚æ”¹å˜gomaxprocsåˆ™è°ƒæ•´Pçš„æ•°é‡
// procresizeä¼šè¿”å›æœ‰å¯è¿è¡Œä»»åŠ¡çš„Pçš„é“¾è¡¨
procs := gomaxprocs
if newprocs != 0 {
    procs = newprocs
    newprocs = 0
}
p1 := procresize(procs)

// å–æ¶ˆGCç­‰å¾…æ ‡è®°
sched.gcwaiting = 0

// å¦‚æœsysmonåœ¨ç­‰å¾…åˆ™å”¤é†’å®ƒ
if sched.sysmonwait != 0 {
    sched.sysmonwait = 0
    notewakeup(&amp;sched.sysmonnote)
}
unlock(&amp;sched.lock)

// å”¤é†’æœ‰å¯è¿è¡Œä»»åŠ¡çš„P
for p1 != nil {
    p := p1
    p1 = p1.link.ptr()
    if p.m != 0 {
        mp := p.m.ptr()
        p.m = 0
        if mp.nextp != 0 {
            throw("startTheWorld: inconsistent mp-&gt;nextp")
        }
        mp.nextp.set(p)
        notewakeup(&amp;mp.park)
    } else {
        // Start M to run P.  Do not start another M below.
        newm(nil, p)
        add = false
    }
}

// å¦‚æœæœ‰ç©ºé—²çš„Pï¼Œå¹¶ä¸”æ²¡æœ‰è‡ªæ—‹ä¸­çš„Måˆ™å”¤é†’æˆ–è€…åˆ›å»ºä¸€ä¸ªM
// Wakeup an additional proc in case we have excessive runnable goroutines
// in local queues or in the global queue. If we don't, the proc will park itself.
// If we have lots of excessive work, resetspinning will unpark additional procs as necessary.
if atomic.Load(&amp;sched.npidle) != 0 &amp;&amp; atomic.Load(&amp;sched.nmspinning) == 0 {
    wakep()
}

// å¯åŠ¨gc helper
if add {
    // If GC could have used another helper proc, start one now,
    // in the hope that it will be available next time.
    // It would have been even better to start it before the collection,
    // but doing so requires allocating memory, so it's tricky to
    // coordinate. This lazy approach works out in practice:
    // we don't mind if the first couple gc rounds don't have quite
    // the maximum number of procs.
    newm(mhelpgc, nil)
}

// å…è®¸Gè¢«æŠ¢å 
_g_.m.locks--

// å¦‚æœå½“å‰Gè¦æ±‚è¢«æŠ¢å åˆ™é‡æ–°å°è¯•
if _g_.m.locks == 0 &amp;&amp; _g_.preempt { // restore the preemption request in case we've cleared it in newstack
    _g_.stackguard0 = stackPreempt
} } é‡å¯ä¸–ç•Œåå„ä¸ªMä¼šé‡æ–°å¼€å§‹è°ƒåº¦, è°ƒåº¦æ—¶ä¼šä¼˜å…ˆä½¿ç”¨ä¸Šé¢æåˆ°çš„findRunnableGCWorkerå‡½æ•°æŸ¥æ‰¾ä»»åŠ¡, ä¹‹åå°±æœ‰å¤§çº¦25%çš„Pè¿è¡Œåå°æ ‡è®°ä»»åŠ¡. åå°æ ‡è®°ä»»åŠ¡çš„å‡½æ•°æ˜¯gcBgMarkWorker:
</code></pre></div></div>

<p>func gcBgMarkWorker(<em>p</em> *p) {
    gp := getg()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// ç”¨äºä¼‘çœ åé‡æ–°è·å–Pçš„æ„é€ ä½“
type parkInfo struct {
    m      muintptr // Release this m on park.
    attach puintptr // If non-nil, attach to this p on park.
}
// We pass park to a gopark unlock function, so it can't be on
// the stack (see gopark). Prevent deadlock from recursively
// starting GC by disabling preemption.
gp.m.preemptoff = "GC worker init"
park := new(parkInfo)
gp.m.preemptoff = ""

// è®¾ç½®å½“å‰çš„Må¹¶ç¦æ­¢æŠ¢å 
park.m.set(acquirem())
// è®¾ç½®å½“å‰çš„P(éœ€è¦å…³è”åˆ°çš„P)
park.attach.set(_p_)

// é€šçŸ¥gcBgMarkStartWorkerså¯ä»¥ç»§ç»­å¤„ç†
// Inform gcBgMarkStartWorkers that this worker is ready.
// After this point, the background mark worker is scheduled
// cooperatively by gcController.findRunnable. Hence, it must
// never be preempted, as this would put it into _Grunnable
// and put it on a run queue. Instead, when the preempt flag
// is set, this puts itself into _Gwaiting to be woken up by
// gcController.findRunnable at the appropriate time.
notewakeup(&amp;work.bgMarkReady)

for {
    // è®©å½“å‰Gè¿›å…¥ä¼‘çœ 
    // Go to sleep until woken by gcController.findRunnable.
    // We can't releasem yet since even the call to gopark
    // may be preempted.
    gopark(func(g *g, parkp unsafe.Pointer) bool {
        park := (*parkInfo)(parkp)
        
        // é‡æ–°å…è®¸æŠ¢å 
        // The worker G is no longer running, so it's
        // now safe to allow preemption.
        releasem(park.m.ptr())
        
        // è®¾ç½®å…³è”çš„P
        // æŠŠå½“å‰çš„Gè®¾åˆ°Pçš„gcBgMarkWorkeræˆå‘˜, ä¸‹æ¬¡findRunnableGCWorkerä¼šä½¿ç”¨
        // è®¾ç½®å¤±è´¥æ—¶ä¸ä¼‘çœ 
        // If the worker isn't attached to its P,
        // attach now. During initialization and after
        // a phase change, the worker may have been
        // running on a different P. As soon as we
        // attach, the owner P may schedule the
        // worker, so this must be done after the G is
        // stopped.
        if park.attach != 0 {
            p := park.attach.ptr()
            park.attach.set(nil)
            // cas the worker because we may be
            // racing with a new worker starting
            // on this P.
            if !p.gcBgMarkWorker.cas(0, guintptr(unsafe.Pointer(g))) {
                // The P got a new worker.
                // Exit this worker.
                return false
            }
        }
        return true
    }, unsafe.Pointer(park), "GC worker (idle)", traceEvGoBlock, 0)
    
    // æ£€æŸ¥Pçš„gcBgMarkWorkeræ˜¯å¦å’Œå½“å‰çš„Gä¸€è‡´, ä¸ä¸€è‡´æ—¶ç»“æŸå½“å‰çš„ä»»åŠ¡
    // Loop until the P dies and disassociates this
    // worker (the P may later be reused, in which case
    // it will get a new worker) or we failed to associate.
    if _p_.gcBgMarkWorker.ptr() != gp {
        break
    }
    
    // ç¦æ­¢Gè¢«æŠ¢å 
    // Disable preemption so we can use the gcw. If the
    // scheduler wants to preempt us, we'll stop draining,
    // dispose the gcw, and then preempt.
    park.m.set(acquirem())
    
    if gcBlackenEnabled == 0 {
        throw("gcBgMarkWorker: blackening not enabled")
    }
    
    // è®°å½•å¼€å§‹æ—¶é—´
    startTime := nanotime()
    
    decnwait := atomic.Xadd(&amp;work.nwait, -1)
    if decnwait == work.nproc {
        println("runtime: work.nwait=", decnwait, "work.nproc=", work.nproc)
        throw("work.nwait was &gt; work.nproc")
    }
    
    // åˆ‡æ¢åˆ°g0è¿è¡Œ
    systemstack(func() {
        // è®¾ç½®Gçš„çŠ¶æ€ä¸ºç­‰å¾…ä¸­è¿™æ ·å®ƒçš„æ ˆå¯ä»¥è¢«æ‰«æ(ä¸¤ä¸ªåå°æ ‡è®°ä»»åŠ¡å¯ä»¥äº’ç›¸æ‰«æå¯¹æ–¹çš„æ ˆ)
        // Mark our goroutine preemptible so its stack
        // can be scanned. This lets two mark workers
        // scan each other (otherwise, they would
        // deadlock). We must not modify anything on
        // the G stack. However, stack shrinking is
        // disabled for mark workers, so it is safe to
        // read from the G stack.
        casgstatus(gp, _Grunning, _Gwaiting)
        
        // åˆ¤æ–­åå°æ ‡è®°ä»»åŠ¡çš„æ¨¡å¼
        switch _p_.gcMarkWorkerMode {
        default:
            throw("gcBgMarkWorker: unexpected gcMarkWorkerMode")
        case gcMarkWorkerDedicatedMode:
            // è¿™ä¸ªæ¨¡å¼ä¸‹Påº”è¯¥ä¸“å¿ƒæ‰§è¡Œæ ‡è®°
            // æ‰§è¡Œæ ‡è®°, ç›´åˆ°è¢«æŠ¢å , å¹¶ä¸”éœ€è¦è®¡ç®—åå°çš„æ‰«æé‡æ¥å‡å°‘è¾…åŠ©GCå’Œå”¤é†’ç­‰å¾…ä¸­çš„G
            gcDrain(&amp;_p_.gcw, gcDrainUntilPreempt|gcDrainFlushBgCredit)
            // è¢«æŠ¢å æ—¶æŠŠæœ¬åœ°è¿è¡Œé˜Ÿåˆ—ä¸­çš„æ‰€æœ‰Géƒ½è¸¢åˆ°å…¨å±€è¿è¡Œé˜Ÿåˆ—
            if gp.preempt {
                // We were preempted. This is
                // a useful signal to kick
                // everything out of the run
                // queue so it can run
                // somewhere else.
                lock(&amp;sched.lock)
                for {
                    gp, _ := runqget(_p_)
                    if gp == nil {
                        break
                    }
                    globrunqput(gp)
                }
                unlock(&amp;sched.lock)
            }
            // ç»§ç»­æ‰§è¡Œæ ‡è®°, ç›´åˆ°æ— æ›´å¤šä»»åŠ¡, å¹¶ä¸”éœ€è¦è®¡ç®—åå°çš„æ‰«æé‡æ¥å‡å°‘è¾…åŠ©GCå’Œå”¤é†’ç­‰å¾…ä¸­çš„G
            // Go back to draining, this time
            // without preemption.
            gcDrain(&amp;_p_.gcw, gcDrainNoBlock|gcDrainFlushBgCredit)
        case gcMarkWorkerFractionalMode:
            // è¿™ä¸ªæ¨¡å¼ä¸‹Påº”è¯¥é€‚å½“æ‰§è¡Œæ ‡è®°
            // æ‰§è¡Œæ ‡è®°, ç›´åˆ°è¢«æŠ¢å , å¹¶ä¸”éœ€è¦è®¡ç®—åå°çš„æ‰«æé‡æ¥å‡å°‘è¾…åŠ©GCå’Œå”¤é†’ç­‰å¾…ä¸­çš„G
            gcDrain(&amp;_p_.gcw, gcDrainUntilPreempt|gcDrainFlushBgCredit)
        case gcMarkWorkerIdleMode:
            // è¿™ä¸ªæ¨¡å¼ä¸‹Påªåœ¨ç©ºé—²æ—¶æ‰§è¡Œæ ‡è®°
            // æ‰§è¡Œæ ‡è®°, ç›´åˆ°è¢«æŠ¢å æˆ–è€…è¾¾åˆ°ä¸€å®šçš„é‡, å¹¶ä¸”éœ€è¦è®¡ç®—åå°çš„æ‰«æé‡æ¥å‡å°‘è¾…åŠ©GCå’Œå”¤é†’ç­‰å¾…ä¸­çš„G
            gcDrain(&amp;_p_.gcw, gcDrainIdle|gcDrainUntilPreempt|gcDrainFlushBgCredit)
        }
        
        // æ¢å¤Gçš„çŠ¶æ€åˆ°è¿è¡Œä¸­
        casgstatus(gp, _Gwaiting, _Grunning)
    })
    
    // å¦‚æœæ ‡è®°äº†ç¦æ­¢æœ¬åœ°æ ‡è®°é˜Ÿåˆ—åˆ™flushåˆ°å…¨å±€æ ‡è®°é˜Ÿåˆ—
    // If we are nearing the end of mark, dispose
    // of the cache promptly. We must do this
    // before signaling that we're no longer
    // working so that other workers can't observe
    // no workers and no work while we have this
    // cached, and before we compute done.
    if gcBlackenPromptly {
        _p_.gcw.dispose()
    }
    
    // ç´¯åŠ æ‰€ç”¨æ—¶é—´
    // Account for time.
    duration := nanotime() - startTime
    switch _p_.gcMarkWorkerMode {
    case gcMarkWorkerDedicatedMode:
        atomic.Xaddint64(&amp;gcController.dedicatedMarkTime, duration)
        atomic.Xaddint64(&amp;gcController.dedicatedMarkWorkersNeeded, 1)
    case gcMarkWorkerFractionalMode:
        atomic.Xaddint64(&amp;gcController.fractionalMarkTime, duration)
        atomic.Xaddint64(&amp;gcController.fractionalMarkWorkersNeeded, 1)
    case gcMarkWorkerIdleMode:
        atomic.Xaddint64(&amp;gcController.idleMarkTime, duration)
    }
    
    // Was this the last worker and did we run out
    // of work?
    incnwait := atomic.Xadd(&amp;work.nwait, +1)
    if incnwait &gt; work.nproc {
        println("runtime: p.gcMarkWorkerMode=", _p_.gcMarkWorkerMode,
            "work.nwait=", incnwait, "work.nproc=", work.nproc)
        throw("work.nwait &gt; work.nproc")
    }
    
    // åˆ¤æ–­æ˜¯å¦æ‰€æœ‰åå°æ ‡è®°ä»»åŠ¡éƒ½å®Œæˆ, å¹¶ä¸”æ²¡æœ‰æ›´å¤šçš„ä»»åŠ¡
    // If this worker reached a background mark completion
    // point, signal the main GC goroutine.
    if incnwait == work.nproc &amp;&amp; !gcMarkWorkAvailable(nil) {
        // å–æ¶ˆå’ŒPçš„å…³è”
        // Make this G preemptible and disassociate it
        // as the worker for this P so
        // findRunnableGCWorker doesn't try to
        // schedule it.
        _p_.gcBgMarkWorker.set(nil)
        
        // å…è®¸Gè¢«æŠ¢å 
        releasem(park.m.ptr())
        
        // å‡†å¤‡è¿›å…¥å®Œæˆæ ‡è®°é˜¶æ®µ
        gcMarkDone()
        
        // ä¼‘çœ ä¹‹å‰ä¼šé‡æ–°å…³è”P
        // å› ä¸ºä¸Šé¢å…è®¸è¢«æŠ¢å , åˆ°è¿™é‡Œçš„æ—¶å€™å¯èƒ½å°±ä¼šå˜æˆå…¶ä»–P
        // å¦‚æœé‡æ–°å…³è”På¤±è´¥åˆ™è¿™ä¸ªä»»åŠ¡ä¼šç»“æŸ
        // Disable preemption and prepare to reattach
        // to the P.
        //
        // We may be running on a different P at this
        // point, so we can't reattach until this G is
        // parked.
        park.m.set(acquirem())
        park.attach.set(_p_)
    }
} } gcDrainå‡½æ•°ç”¨äºæ‰§è¡Œæ ‡è®°:
</code></pre></div></div>

<p>// gcDrain scans roots and objects in work buffers, blackening grey
// objects until all roots and work buffers have been drained.
//
// If flags&amp;gcDrainUntilPreempt != 0, gcDrain returns when g.preempt
// is set. This implies gcDrainNoBlock.
//
// If flags&amp;gcDrainIdle != 0, gcDrain returns when there is other work
// to do. This implies gcDrainNoBlock.
//
// If flags&amp;gcDrainNoBlock != 0, gcDrain returns as soon as it is
// unable to get more work. Otherwise, it will block until all
// blocking calls are blocked in gcDrain.
//
// If flags&amp;gcDrainFlushBgCredit != 0, gcDrain flushes scan work
// credit to gcController.bgScanCredit every gcCreditSlack units of
// scan work.
//
//go:nowritebarrier
func gcDrain(gcw *gcWork, flags gcDrainFlags) {
    if !writeBarrier.needed {
        throw(â€œgcDrain phase incorrectâ€)
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gp := getg().m.curg

// çœ‹åˆ°æŠ¢å æ ‡å¿—æ—¶æ˜¯å¦è¦è¿”å›
preemptible := flags&amp;gcDrainUntilPreempt != 0

// æ²¡æœ‰ä»»åŠ¡æ—¶æ˜¯å¦è¦ç­‰å¾…ä»»åŠ¡
blocking := flags&amp;(gcDrainUntilPreempt|gcDrainIdle|gcDrainNoBlock) == 0

// æ˜¯å¦è®¡ç®—åå°çš„æ‰«æé‡æ¥å‡å°‘è¾…åŠ©GCå’Œå”¤é†’ç­‰å¾…ä¸­çš„G
flushBgCredit := flags&amp;gcDrainFlushBgCredit != 0

// æ˜¯å¦åªæ‰§è¡Œä¸€å®šé‡çš„å·¥ä½œ
idle := flags&amp;gcDrainIdle != 0

// è®°å½•åˆå§‹çš„å·²æ‰«ææ•°é‡
initScanWork := gcw.scanWork

// æ‰«æidleCheckThreshold(100000)ä¸ªå¯¹è±¡ä»¥åæ£€æŸ¥æ˜¯å¦è¦è¿”å›
// idleCheck is the scan work at which to perform the next
// idle check with the scheduler.
idleCheck := initScanWork + idleCheckThreshold

// å¦‚æœæ ¹å¯¹è±¡æœªæ‰«æå®Œ, åˆ™å…ˆæ‰«ææ ¹å¯¹è±¡
// Drain root marking jobs.
if work.markrootNext &lt; work.markrootJobs {
    // å¦‚æœæ ‡è®°äº†preemptible, å¾ªç¯ç›´åˆ°è¢«æŠ¢å 
    for !(preemptible &amp;&amp; gp.preempt) {
        // ä»æ ¹å¯¹è±¡æ‰«æé˜Ÿåˆ—å–å‡ºä¸€ä¸ªå€¼(åŸå­é€’å¢)
        job := atomic.Xadd(&amp;work.markrootNext, +1) - 1
        if job &gt;= work.markrootJobs {
            break
        }
        // æ‰§è¡Œæ ¹å¯¹è±¡æ‰«æå·¥ä½œ
        markroot(gcw, job)
        // å¦‚æœæ˜¯idleæ¨¡å¼å¹¶ä¸”æœ‰å…¶ä»–å·¥ä½œ, åˆ™è¿”å›
        if idle &amp;&amp; pollWork() {
            goto done
        }
    }
}

// æ ¹å¯¹è±¡å·²ç»åœ¨æ ‡è®°é˜Ÿåˆ—ä¸­, æ¶ˆè´¹æ ‡è®°é˜Ÿåˆ—
// å¦‚æœæ ‡è®°äº†preemptible, å¾ªç¯ç›´åˆ°è¢«æŠ¢å 
// Drain heap marking jobs.
for !(preemptible &amp;&amp; gp.preempt) {
    // å¦‚æœå…¨å±€æ ‡è®°é˜Ÿåˆ—ä¸ºç©º, æŠŠæœ¬åœ°æ ‡è®°é˜Ÿåˆ—çš„ä¸€éƒ¨åˆ†å·¥ä½œåˆ†è¿‡å»
    // (å¦‚æœwbuf2ä¸ä¸ºç©ºåˆ™ç§»åŠ¨wbuf2è¿‡å», å¦åˆ™ç§»åŠ¨wbuf1çš„ä¸€åŠè¿‡å»)
    // Try to keep work available on the global queue. We used to
    // check if there were waiting workers, but it's better to
    // just keep work available than to make workers wait. In the
    // worst case, we'll do O(log(_WorkbufSize)) unnecessary
    // balances.
    if work.full == 0 {
        gcw.balance()
    }
    
    // ä»æœ¬åœ°æ ‡è®°é˜Ÿåˆ—ä¸­è·å–å¯¹è±¡, è·å–ä¸åˆ°åˆ™ä»å…¨å±€æ ‡è®°é˜Ÿåˆ—è·å–
    var b uintptr
    if blocking {
        // é˜»å¡è·å–
        b = gcw.get()
    } else {
        // éé˜»å¡è·å–
        b = gcw.tryGetFast()
        if b == 0 {
            b = gcw.tryGet()
        }
    }
    
    // è·å–ä¸åˆ°å¯¹è±¡, æ ‡è®°é˜Ÿåˆ—å·²ä¸ºç©º, è·³å‡ºå¾ªç¯
    if b == 0 {
        // work barrier reached or tryGet failed.
        break
    }
    
    // æ‰«æè·å–åˆ°çš„å¯¹è±¡
    scanobject(b, gcw)
    
    // å¦‚æœå·²ç»æ‰«æäº†ä¸€å®šæ•°é‡çš„å¯¹è±¡(gcCreditSlackçš„å€¼æ˜¯2000)
    // Flush background scan work credit to the global
    // account if we've accumulated enough locally so
    // mutator assists can draw on it.
    if gcw.scanWork &gt;= gcCreditSlack {
        // æŠŠæ‰«æçš„å¯¹è±¡æ•°é‡æ·»åŠ åˆ°å…¨å±€
        atomic.Xaddint64(&amp;gcController.scanWork, gcw.scanWork)
        // å‡å°‘è¾…åŠ©GCçš„å·¥ä½œé‡å’Œå”¤é†’ç­‰å¾…ä¸­çš„G
        if flushBgCredit {
            gcFlushBgCredit(gcw.scanWork - initScanWork)
            initScanWork = 0
        }
        idleCheck -= gcw.scanWork
        gcw.scanWork = 0
        
        // å¦‚æœæ˜¯idleæ¨¡å¼ä¸”è¾¾åˆ°äº†æ£€æŸ¥çš„æ‰«æé‡, åˆ™æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ä»»åŠ¡(G), å¦‚æœæœ‰åˆ™è·³å‡ºå¾ªç¯
        if idle &amp;&amp; idleCheck &lt;= 0 {
            idleCheck += idleCheckThreshold
            if pollWork() {
                break
            }
        }
    }
}

// In blocking mode, write barriers are not allowed after this
// point because we must preserve the condition that the work
// buffers are empty.
</code></pre></div></div>

<p>done:
    // æŠŠæ‰«æçš„å¯¹è±¡æ•°é‡æ·»åŠ åˆ°å…¨å±€
    // Flush remaining scan work credit.
    if gcw.scanWork &gt; 0 {
        atomic.Xaddint64(&amp;gcController.scanWork, gcw.scanWork)
        // å‡å°‘è¾…åŠ©GCçš„å·¥ä½œé‡å’Œå”¤é†’ç­‰å¾…ä¸­çš„G
        if flushBgCredit {
            gcFlushBgCredit(gcw.scanWork - initScanWork)
        }
        gcw.scanWork = 0
    }
}
markrootå‡½æ•°ç”¨äºæ‰§è¡Œæ ¹å¯¹è±¡æ‰«æå·¥ä½œ:</p>

<p>// markroot scans the iâ€™th root.
//
// Preemption must be disabled (because this uses a gcWork).
//
// nowritebarrier is only advisory here.
//
//go:nowritebarrier
func markroot(gcw *gcWork, i uint32) {
    // åˆ¤æ–­å–å‡ºçš„æ•°å€¼å¯¹åº”å“ªç§ä»»åŠ¡
    // (googleçš„å·¥ç¨‹å¸ˆè§‰å¾—è¿™ç§åŠæ³•å¯ç¬‘)
    // TODO(austin): This is a bit ridiculous. Compute and store
    // the bases in gcMarkRootPrepare instead of the counts.
    baseFlushCache := uint32(fixedRootCount)
    baseData := baseFlushCache + uint32(work.nFlushCacheRoots)
    baseBSS := baseData + uint32(work.nDataRoots)
    baseSpans := baseBSS + uint32(work.nBSSRoots)
    baseStacks := baseSpans + uint32(work.nSpanRoots)
    end := baseStacks + uint32(work.nStackRoots)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Note: if you add a case here, please also update heapdump.go:dumproots.
switch {
// é‡Šæ”¾mcacheä¸­çš„æ‰€æœ‰span, è¦æ±‚STW
case baseFlushCache &lt;= i &amp;&amp; i &lt; baseData:
    flushmcache(int(i - baseFlushCache))

// æ‰«æå¯è¯»å†™çš„å…¨å±€å˜é‡
// è¿™é‡Œåªä¼šæ‰«æiå¯¹åº”çš„block, æ‰«ææ—¶ä¼ å…¥åŒ…å«å“ªé‡Œæœ‰æŒ‡é’ˆçš„bitmapæ•°æ®
case baseData &lt;= i &amp;&amp; i &lt; baseBSS:
    for _, datap := range activeModules() {
        markrootBlock(datap.data, datap.edata-datap.data, datap.gcdatamask.bytedata, gcw, int(i-baseData))
    }

// æ‰«æåªè¯»çš„å…¨å±€å˜é‡
// è¿™é‡Œåªä¼šæ‰«æiå¯¹åº”çš„block, æ‰«ææ—¶ä¼ å…¥åŒ…å«å“ªé‡Œæœ‰æŒ‡é’ˆçš„bitmapæ•°æ®
case baseBSS &lt;= i &amp;&amp; i &lt; baseSpans:
    for _, datap := range activeModules() {
        markrootBlock(datap.bss, datap.ebss-datap.bss, datap.gcbssmask.bytedata, gcw, int(i-baseBSS))
    }

// æ‰«æææ„å™¨é˜Ÿåˆ—
case i == fixedRootFinalizers:
    // Only do this once per GC cycle since we don't call
    // queuefinalizer during marking.
    if work.markrootDone {
        break
    }
    for fb := allfin; fb != nil; fb = fb.alllink {
        cnt := uintptr(atomic.Load(&amp;fb.cnt))
        scanblock(uintptr(unsafe.Pointer(&amp;fb.fin[0])), cnt*unsafe.Sizeof(fb.fin[0]), &amp;finptrmask[0], gcw)
    }

// é‡Šæ”¾å·²ä¸­æ­¢çš„Gçš„æ ˆ
case i == fixedRootFreeGStacks:
    // Only do this once per GC cycle; preferably
    // concurrently.
    if !work.markrootDone {
        // Switch to the system stack so we can call
        // stackfree.
        systemstack(markrootFreeGStacks)
    }

// æ‰«æå„ä¸ªspanä¸­ç‰¹æ®Šå¯¹è±¡(ææ„å™¨åˆ—è¡¨)
case baseSpans &lt;= i &amp;&amp; i &lt; baseStacks:
    // mark MSpan.specials
    markrootSpans(gcw, int(i-baseSpans))

// æ‰«æå„ä¸ªGçš„æ ˆ
default:
    // è·å–éœ€è¦æ‰«æçš„G
    // the rest is scanning goroutine stacks
    var gp *g
    if baseStacks &lt;= i &amp;&amp; i &lt; end {
        gp = allgs[i-baseStacks]
    } else {
        throw("markroot: bad index")
    }

    // è®°å½•ç­‰å¾…å¼€å§‹çš„æ—¶é—´
    // remember when we've first observed the G blocked
    // needed only to output in traceback
    status := readgstatus(gp) // We are not in a scan state
    if (status == _Gwaiting || status == _Gsyscall) &amp;&amp; gp.waitsince == 0 {
        gp.waitsince = work.tstart
    }

    // åˆ‡æ¢åˆ°g0è¿è¡Œ(æœ‰å¯èƒ½ä¼šæ‰«åˆ°è‡ªå·±çš„æ ˆ)
    // scang must be done on the system stack in case
    // we're trying to scan our own stack.
    systemstack(func() {
        // åˆ¤æ–­æ‰«æçš„æ ˆæ˜¯å¦è‡ªå·±çš„
        // If this is a self-scan, put the user G in
        // _Gwaiting to prevent self-deadlock. It may
        // already be in _Gwaiting if this is a mark
        // worker or we're in mark termination.
        userG := getg().m.curg
        selfScan := gp == userG &amp;&amp; readgstatus(userG) == _Grunning
        
        // å¦‚æœæ­£åœ¨æ‰«æè‡ªå·±çš„æ ˆåˆ™åˆ‡æ¢çŠ¶æ€åˆ°ç­‰å¾…ä¸­é˜²æ­¢æ­»é”
        if selfScan {
            casgstatus(userG, _Grunning, _Gwaiting)
            userG.waitreason = "garbage collection scan"
        }
        
        // æ‰«æGçš„æ ˆ
        // TODO: scang blocks until gp's stack has
        // been scanned, which may take a while for
        // running goroutines. Consider doing this in
        // two phases where the first is non-blocking:
        // we scan the stacks we can and ask running
        // goroutines to scan themselves; and the
        // second blocks.
        scang(gp, gcw)
        
        // å¦‚æœæ­£åœ¨æ‰«æè‡ªå·±çš„æ ˆåˆ™æŠŠçŠ¶æ€åˆ‡æ¢å›è¿è¡Œä¸­
        if selfScan {
            casgstatus(userG, _Gwaiting, _Grunning)
        }
    })
} } scangå‡½æ•°è´Ÿè´£æ‰«æGçš„æ ˆ:
</code></pre></div></div>

<p>// scang blocks until gpâ€™s stack has been scanned.
// It might be scanned by scang or it might be scanned by the goroutine itself.
// Either way, the stack scan has completed when scang returns.
func scang(gp *g, gcw *gcWork) {
    // Invariant; we (the caller, markroot for a specific goroutine) own gp.gcscandone.
    // Nothing is racing with us now, but gcscandone might be set to true left over
    // from an earlier round of stack scanning (we scan twice per GC).
    // We use gcscandone to record whether the scan has been done during this round.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// æ ‡è®°æ‰«ææœªå®Œæˆ
gp.gcscandone = false

// See http://golang.org/cl/21503 for justification of the yield delay.
const yieldDelay = 10 * 1000
var nextYield int64

// å¾ªç¯ç›´åˆ°æ‰«æå®Œæˆ
// Endeavor to get gcscandone set to true,
// either by doing the stack scan ourselves or by coercing gp to scan itself.
// gp.gcscandone can transition from false to true when we're not looking
// (if we asked for preemption), so any time we lock the status using
// castogscanstatus we have to double-check that the scan is still not done. loop:
for i := 0; !gp.gcscandone; i++ {
    // åˆ¤æ–­Gçš„å½“å‰çŠ¶æ€
    switch s := readgstatus(gp); s {
    default:
        dumpgstatus(gp)
        throw("stopg: invalid status")

    // Gå·²ä¸­æ­¢, ä¸éœ€è¦æ‰«æå®ƒ
    case _Gdead:
        // No stack.
        gp.gcscandone = true
        break loop

    // Gçš„æ ˆæ­£åœ¨æ‰©å±•, ä¸‹ä¸€è½®é‡è¯•
    case _Gcopystack:
    // Stack being switched. Go around again.

    // Gä¸æ˜¯è¿è¡Œä¸­, é¦–å…ˆéœ€è¦é˜²æ­¢å®ƒè¿è¡Œ
    case _Grunnable, _Gsyscall, _Gwaiting:
        // Claim goroutine by setting scan bit.
        // Racing with execution or readying of gp.
        // The scan bit keeps them from running
        // the goroutine until we're done.
        if castogscanstatus(gp, s, s|_Gscan) {
            // åŸå­åˆ‡æ¢çŠ¶æ€æˆåŠŸæ—¶æ‰«æå®ƒçš„æ ˆ
            if !gp.gcscandone {
                scanstack(gp, gcw)
                gp.gcscandone = true
            }
            // æ¢å¤Gçš„çŠ¶æ€, å¹¶è·³å‡ºå¾ªç¯
            restartg(gp)
            break loop
        }

    // Gæ­£åœ¨æ‰«æå®ƒè‡ªå·±, ç­‰å¾…æ‰«æå®Œæ¯•
    case _Gscanwaiting:
    // newstack is doing a scan for us right now. Wait.

    // Gæ­£åœ¨è¿è¡Œ
    case _Grunning:
        // Goroutine running. Try to preempt execution so it can scan itself.
        // The preemption handler (in newstack) does the actual scan.

        // å¦‚æœå·²ç»æœ‰æŠ¢å è¯·æ±‚, åˆ™æŠ¢å æˆåŠŸæ—¶ä¼šå¸®æˆ‘ä»¬å¤„ç†
        // Optimization: if there is already a pending preemption request
        // (from the previous loop iteration), don't bother with the atomics.
        if gp.preemptscan &amp;&amp; gp.preempt &amp;&amp; gp.stackguard0 == stackPreempt {
            break
        }

        // æŠ¢å G, æŠ¢å æˆåŠŸæ—¶Gä¼šæ‰«æå®ƒè‡ªå·±
        // Ask for preemption and self scan.
        if castogscanstatus(gp, _Grunning, _Gscanrunning) {
            if !gp.gcscandone {
                gp.preemptscan = true
                gp.preempt = true
                gp.stackguard0 = stackPreempt
            }
            casfrom_Gscanstatus(gp, _Gscanrunning, _Grunning)
        }
    }

    // ç¬¬ä¸€è½®ä¼‘çœ 10æ¯«ç§’, ç¬¬äºŒè½®ä¼‘çœ 5æ¯«ç§’
    if i == 0 {
        nextYield = nanotime() + yieldDelay
    }
    if nanotime() &lt; nextYield {
        procyield(10)
    } else {
        osyield()
        nextYield = nanotime() + yieldDelay/2
    }
}

// æ‰«æå®Œæˆ, å–æ¶ˆæŠ¢å æ‰«æçš„è¯·æ±‚
gp.preemptscan = false // cancel scan request if no longer needed } è®¾ç½®preemptscanå, åœ¨æŠ¢å GæˆåŠŸæ—¶ä¼šè°ƒç”¨scanstackæ‰«æå®ƒè‡ªå·±çš„æ ˆ, å…·ä½“ä»£ç åœ¨è¿™é‡Œ. æ‰«ææ ˆç”¨çš„å‡½æ•°æ˜¯scanstack:
</code></pre></div></div>

<p>// scanstack scans gpâ€™s stack, greying all pointers found on the stack.
//
// scanstack is marked go:systemstack because it must not be preempted
// while using a workbuf.
//
//go:nowritebarrier
//go:systemstack
func scanstack(gp *g, gcw *gcWork) {
    if gp.gcscanvalid {
        return
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if readgstatus(gp)&amp;_Gscan == 0 {
    print("runtime:scanstack: gp=", gp, ", goid=", gp.goid, ", gp-&gt;atomicstatus=", hex(readgstatus(gp)), "\n")
    throw("scanstack - bad status")
}

switch readgstatus(gp) &amp;^ _Gscan {
default:
    print("runtime: gp=", gp, ", goid=", gp.goid, ", gp-&gt;atomicstatus=", readgstatus(gp), "\n")
    throw("mark - bad status")
case _Gdead:
    return
case _Grunning:
    print("runtime: gp=", gp, ", goid=", gp.goid, ", gp-&gt;atomicstatus=", readgstatus(gp), "\n")
    throw("scanstack: goroutine not stopped")
case _Grunnable, _Gsyscall, _Gwaiting:
    // ok
}

if gp == getg() {
    throw("can't scan our own stack")
}
mp := gp.m
if mp != nil &amp;&amp; mp.helpgc != 0 {
    throw("can't scan gchelper stack")
}

// Shrink the stack if not much of it is being used. During
// concurrent GC, we can do this during concurrent mark.
if !work.markrootDone {
    shrinkstack(gp)
}

// Scan the stack.
var cache pcvalueCache
scanframe := func(frame *stkframe, unused unsafe.Pointer) bool {
    // scanframeworkerä¼šæ ¹æ®ä»£ç åœ°å€(pc)è·å–å‡½æ•°ä¿¡æ¯
    // ç„¶åæ‰¾åˆ°å‡½æ•°ä¿¡æ¯ä¸­çš„stackmap.bytedata, å®ƒä¿å­˜äº†å‡½æ•°çš„æ ˆä¸Šå“ªäº›åœ°æ–¹æœ‰æŒ‡é’ˆ
    // å†è°ƒç”¨scanblockæ¥æ‰«æå‡½æ•°çš„æ ˆç©ºé—´, åŒæ—¶å‡½æ•°çš„å‚æ•°ä¹Ÿä¼šè¿™æ ·æ‰«æ
    scanframeworker(frame, &amp;cache, gcw)
    return true
}
// æšä¸¾æ‰€æœ‰è°ƒç”¨å¸§, åˆ†åˆ«è°ƒç”¨scanframeå‡½æ•°
gentraceback(^uintptr(0), ^uintptr(0), 0, gp, 0, nil, 0x7fffffff, scanframe, nil, 0)
// æšä¸¾æ‰€æœ‰deferçš„è°ƒç”¨å¸§, åˆ†åˆ«è°ƒç”¨scanframeå‡½æ•°
tracebackdefers(gp, scanframe, nil)
gp.gcscanvalid = true } scanblockå‡½æ•°æ˜¯ä¸€ä¸ªé€šç”¨çš„æ‰«æå‡½æ•°, æ‰«æå…¨å±€å˜é‡å’Œæ ˆç©ºé—´éƒ½ä¼šç”¨å®ƒ, å’Œscanobjectä¸åŒçš„æ˜¯bitmapéœ€è¦æ‰‹åŠ¨ä¼ å…¥:
</code></pre></div></div>

<p>// scanblock scans b as scanobject would, but using an explicit
// pointer bitmap instead of the heap bitmap.
//
// This is used to scan non-heap roots, so it does not update
// gcw.bytesMarked or gcw.scanWork.
//
//go:nowritebarrier
func scanblock(b0, n0 uintptr, ptrmask *uint8, gcw *gcWork) {
    // Use local copies of original parameters, so that a stack trace
    // due to one of the throws below shows the original block
    // base and extent.
    b := b0
    n := n0</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>arena_start := mheap_.arena_start
arena_used := mheap_.arena_used

// æšä¸¾æ‰«æçš„åœ°å€
for i := uintptr(0); i &lt; n; {
    // æ‰¾åˆ°bitmapä¸­å¯¹åº”çš„byte
    // Find bits for the next word.
    bits := uint32(*addb(ptrmask, i/(sys.PtrSize*8)))
    if bits == 0 {
        i += sys.PtrSize * 8
        continue
    }
    // æšä¸¾byte
    for j := 0; j &lt; 8 &amp;&amp; i &lt; n; j++ {
        // å¦‚æœè¯¥åœ°å€åŒ…å«æŒ‡é’ˆ
        if bits&amp;1 != 0 {
            // æ ‡è®°åœ¨è¯¥åœ°å€çš„å¯¹è±¡å­˜æ´», å¹¶æŠŠå®ƒåŠ åˆ°æ ‡è®°é˜Ÿåˆ—(è¯¥å¯¹è±¡å˜ä¸ºç°è‰²)
            // Same work as in scanobject; see comments there.
            obj := *(*uintptr)(unsafe.Pointer(b + i))
            if obj != 0 &amp;&amp; arena_start &lt;= obj &amp;&amp; obj &lt; arena_used {
                // æ‰¾åˆ°è¯¥å¯¹è±¡å¯¹åº”çš„spanå’Œbitmap
                if obj, hbits, span, objIndex := heapBitsForObject(obj, b, i); obj != 0 {
                    // æ ‡è®°ä¸€ä¸ªå¯¹è±¡å­˜æ´», å¹¶æŠŠå®ƒåŠ åˆ°æ ‡è®°é˜Ÿåˆ—(è¯¥å¯¹è±¡å˜ä¸ºç°è‰²)
                    greyobject(obj, b, i, hbits, span, gcw, objIndex)
                }
            }
        }
        // å¤„ç†ä¸‹ä¸€ä¸ªæŒ‡é’ˆä¸‹ä¸€ä¸ªbit
        bits &gt;&gt;= 1
        i += sys.PtrSize
    }
} } greyobjectç”¨äºæ ‡è®°ä¸€ä¸ªå¯¹è±¡å­˜æ´», å¹¶æŠŠå®ƒåŠ åˆ°æ ‡è®°é˜Ÿåˆ—(è¯¥å¯¹è±¡å˜ä¸ºç°è‰²):
</code></pre></div></div>

<p>// obj is the start of an object with mark mbits.
// If it isnâ€™t already marked, mark it and enqueue into gcw.
// base and off are for debugging only and could be removed.
//go:nowritebarrierrec
func greyobject(obj, base, off uintptr, hbits heapBits, span *mspan, gcw *gcWork, objIndex uintptr) {
    // obj should be start of allocation, and so must be at least pointer-aligned.
    if obj&amp;(sys.PtrSize-1) != 0 {
        throw(â€œgreyobject: obj not pointer-alignedâ€)
    }
    mbits := span.markBitsForIndex(objIndex)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if useCheckmark {
    // checkmarkæ˜¯ç”¨äºæ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¯åˆ°è¾¾çš„å¯¹è±¡éƒ½è¢«æ­£ç¡®æ ‡è®°çš„æœºåˆ¶, ä»…é™¤é”™ä½¿ç”¨
    if !mbits.isMarked() {
        printlock()
        print("runtime:greyobject: checkmarks finds unexpected unmarked object obj=", hex(obj), "\n")
        print("runtime: found obj at *(", hex(base), "+", hex(off), ")\n")

        // Dump the source (base) object
        gcDumpObject("base", base, off)

        // Dump the object
        gcDumpObject("obj", obj, ^uintptr(0))

        getg().m.traceback = 2
        throw("checkmark found unmarked object")
    }
    if hbits.isCheckmarked(span.elemsize) {
        return
    }
    hbits.setCheckmarked(span.elemsize)
    if !hbits.isCheckmarked(span.elemsize) {
        throw("setCheckmarked and isCheckmarked disagree")
    }
} else {
    if debug.gccheckmark &gt; 0 &amp;&amp; span.isFree(objIndex) {
        print("runtime: marking free object ", hex(obj), " found at *(", hex(base), "+", hex(off), ")\n")
        gcDumpObject("base", base, off)
        gcDumpObject("obj", obj, ^uintptr(0))
        getg().m.traceback = 2
        throw("marking free object")
    }

    // å¦‚æœå¯¹è±¡æ‰€åœ¨çš„spanä¸­çš„gcmarkBitså¯¹åº”çš„bitå·²ç»è®¾ç½®ä¸º1åˆ™å¯ä»¥è·³è¿‡å¤„ç†
    // If marked we have nothing to do.
    if mbits.isMarked() {
        return
    }
    
    // è®¾ç½®å¯¹è±¡æ‰€åœ¨çš„spanä¸­çš„gcmarkBitså¯¹åº”çš„bitä¸º1
    // mbits.setMarked() // Avoid extra call overhead with manual inlining.
    atomic.Or8(mbits.bytep, mbits.mask)
    
    // å¦‚æœç¡®å®šå¯¹è±¡ä¸åŒ…å«æŒ‡é’ˆ(æ‰€åœ¨spançš„ç±»å‹æ˜¯noscan), åˆ™ä¸éœ€è¦æŠŠå¯¹è±¡æ”¾å…¥æ ‡è®°é˜Ÿåˆ—
    // If this is a noscan object, fast-track it to black
    // instead of greying it.
    if span.spanclass.noscan() {
        gcw.bytesMarked += uint64(span.elemsize)
        return
    }
}

// æŠŠå¯¹è±¡æ”¾å…¥æ ‡è®°é˜Ÿåˆ—
// å…ˆæ”¾å…¥æœ¬åœ°æ ‡è®°é˜Ÿåˆ—, å¤±è´¥æ—¶æŠŠæœ¬åœ°æ ‡è®°é˜Ÿåˆ—ä¸­çš„éƒ¨åˆ†å·¥ä½œè½¬ç§»åˆ°å…¨å±€æ ‡è®°é˜Ÿåˆ—, å†æ”¾å…¥æœ¬åœ°æ ‡è®°é˜Ÿåˆ—
// Queue the obj for scanning. The PREFETCH(obj) logic has been removed but
// seems like a nice optimization that can be added back in.
// There needs to be time between the PREFETCH and the use.
// Previously we put the obj in an 8 element buffer that is drained at a rate
// to give the PREFETCH time to do its work.
// Use of PREFETCHNTA might be more appropriate than PREFETCH
if !gcw.putFast(obj) {
    gcw.put(obj)
} } gcDrainå‡½æ•°æ‰«æå®Œæ ¹å¯¹è±¡, å°±ä¼šå¼€å§‹æ¶ˆè´¹æ ‡è®°é˜Ÿåˆ—, å¯¹ä»æ ‡è®°é˜Ÿåˆ—ä¸­å–å‡ºçš„å¯¹è±¡è°ƒç”¨scanobjectå‡½æ•°:
</code></pre></div></div>

<p>// scanobject scans the object starting at b, adding pointers to gcw.
// b must point to the beginning of a heap object or an oblet.
// scanobject consults the GC bitmap for the pointer mask and the
// spans for the size of the object.
//
//go:nowritebarrier
func scanobject(b uintptr, gcw <em>gcWork) {
    // Note that arena_used may change concurrently during
    // scanobject and hence scanobject may encounter a pointer to
    // a newly allocated heap object that is *not</em> in
    // [start,used). It will not mark this object; however, we
    // know that it was just installed by a mutator, which means
    // that mutator will execute a write barrier and take care of
    // marking it. This is even more pronounced on relaxed memory
    // architectures since we access arena_used without barriers
    // or synchronization, but the same logic applies.
    arena_start := mheap_.arena_start
    arena_used := mheap_.arena_used</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Find the bits for b and the size of the object at b.
//
// b is either the beginning of an object, in which case this
// is the size of the object to scan, or it points to an
// oblet, in which case we compute the size to scan below.
// è·å–å¯¹è±¡å¯¹åº”çš„bitmap
hbits := heapBitsForAddr(b)

// è·å–å¯¹è±¡æ‰€åœ¨çš„span
s := spanOfUnchecked(b)

// è·å–å¯¹è±¡çš„å¤§å°
n := s.elemsize
if n == 0 {
    throw("scanobject n == 0")
}

// å¯¹è±¡å¤§å°è¿‡å¤§æ—¶(maxObletBytesæ˜¯128KB)éœ€è¦åˆ†å‰²æ‰«æ
// æ¯æ¬¡æœ€å¤šåªæ‰«æ128KB
if n &gt; maxObletBytes {
    // Large object. Break into oblets for better
    // parallelism and lower latency.
    if b == s.base() {
        // It's possible this is a noscan object (not
        // from greyobject, but from other code
        // paths), in which case we must *not* enqueue
        // oblets since their bitmaps will be
        // uninitialized.
        if s.spanclass.noscan() {
            // Bypass the whole scan.
            gcw.bytesMarked += uint64(n)
            return
        }

        // Enqueue the other oblets to scan later.
        // Some oblets may be in b's scalar tail, but
        // these will be marked as "no more pointers",
        // so we'll drop out immediately when we go to
        // scan those.
        for oblet := b + maxObletBytes; oblet &lt; s.base()+s.elemsize; oblet += maxObletBytes {
            if !gcw.putFast(oblet) {
                gcw.put(oblet)
            }
        }
    }

    // Compute the size of the oblet. Since this object
    // must be a large object, s.base() is the beginning
    // of the object.
    n = s.base() + s.elemsize - b
    if n &gt; maxObletBytes {
        n = maxObletBytes
    }
}

// æ‰«æå¯¹è±¡ä¸­çš„æŒ‡é’ˆ
var i uintptr
for i = 0; i &lt; n; i += sys.PtrSize {
    // è·å–å¯¹åº”çš„bit
    // Find bits for this word.
    if i != 0 {
        // Avoid needless hbits.next() on last iteration.
        hbits = hbits.next()
    }
    // Load bits once. See CL 22712 and issue 16973 for discussion.
    bits := hbits.bits()
    
    // æ£€æŸ¥scan bitåˆ¤æ–­æ˜¯å¦ç»§ç»­æ‰«æ, æ³¨æ„ç¬¬äºŒä¸ªscan bitæ˜¯checkmark
    // During checkmarking, 1-word objects store the checkmark
    // in the type bit for the one word. The only one-word objects
    // are pointers, or else they'd be merged with other non-pointer
    // data into larger allocations.
    if i != 1*sys.PtrSize &amp;&amp; bits&amp;bitScan == 0 {
        break // no more pointers in this object
    }
    
    // æ£€æŸ¥pointer bit, ä¸æ˜¯æŒ‡é’ˆåˆ™ç»§ç»­
    if bits&amp;bitPointer == 0 {
        continue // not a pointer
    }

    // å–å‡ºæŒ‡é’ˆçš„å€¼
    // Work here is duplicated in scanblock and above.
    // If you make changes here, make changes there too.
    obj := *(*uintptr)(unsafe.Pointer(b + i))

    // å¦‚æœæŒ‡é’ˆåœ¨arenaåŒºåŸŸä¸­, åˆ™è°ƒç”¨greyobjectæ ‡è®°å¯¹è±¡å¹¶æŠŠå¯¹è±¡æ”¾åˆ°æ ‡è®°é˜Ÿåˆ—ä¸­
    // At this point we have extracted the next potential pointer.
    // Check if it points into heap and not back at the current object.
    if obj != 0 &amp;&amp; arena_start &lt;= obj &amp;&amp; obj &lt; arena_used &amp;&amp; obj-b &gt;= n {
        // Mark the object.
        if obj, hbits, span, objIndex := heapBitsForObject(obj, b, i); obj != 0 {
            greyobject(obj, b, i, hbits, span, gcw, objIndex)
        }
    }
}

// ç»Ÿè®¡æ‰«æè¿‡çš„å¤§å°å’Œå¯¹è±¡æ•°é‡
gcw.bytesMarked += uint64(n)
gcw.scanWork += int64(i) } åœ¨æ‰€æœ‰åå°æ ‡è®°ä»»åŠ¡éƒ½æŠŠæ ‡è®°é˜Ÿåˆ—æ¶ˆè´¹å®Œæ¯•æ—¶, ä¼šæ‰§è¡ŒgcMarkDoneå‡½æ•°å‡†å¤‡è¿›å…¥å®Œæˆæ ‡è®°é˜¶æ®µ(mark termination): åœ¨å¹¶è¡ŒGCä¸­gcMarkDoneä¼šè¢«æ‰§è¡Œä¸¤æ¬¡, ç¬¬ä¸€æ¬¡ä¼šç¦æ­¢æœ¬åœ°æ ‡è®°é˜Ÿåˆ—ç„¶åé‡æ–°å¼€å§‹åå°æ ‡è®°ä»»åŠ¡, ç¬¬äºŒæ¬¡ä¼šè¿›å…¥å®Œæˆæ ‡è®°é˜¶æ®µ(mark termination)ã€‚
</code></pre></div></div>

<p>// gcMarkDone transitions the GC from mark 1 to mark 2 and from mark 2
// to mark termination.
//
// This should be called when all mark work has been drained. In mark
// 1, this includes all root marking jobs, global work buffers, and
// active work buffers in assists and background workers; however,
// work may still be cached in per-P work buffers. In mark 2, per-P
// caches are disabled.
//
// The calling context must be preemptible.
//
// Note that it is explicitly okay to have write barriers in this
// function because completion of concurrent mark is best-effort
// anyway. Any work created by write barriers here will be cleaned up
// by mark termination.
func gcMarkDone() {
top:
    semacquire(&amp;work.markDoneSema)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Re-check transition condition under transition lock.
if !(gcphase == _GCmark &amp;&amp; work.nwait == work.nproc &amp;&amp; !gcMarkWorkAvailable(nil)) {
    semrelease(&amp;work.markDoneSema)
    return
}

// æš‚æ—¶ç¦æ­¢å¯åŠ¨æ–°çš„åå°æ ‡è®°ä»»åŠ¡
// Disallow starting new workers so that any remaining workers
// in the current mark phase will drain out.
//
// TODO(austin): Should dedicated workers keep an eye on this
// and exit gcDrain promptly?
atomic.Xaddint64(&amp;gcController.dedicatedMarkWorkersNeeded, -0xffffffff)
atomic.Xaddint64(&amp;gcController.fractionalMarkWorkersNeeded, -0xffffffff)

// åˆ¤æ–­æœ¬åœ°æ ‡è®°é˜Ÿåˆ—æ˜¯å¦å·²ç¦ç”¨
if !gcBlackenPromptly {
    // æœ¬åœ°æ ‡è®°é˜Ÿåˆ—æ˜¯å¦æœªç¦ç”¨, ç¦ç”¨ç„¶åé‡æ–°å¼€å§‹åå°æ ‡è®°ä»»åŠ¡
    // Transition from mark 1 to mark 2.
    //
    // The global work list is empty, but there can still be work
    // sitting in the per-P work caches.
    // Flush and disable work caches.

    // ç¦ç”¨æœ¬åœ°æ ‡è®°é˜Ÿåˆ—
    // Disallow caching workbufs and indicate that we're in mark 2.
    gcBlackenPromptly = true

    // Prevent completion of mark 2 until we've flushed
    // cached workbufs.
    atomic.Xadd(&amp;work.nwait, -1)

    // GC is set up for mark 2. Let Gs blocked on the
    // transition lock go while we flush caches.
    semrelease(&amp;work.markDoneSema)

    // æŠŠæ‰€æœ‰æœ¬åœ°æ ‡è®°é˜Ÿåˆ—ä¸­çš„å¯¹è±¡éƒ½æ¨åˆ°å…¨å±€æ ‡è®°é˜Ÿåˆ—
    systemstack(func() {
        // Flush all currently cached workbufs and
        // ensure all Ps see gcBlackenPromptly. This
        // also blocks until any remaining mark 1
        // workers have exited their loop so we can
        // start new mark 2 workers.
        forEachP(func(_p_ *p) {
            _p_.gcw.dispose()
        })
    })

    // é™¤é”™ç”¨
    // Check that roots are marked. We should be able to
    // do this before the forEachP, but based on issue
    // #16083 there may be a (harmless) race where we can
    // enter mark 2 while some workers are still scanning
    // stacks. The forEachP ensures these scans are done.
    //
    // TODO(austin): Figure out the race and fix this
    // properly.
    gcMarkRootCheck()

    // å…è®¸å¯åŠ¨æ–°çš„åå°æ ‡è®°ä»»åŠ¡
    // Now we can start up mark 2 workers.
    atomic.Xaddint64(&amp;gcController.dedicatedMarkWorkersNeeded, 0xffffffff)
    atomic.Xaddint64(&amp;gcController.fractionalMarkWorkersNeeded, 0xffffffff)

    // å¦‚æœç¡®å®šæ²¡æœ‰æ›´å¤šçš„ä»»åŠ¡åˆ™å¯ä»¥ç›´æ¥è·³åˆ°å‡½æ•°é¡¶éƒ¨
    // è¿™æ ·å°±å½“ä½œæ˜¯ç¬¬äºŒæ¬¡è°ƒç”¨äº†
    incnwait := atomic.Xadd(&amp;work.nwait, +1)
    if incnwait == work.nproc &amp;&amp; !gcMarkWorkAvailable(nil) {
        // This loop will make progress because
        // gcBlackenPromptly is now true, so it won't
        // take this same "if" branch.
        goto top
    }
} else {
    // è®°å½•å®Œæˆæ ‡è®°é˜¶æ®µå¼€å§‹çš„æ—¶é—´å’ŒSTWå¼€å§‹çš„æ—¶é—´
    // Transition to mark termination.
    now := nanotime()
    work.tMarkTerm = now
    work.pauseStart = now
    
    // ç¦æ­¢Gè¢«æŠ¢å 
    getg().m.preemptoff = "gcing"
    
    // åœæ­¢æ‰€æœ‰è¿è¡Œä¸­çš„G, å¹¶ç¦æ­¢å®ƒä»¬è¿è¡Œ
    systemstack(stopTheWorldWithSema)
    
    // !!!!!!!!!!!!!!!!
    // ä¸–ç•Œå·²åœæ­¢(STW)...
    // !!!!!!!!!!!!!!!!
    
    // The gcphase is _GCmark, it will transition to _GCmarktermination
    // below. The important thing is that the wb remains active until
    // all marking is complete. This includes writes made by the GC.
    
    // æ ‡è®°å¯¹æ ¹å¯¹è±¡çš„æ‰«æå·²å®Œæˆ, ä¼šå½±å“gcMarkRootPrepareä¸­çš„å¤„ç†
    // Record that one root marking pass has completed.
    work.markrootDone = true
    
    // ç¦æ­¢è¾…åŠ©GCå’Œåå°æ ‡è®°ä»»åŠ¡çš„è¿è¡Œ
    // Disable assists and background workers. We must do
    // this before waking blocked assists.
    atomic.Store(&amp;gcBlackenEnabled, 0)
    
    // å”¤é†’æ‰€æœ‰å› ä¸ºè¾…åŠ©GCè€Œä¼‘çœ çš„G
    // Wake all blocked assists. These will run when we
    // start the world again.
    gcWakeAllAssists()
    
    // Likewise, release the transition lock. Blocked
    // workers and assists will run when we start the
    // world again.
    semrelease(&amp;work.markDoneSema)
    
    // è®¡ç®—ä¸‹ä¸€æ¬¡è§¦å‘gcéœ€è¦çš„heapå¤§å°
    // endCycle depends on all gcWork cache stats being
    // flushed. This is ensured by mark 2.
    nextTriggerRatio := gcController.endCycle()
    
    // è¿›å…¥å®Œæˆæ ‡è®°é˜¶æ®µ, ä¼šé‡æ–°å¯åŠ¨ä¸–ç•Œ
    // Perform mark termination. This will restart the world.
    gcMarkTermination(nextTriggerRatio)
} } gcMarkTerminationå‡½æ•°ä¼šè¿›å…¥å®Œæˆæ ‡è®°é˜¶æ®µ:
</code></pre></div></div>

<p>func gcMarkTermination(nextTriggerRatio float64) {
    // World is stopped.
    // Start marktermination which includes enabling the write barrier.
    // ç¦æ­¢è¾…åŠ©GCå’Œåå°æ ‡è®°ä»»åŠ¡çš„è¿è¡Œ
    atomic.Store(&amp;gcBlackenEnabled, 0)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// é‡æ–°å…è®¸æœ¬åœ°æ ‡è®°é˜Ÿåˆ—(ä¸‹æ¬¡GCä½¿ç”¨)
gcBlackenPromptly = false

// è®¾ç½®å½“å‰GCé˜¶æ®µåˆ°å®Œæˆæ ‡è®°é˜¶æ®µ, å¹¶å¯ç”¨å†™å±éšœ
setGCPhase(_GCmarktermination)

// è®°å½•å¼€å§‹æ—¶é—´
work.heap1 = memstats.heap_live
startTime := nanotime()

// ç¦æ­¢Gè¢«æŠ¢å 
mp := acquirem()
mp.preemptoff = "gcing"
_g_ := getg()
_g_.m.traceback = 2

// è®¾ç½®Gçš„çŠ¶æ€ä¸ºç­‰å¾…ä¸­è¿™æ ·å®ƒçš„æ ˆå¯ä»¥è¢«æ‰«æ
gp := _g_.m.curg
casgstatus(gp, _Grunning, _Gwaiting)
gp.waitreason = "garbage collection"

// åˆ‡æ¢åˆ°g0è¿è¡Œ
// Run gc on the g0 stack. We do this so that the g stack
// we're currently running on will no longer change. Cuts
// the root set down a bit (g0 stacks are not scanned, and
// we don't need to scan gc's internal state).  We also
// need to switch to g0 so we can shrink the stack.
systemstack(func() {
    // å¼€å§‹STWä¸­çš„æ ‡è®°
    gcMark(startTime)
    
    // å¿…é¡»ç«‹åˆ»è¿”å›, å› ä¸ºå¤–é¢çš„Gçš„æ ˆæœ‰å¯èƒ½è¢«ç§»åŠ¨, ä¸èƒ½åœ¨è¿™ä¹‹åè®¿é—®å¤–é¢çš„å˜é‡
    // Must return immediately.
    // The outer function's stack may have moved
    // during gcMark (it shrinks stacks, including the
    // outer function's stack), so we must not refer
    // to any of its variables. Return back to the
    // non-system stack to pick up the new addresses
    // before continuing.
})

// é‡æ–°åˆ‡æ¢åˆ°g0è¿è¡Œ
systemstack(func() {
    work.heap2 = work.bytesMarked
    
    // å¦‚æœå¯ç”¨äº†checkmarkåˆ™æ‰§è¡Œæ£€æŸ¥, æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¯åˆ°è¾¾çš„å¯¹è±¡éƒ½æœ‰æ ‡è®°
    if debug.gccheckmark &gt; 0 {
        // Run a full stop-the-world mark using checkmark bits,
        // to check that we didn't forget to mark anything during
        // the concurrent mark process.
        gcResetMarkState()
        initCheckmarks()
        gcMark(startTime)
        clearCheckmarks()
    }

    // è®¾ç½®å½“å‰GCé˜¶æ®µåˆ°å…³é—­, å¹¶ç¦ç”¨å†™å±éšœ
    // marking is complete so we can turn the write barrier off
    setGCPhase(_GCoff)
    
    // å”¤é†’åå°æ¸…æ‰«ä»»åŠ¡, å°†åœ¨STWç»“æŸåå¼€å§‹è¿è¡Œ
    gcSweep(work.mode)

    // é™¤é”™ç”¨
    if debug.gctrace &gt; 1 {
        startTime = nanotime()
        // The g stacks have been scanned so
        // they have gcscanvalid==true and gcworkdone==true.
        // Reset these so that all stacks will be rescanned.
        gcResetMarkState()
        finishsweep_m()

        // Still in STW but gcphase is _GCoff, reset to _GCmarktermination
        // At this point all objects will be found during the gcMark which
        // does a complete STW mark and object scan.
        setGCPhase(_GCmarktermination)
        gcMark(startTime)
        setGCPhase(_GCoff) // marking is done, turn off wb.
        gcSweep(work.mode)
    }
})

// è®¾ç½®Gçš„çŠ¶æ€ä¸ºè¿è¡Œä¸­
_g_.m.traceback = 0
casgstatus(gp, _Gwaiting, _Grunning)

// è·Ÿè¸ªå¤„ç†
if trace.enabled {
    traceGCDone()
}

// all done
mp.preemptoff = ""

if gcphase != _GCoff {
    throw("gc done but gcphase != _GCoff")
}

// æ›´æ–°ä¸‹ä¸€æ¬¡è§¦å‘gcéœ€è¦çš„heapå¤§å°(gc_trigger)
// Update GC trigger and pacing for the next cycle.
gcSetTriggerRatio(nextTriggerRatio)

// æ›´æ–°ç”¨æ—¶è®°å½•
// Update timing memstats
now := nanotime()
sec, nsec, _ := time_now()
unixNow := sec*1e9 + int64(nsec)
work.pauseNS += now - work.pauseStart
work.tEnd = now
atomic.Store64(&amp;memstats.last_gc_unix, uint64(unixNow)) // must be Unix time to make sense to user
atomic.Store64(&amp;memstats.last_gc_nanotime, uint64(now)) // monotonic time for us
memstats.pause_ns[memstats.numgc%uint32(len(memstats.pause_ns))] = uint64(work.pauseNS)
memstats.pause_end[memstats.numgc%uint32(len(memstats.pause_end))] = uint64(unixNow)
memstats.pause_total_ns += uint64(work.pauseNS)

// æ›´æ–°æ‰€ç”¨cpuè®°å½•
// Update work.totaltime.
sweepTermCpu := int64(work.stwprocs) * (work.tMark - work.tSweepTerm)
// We report idle marking time below, but omit it from the
// overall utilization here since it's "free".
markCpu := gcController.assistTime + gcController.dedicatedMarkTime + gcController.fractionalMarkTime
markTermCpu := int64(work.stwprocs) * (work.tEnd - work.tMarkTerm)
cycleCpu := sweepTermCpu + markCpu + markTermCpu
work.totaltime += cycleCpu

// Compute overall GC CPU utilization.
totalCpu := sched.totaltime + (now-sched.procresizetime)*int64(gomaxprocs)
memstats.gc_cpu_fraction = float64(work.totaltime) / float64(totalCpu)

// é‡ç½®æ¸…æ‰«çŠ¶æ€
// Reset sweep state.
sweep.nbgsweep = 0
sweep.npausesweep = 0

// ç»Ÿè®¡å¼ºåˆ¶å¼€å§‹GCçš„æ¬¡æ•°
if work.userForced {
    memstats.numforcedgc++
}

// ç»Ÿè®¡æ‰§è¡ŒGCçš„æ¬¡æ•°ç„¶åå”¤é†’ç­‰å¾…æ¸…æ‰«çš„G
// Bump GC cycle count and wake goroutines waiting on sweep.
lock(&amp;work.sweepWaiters.lock)
memstats.numgc++
injectglist(work.sweepWaiters.head.ptr())
work.sweepWaiters.head = 0
unlock(&amp;work.sweepWaiters.lock)

// æ€§èƒ½ç»Ÿè®¡ç”¨
// Finish the current heap profiling cycle and start a new
// heap profiling cycle. We do this before starting the world
// so events don't leak into the wrong cycle.
mProf_NextCycle()

// é‡æ–°å¯åŠ¨ä¸–ç•Œ
systemstack(startTheWorldWithSema)

// !!!!!!!!!!!!!!!
// ä¸–ç•Œå·²é‡æ–°å¯åŠ¨...
// !!!!!!!!!!!!!!!

// æ€§èƒ½ç»Ÿè®¡ç”¨
// Flush the heap profile so we can start a new cycle next GC.
// This is relatively expensive, so we don't do it with the
// world stopped.
mProf_Flush()

// ç§»åŠ¨æ ‡è®°é˜Ÿåˆ—ä½¿ç”¨çš„ç¼“å†²åŒºåˆ°è‡ªç”±åˆ—è¡¨, ä½¿å¾—å®ƒä»¬å¯ä»¥è¢«å›æ”¶
// Prepare workbufs for freeing by the sweeper. We do this
// asynchronously because it can take non-trivial time.
prepareFreeWorkbufs()

// é‡Šæ”¾æœªä½¿ç”¨çš„æ ˆ
// Free stack spans. This must be done between GC cycles.
systemstack(freeStackSpans)

// é™¤é”™ç”¨
// Print gctrace before dropping worldsema. As soon as we drop
// worldsema another cycle could start and smash the stats
// we're trying to print.
if debug.gctrace &gt; 0 {
    util := int(memstats.gc_cpu_fraction * 100)

    var sbuf [24]byte
    printlock()
    print("gc ", memstats.numgc,
        " @", string(itoaDiv(sbuf[:], uint64(work.tSweepTerm-runtimeInitTime)/1e6, 3)), "s ",
        util, "%: ")
    prev := work.tSweepTerm
    for i, ns := range []int64{work.tMark, work.tMarkTerm, work.tEnd} {
        if i != 0 {
            print("+")
        }
        print(string(fmtNSAsMS(sbuf[:], uint64(ns-prev))))
        prev = ns
    }
    print(" ms clock, ")
    for i, ns := range []int64{sweepTermCpu, gcController.assistTime, gcController.dedicatedMarkTime + gcController.fractionalMarkTime, gcController.idleMarkTime, markTermCpu} {
        if i == 2 || i == 3 {
            // Separate mark time components with /.
            print("/")
        } else if i != 0 {
            print("+")
        }
        print(string(fmtNSAsMS(sbuf[:], uint64(ns))))
    }
    print(" ms cpu, ",
        work.heap0&gt;&gt;20, "-&gt;", work.heap1&gt;&gt;20, "-&gt;", work.heap2&gt;&gt;20, " MB, ",
        work.heapGoal&gt;&gt;20, " MB goal, ",
        work.maxprocs, " P")
    if work.userForced {
        print(" (forced)")
    }
    print("\n")
    printunlock()
}

semrelease(&amp;worldsema)
// Careful: another GC cycle may start now.

// é‡æ–°å…è®¸å½“å‰çš„Gè¢«æŠ¢å 
releasem(mp)
mp = nil

// å¦‚æœæ˜¯å¹¶è¡ŒGC, è®©å½“å‰Mç»§ç»­è¿è¡Œ(ä¼šå›åˆ°gcBgMarkWorkerç„¶åä¼‘çœ )
// å¦‚æœä¸æ˜¯å¹¶è¡ŒGC, åˆ™è®©å½“å‰Må¼€å§‹è°ƒåº¦
// now that gc is done, kick off finalizer thread if needed
if !concurrentSweep {
    // give the queued finalizers, if any, a chance to run
    Gosched()
} } gcSweepå‡½æ•°ä¼šå”¤é†’åå°æ¸…æ‰«ä»»åŠ¡: åå°æ¸…æ‰«ä»»åŠ¡ä¼šåœ¨ç¨‹åºå¯åŠ¨æ—¶è°ƒç”¨çš„gcenableå‡½æ•°ä¸­å¯åŠ¨.
</code></pre></div></div>

<p>func gcSweep(mode gcMode) {
    if gcphase != _GCoff {
        throw(â€œgcSweep being done but phase is not GCoffâ€)
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// å¢åŠ sweepgen, è¿™æ ·sweepSpansä¸­ä¸¤ä¸ªé˜Ÿåˆ—è§’è‰²ä¼šäº¤æ¢, æ‰€æœ‰spanéƒ½ä¼šå˜ä¸º"å¾…æ¸…æ‰«"çš„span
lock(&amp;mheap_.lock)
mheap_.sweepgen += 2
mheap_.sweepdone = 0
if mheap_.sweepSpans[mheap_.sweepgen/2%2].index != 0 {
    // We should have drained this list during the last
    // sweep phase. We certainly need to start this phase
    // with an empty swept list.
    throw("non-empty swept list")
}
mheap_.pagesSwept = 0
unlock(&amp;mheap_.lock)

// å¦‚æœéå¹¶è¡ŒGCåˆ™åœ¨è¿™é‡Œå®Œæˆæ‰€æœ‰å·¥ä½œ(STWä¸­)
if !_ConcurrentSweep || mode == gcForceBlockMode {
    // Special case synchronous sweep.
    // Record that no proportional sweeping has to happen.
    lock(&amp;mheap_.lock)
    mheap_.sweepPagesPerByte = 0
    unlock(&amp;mheap_.lock)
    // Sweep all spans eagerly.
    for sweepone() != ^uintptr(0) {
        sweep.npausesweep++
    }
    // Free workbufs eagerly.
    prepareFreeWorkbufs()
    for freeSomeWbufs(false) {
    }
    // All "free" events for this mark/sweep cycle have
    // now happened, so we can make this profile cycle
    // available immediately.
    mProf_NextCycle()
    mProf_Flush()
    return
}

// å”¤é†’åå°æ¸…æ‰«ä»»åŠ¡
// Background sweep.
lock(&amp;sweep.lock)
if sweep.parked {
    sweep.parked = false
    ready(sweep.g, 0, true)
}
unlock(&amp;sweep.lock) } åå°æ¸…æ‰«ä»»åŠ¡çš„å‡½æ•°æ˜¯bgsweep:
</code></pre></div></div>

<p>func bgsweep(c chan int) {
    sweep.g = getg()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// ç­‰å¾…å”¤é†’
lock(&amp;sweep.lock)
sweep.parked = true
c &lt;- 1
goparkunlock(&amp;sweep.lock, "GC sweep wait", traceEvGoBlock, 1)

// å¾ªç¯æ¸…æ‰«
for {
    // æ¸…æ‰«ä¸€ä¸ªspan, ç„¶åè¿›å…¥è°ƒåº¦(ä¸€æ¬¡åªåšå°‘é‡å·¥ä½œ)
    for gosweepone() != ^uintptr(0) {
        sweep.nbgsweep++
        Gosched()
    }
    // é‡Šæ”¾ä¸€äº›æœªä½¿ç”¨çš„æ ‡è®°é˜Ÿåˆ—ç¼“å†²åŒºåˆ°heap
    for freeSomeWbufs(true) {
        Gosched()
    }
    // å¦‚æœæ¸…æ‰«æœªå®Œæˆåˆ™ç»§ç»­å¾ªç¯
    lock(&amp;sweep.lock)
    if !gosweepdone() {
        // This can happen if a GC runs between
        // gosweepone returning ^0 above
        // and the lock being acquired.
        unlock(&amp;sweep.lock)
        continue
    }
    // å¦åˆ™è®©åå°æ¸…æ‰«ä»»åŠ¡è¿›å…¥ä¼‘çœ , å½“å‰Mç»§ç»­è°ƒåº¦
    sweep.parked = true
    goparkunlock(&amp;sweep.lock, "GC sweep wait", traceEvGoBlock, 1)
} } gosweeponeå‡½æ•°ä¼šä»sweepSpansä¸­å–å‡ºå•ä¸ªspanæ¸…æ‰«:
</code></pre></div></div>

<p>//go:nowritebarrier
func gosweepone() uintptr {
    var ret uintptr
    // åˆ‡æ¢åˆ°g0è¿è¡Œ
    systemstack(func() {
        ret = sweepone()
    })
    return ret
}
sweeponeå‡½æ•°å¦‚ä¸‹:</p>

<p>// sweeps one span
// returns number of pages returned to heap, or ^uintptr(0) if there is nothing to sweep
//go:nowritebarrier
func sweepone() uintptr {
    <em>g</em> := getg()
    sweepRatio := mheap_.sweepPagesPerByte // For debugging</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// ç¦æ­¢Gè¢«æŠ¢å 
// increment locks to ensure that the goroutine is not preempted
// in the middle of sweep thus leaving the span in an inconsistent state for next GC
_g_.m.locks++

// æ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ¸…æ‰«
if atomic.Load(&amp;mheap_.sweepdone) != 0 {
    _g_.m.locks--
    return ^uintptr(0)
}

// æ›´æ–°åŒæ—¶æ‰§è¡Œsweepçš„ä»»åŠ¡æ•°é‡
atomic.Xadd(&amp;mheap_.sweepers, +1)

npages := ^uintptr(0)
sg := mheap_.sweepgen
for {
    // ä»sweepSpansä¸­å–å‡ºä¸€ä¸ªspan
    s := mheap_.sweepSpans[1-sg/2%2].pop()
    // å…¨éƒ¨æ¸…æ‰«å®Œæ¯•æ—¶è·³å‡ºå¾ªç¯
    if s == nil {
        atomic.Store(&amp;mheap_.sweepdone, 1)
        break
    }
    // å…¶ä»–Må·²ç»åœ¨æ¸…æ‰«è¿™ä¸ªspanæ—¶è·³è¿‡
    if s.state != mSpanInUse {
        // This can happen if direct sweeping already
        // swept this span, but in that case the sweep
        // generation should always be up-to-date.
        if s.sweepgen != sg {
            print("runtime: bad span s.state=", s.state, " s.sweepgen=", s.sweepgen, " sweepgen=", sg, "\n")
            throw("non in-use span in unswept list")
        }
        continue
    }
    // åŸå­å¢åŠ spançš„sweepgen, å¤±è´¥è¡¨ç¤ºå…¶ä»–Må·²ç»å¼€å§‹æ¸…æ‰«è¿™ä¸ªspan, è·³è¿‡
    if s.sweepgen != sg-2 || !atomic.Cas(&amp;s.sweepgen, sg-2, sg-1) {
        continue
    }
    // æ¸…æ‰«è¿™ä¸ªspan, ç„¶åè·³å‡ºå¾ªç¯
    npages = s.npages
    if !s.sweep(false) {
        // Span is still in-use, so this returned no
        // pages to the heap and the span needs to
        // move to the swept in-use list.
        npages = 0
    }
    break
}

// æ›´æ–°åŒæ—¶æ‰§è¡Œsweepçš„ä»»åŠ¡æ•°é‡
// Decrement the number of active sweepers and if this is the
// last one print trace information.
if atomic.Xadd(&amp;mheap_.sweepers, -1) == 0 &amp;&amp; atomic.Load(&amp;mheap_.sweepdone) != 0 {
    if debug.gcpacertrace &gt; 0 {
        print("pacer: sweep done at heap size ", memstats.heap_live&gt;&gt;20, "MB; allocated ", (memstats.heap_live-mheap_.sweepHeapLiveBasis)&gt;&gt;20, "MB during sweep; swept ", mheap_.pagesSwept, " pages at ", sweepRatio, " pages/byte\n")
    }
}
// å…è®¸Gè¢«æŠ¢å 
_g_.m.locks--
// è¿”å›æ¸…æ‰«çš„é¡µæ•°
return npages } spançš„sweepå‡½æ•°ç”¨äºæ¸…æ‰«å•ä¸ªspan:
</code></pre></div></div>

<p>// Sweep frees or collects finalizers for blocks not marked in the mark phase.
// It clears the mark bits in preparation for the next GC round.
// Returns true if the span was returned to heap.
// If preserve=true, donâ€™t return it to heap nor relink in MCentral lists;
// caller takes care of it.
//TODO go:nowritebarrier
func (s *mspan) sweep(preserve bool) bool {
    // Itâ€™s critical that we enter this function with preemption disabled,
    // GC must not start while we are in the middle of this function.
    <em>g</em> := getg()
    if <em>g</em>.m.locks == 0 &amp;&amp; <em>g</em>.m.mallocing == 0 &amp;&amp; <em>g</em> != <em>g</em>.m.g0 {
        throw(â€œMSpan_Sweep: m is not lockedâ€)
    }
    sweepgen := mheap_.sweepgen
    if s.state != mSpanInUse || s.sweepgen != sweepgen-1 {
        print(â€œMSpan_Sweep: state=â€, s.state, â€œ sweepgen=â€, s.sweepgen, â€œ mheap.sweepgen=â€, sweepgen, â€œ\nâ€)
        throw(â€œMSpan_Sweep: bad span stateâ€)
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if trace.enabled {
    traceGCSweepSpan(s.npages * _PageSize)
}

// ç»Ÿè®¡å·²æ¸…ç†çš„é¡µæ•°
atomic.Xadd64(&amp;mheap_.pagesSwept, int64(s.npages))

spc := s.spanclass
size := s.elemsize
res := false

c := _g_.m.mcache
freeToHeap := false

// The allocBits indicate which unmarked objects don't need to be
// processed since they were free at the end of the last GC cycle
// and were not allocated since then.
// If the allocBits index is &gt;= s.freeindex and the bit
// is not marked then the object remains unallocated
// since the last GC.
// This situation is analogous to being on a freelist.

// åˆ¤æ–­åœ¨specialä¸­çš„ææ„å™¨, å¦‚æœå¯¹åº”çš„å¯¹è±¡å·²ç»ä¸å†å­˜æ´»åˆ™æ ‡è®°å¯¹è±¡å­˜æ´»é˜²æ­¢å›æ”¶, ç„¶åæŠŠææ„å™¨ç§»åˆ°è¿è¡Œé˜Ÿåˆ—
// Unlink &amp; free special records for any objects we're about to free.
// Two complications here:
// 1. An object can have both finalizer and profile special records.
//    In such case we need to queue finalizer for execution,
//    mark the object as live and preserve the profile special.
// 2. A tiny object can have several finalizers setup for different offsets.
//    If such object is not marked, we need to queue all finalizers at once.
// Both 1 and 2 are possible at the same time.
specialp := &amp;s.specials
special := *specialp
for special != nil {
    // A finalizer can be set for an inner byte of an object, find object beginning.
    objIndex := uintptr(special.offset) / size
    p := s.base() + objIndex*size
    mbits := s.markBitsForIndex(objIndex)
    if !mbits.isMarked() {
        // This object is not marked and has at least one special record.
        // Pass 1: see if it has at least one finalizer.
        hasFin := false
        endOffset := p - s.base() + size
        for tmp := special; tmp != nil &amp;&amp; uintptr(tmp.offset) &lt; endOffset; tmp = tmp.next {
            if tmp.kind == _KindSpecialFinalizer {
                // Stop freeing of object if it has a finalizer.
                mbits.setMarkedNonAtomic()
                hasFin = true
                break
            }
        }
        // Pass 2: queue all finalizers _or_ handle profile record.
        for special != nil &amp;&amp; uintptr(special.offset) &lt; endOffset {
            // Find the exact byte for which the special was setup
            // (as opposed to object beginning).
            p := s.base() + uintptr(special.offset)
            if special.kind == _KindSpecialFinalizer || !hasFin {
                // Splice out special record.
                y := special
                special = special.next
                *specialp = special
                freespecial(y, unsafe.Pointer(p), size)
            } else {
                // This is profile record, but the object has finalizers (so kept alive).
                // Keep special record.
                specialp = &amp;special.next
                special = *specialp
            }
        }
    } else {
        // object is still live: keep special record
        specialp = &amp;special.next
        special = *specialp
    }
}

// é™¤é”™ç”¨
if debug.allocfreetrace != 0 || raceenabled || msanenabled {
    // Find all newly freed objects. This doesn't have to
    // efficient; allocfreetrace has massive overhead.
    mbits := s.markBitsForBase()
    abits := s.allocBitsForIndex(0)
    for i := uintptr(0); i &lt; s.nelems; i++ {
        if !mbits.isMarked() &amp;&amp; (abits.index &lt; s.freeindex || abits.isMarked()) {
            x := s.base() + i*s.elemsize
            if debug.allocfreetrace != 0 {
                tracefree(unsafe.Pointer(x), size)
            }
            if raceenabled {
                racefree(unsafe.Pointer(x), size)
            }
            if msanenabled {
                msanfree(unsafe.Pointer(x), size)
            }
        }
        mbits.advance()
        abits.advance()
    }
}

// è®¡ç®—é‡Šæ”¾çš„å¯¹è±¡æ•°é‡
// Count the number of free objects in this span.
nalloc := uint16(s.countAlloc())
if spc.sizeclass() == 0 &amp;&amp; nalloc == 0 {
    // å¦‚æœspançš„ç±»å‹æ˜¯0(å¤§å¯¹è±¡)å¹¶ä¸”å…¶ä¸­çš„å¯¹è±¡å·²ç»ä¸å­˜æ´»åˆ™é‡Šæ”¾åˆ°heap
    s.needzero = 1
    freeToHeap = true
}
nfreed := s.allocCount - nalloc
if nalloc &gt; s.allocCount {
    print("runtime: nelems=", s.nelems, " nalloc=", nalloc, " previous allocCount=", s.allocCount, " nfreed=", nfreed, "\n")
    throw("sweep increased allocation count")
}

// è®¾ç½®æ–°çš„allocCount
s.allocCount = nalloc

// åˆ¤æ–­spanæ˜¯å¦æ— æœªåˆ†é…çš„å¯¹è±¡
wasempty := s.nextFreeIndex() == s.nelems

// é‡ç½®freeindex, ä¸‹æ¬¡åˆ†é…ä»0å¼€å§‹æœç´¢
s.freeindex = 0 // reset allocation index to start of span.
if trace.enabled {
    getg().m.p.ptr().traceReclaimed += uintptr(nfreed) * s.elemsize
}

// gcmarkBitså˜ä¸ºæ–°çš„allocBits
// ç„¶åé‡æ–°åˆ†é…ä¸€å—å…¨éƒ¨ä¸º0çš„gcmarkBits
// ä¸‹æ¬¡åˆ†é…å¯¹è±¡æ—¶å¯ä»¥æ ¹æ®allocBitså¾—çŸ¥å“ªäº›å…ƒç´ æ˜¯æœªåˆ†é…çš„
// gcmarkBits becomes the allocBits.
// get a fresh cleared gcmarkBits in preparation for next GC
s.allocBits = s.gcmarkBits
s.gcmarkBits = newMarkBits(s.nelems)

// æ›´æ–°freeindexå¼€å§‹çš„allocCache
// Initialize alloc bits cache.
s.refillAllocCache(0)

// å¦‚æœspanä¸­å·²ç»æ— å­˜æ´»çš„å¯¹è±¡åˆ™æ›´æ–°sweepgenåˆ°æœ€æ–°
// ä¸‹é¢ä¼šæŠŠspanåŠ åˆ°mcentralæˆ–è€…mheap
// We need to set s.sweepgen = h.sweepgen only when all blocks are swept,
// because of the potential for a concurrent free/SetFinalizer.
// But we need to set it before we make the span available for allocation
// (return it to heap or mcentral), because allocation code assumes that a
// span is already swept if available for allocation.
if freeToHeap || nfreed == 0 {
    // The span must be in our exclusive ownership until we update sweepgen,
    // check for potential races.
    if s.state != mSpanInUse || s.sweepgen != sweepgen-1 {
        print("MSpan_Sweep: state=", s.state, " sweepgen=", s.sweepgen, " mheap.sweepgen=", sweepgen, "\n")
        throw("MSpan_Sweep: bad span state after sweep")
    }
    // Serialization point.
    // At this point the mark bits are cleared and allocation ready
    // to go so release the span.
    atomic.Store(&amp;s.sweepgen, sweepgen)
}

if nfreed &gt; 0 &amp;&amp; spc.sizeclass() != 0 {
    // æŠŠspanåŠ åˆ°mcentral, resç­‰äºæ˜¯å¦æ·»åŠ æˆåŠŸ
    c.local_nsmallfree[spc.sizeclass()] += uintptr(nfreed)
    res = mheap_.central[spc].mcentral.freeSpan(s, preserve, wasempty)
    // freeSpanä¼šæ›´æ–°sweepgen
    // MCentral_FreeSpan updates sweepgen
} else if freeToHeap {
    // æŠŠspané‡Šæ”¾åˆ°mheap
    // Free large span to heap

    // NOTE(rsc,dvyukov): The original implementation of efence
    // in CL 22060046 used SysFree instead of SysFault, so that
    // the operating system would eventually give the memory
    // back to us again, so that an efence program could run
    // longer without running out of memory. Unfortunately,
    // calling SysFree here without any kind of adjustment of the
    // heap data structures means that when the memory does
    // come back to us, we have the wrong metadata for it, either in
    // the MSpan structures or in the garbage collection bitmap.
    // Using SysFault here means that the program will run out of
    // memory fairly quickly in efence mode, but at least it won't
    // have mysterious crashes due to confused memory reuse.
    // It should be possible to switch back to SysFree if we also
    // implement and then call some kind of MHeap_DeleteSpan.
    if debug.efence &gt; 0 {
        s.limit = 0 // prevent mlookup from finding this span
        sysFault(unsafe.Pointer(s.base()), size)
    } else {
        mheap_.freeSpan(s, 1)
    }
    c.local_nlargefree++
    c.local_largefree += size
    res = true
}

// å¦‚æœspanæœªåŠ åˆ°mcentralæˆ–è€…æœªé‡Šæ”¾åˆ°mheap, åˆ™è¡¨ç¤ºspanä»åœ¨ä½¿ç”¨
if !res {
    // æŠŠä»åœ¨ä½¿ç”¨çš„spanåŠ åˆ°sweepSpansçš„"å·²æ¸…æ‰«"é˜Ÿåˆ—ä¸­
    // The span has been swept and is still in-use, so put
    // it on the swept in-use list.
    mheap_.sweepSpans[sweepgen/2%2].push(s)
}
return res } ä»bgsweepå’Œå‰é¢çš„åˆ†é…å™¨å¯ä»¥çœ‹å‡ºæ‰«æé˜¶æ®µçš„å·¥ä½œæ˜¯ååˆ†æ‡’æƒ°(lazy)çš„, å®é™…å¯èƒ½ä¼šå‡ºç°å‰ä¸€é˜¶æ®µçš„æ‰«æè¿˜æœªå®Œæˆ, å°±éœ€è¦å¼€å§‹æ–°ä¸€è½®çš„GCçš„æƒ…å†µ, æ‰€ä»¥æ¯ä¸€è½®GCå¼€å§‹ä¹‹å‰éƒ½éœ€è¦å®Œæˆå‰ä¸€è½®GCçš„æ‰«æå·¥ä½œ(Sweep Terminationé˜¶æ®µ).
</code></pre></div></div>

<p>GCçš„æ•´ä¸ªæµç¨‹éƒ½åˆ†æå®Œæ¯•äº†, æœ€åè´´ä¸Šå†™å±éšœå‡½æ•°writebarrierptrçš„å®ç°:</p>

<p>// NOTE: Really dst *unsafe.Pointer, src unsafe.Pointer,
// but if we do that, Go inserts a write barrier on *dst = src.
//go:nosplit
func writebarrierptr(dst *uintptr, src uintptr) {
    if writeBarrier.cgo {
        cgoCheckWriteBarrier(dst, src)
    }
    if !writeBarrier.needed {
        *dst = src
        return
    }
    if src != 0 &amp;&amp; src &lt; minPhysPageSize {
        systemstack(func() {
            print(â€œruntime: writebarrierptr *â€, dst, â€œ = â€œ, hex(src), â€œ\nâ€)
            throw(â€œbad pointer in write barrierâ€)
        })
    }
    // æ ‡è®°æŒ‡é’ˆ
    writebarrierptr_prewrite1(dst, src)
    // è®¾ç½®æŒ‡é’ˆåˆ°ç›®æ ‡
    *dst = src
}
writebarrierptr_prewrite1å‡½æ•°å¦‚ä¸‹:</p>

<p>// writebarrierptr_prewrite1 invokes a write barrier for *dst = src
// prior to the write happening.
//
// Write barrier calls must not happen during critical GC and scheduler
// related operations. In particular there are times when the GC assumes
// that the world is stopped but scheduler related code is still being
// executed, dealing with syscalls, dealing with putting gs on runnable
// queues and so forth. This code cannot execute write barriers because
// the GC might drop them on the floor. Stopping the world involves removing
// the p associated with an m. We use the fact that m.p == nil to indicate
// that we are in one these critical section and throw if the write is of
// a pointer to a heap object.
//go:nosplit
func writebarrierptr_prewrite1(dst *uintptr, src uintptr) {
    mp := acquirem()
    if mp.inwb || mp.dying &gt; 0 {
        releasem(mp)
        return
    }
    systemstack(func() {
        if mp.p == 0 &amp;&amp; memstats.enablegc &amp;&amp; !mp.inwb &amp;&amp; inheap(src) {
            throw(â€œwritebarrierptr_prewrite1 called with mp.p == nilâ€)
        }
        mp.inwb = true
        gcmarkwb_m(dst, src)
    })
    mp.inwb = false
    releasem(mp)
}
gcmarkwb_må‡½æ•°å¦‚ä¸‹:</p>

<p>func gcmarkwb_m(slot *uintptr, ptr uintptr) {
    if writeBarrier.needed {
        // Note: This turns bad pointer writes into bad
        // pointer reads, which could be confusing. We avoid
        // reading from obviously bad pointers, which should
        // take care of the vast majority of these. We could
        // patch this up in the signal handler, or use XCHG to
        // combine the read and the write. Checking inheap is
        // insufficient since we need to track changes to
        // roots outside the heap.
        //
        // Note: profbuf.go omits a barrier during signal handler
        // profile logging; thatâ€™s safe only because this deletion barrier exists.
        // If we remove the deletion barrier, weâ€™ll have to work out
        // a new way to handle the profile logging.
        if slot1 := uintptr(unsafe.Pointer(slot)); slot1 &gt;= minPhysPageSize {
            if optr := *slot; optr != 0 {
                // æ ‡è®°æ—§æŒ‡é’ˆ
                shade(optr)
            }
        }
        // TODO: Make this conditional on the callerâ€™s stack color.
        if ptr != 0 &amp;&amp; inheap(ptr) {
            // æ ‡è®°æ–°æŒ‡é’ˆ
            shade(ptr)
        }
    }
}
shadeå‡½æ•°å¦‚ä¸‹:</p>

<p>// Shade the object if it isnâ€™t already.
// The object is not nil and known to be in the heap.
// Preemption must be disabled.
//go:nowritebarrier
func shade(b uintptr) {
    if obj, hbits, span, objIndex := heapBitsForObject(b, 0, 0); obj != 0 {
        gcw := &amp;getg().m.p.ptr().gcw
        // æ ‡è®°ä¸€ä¸ªå¯¹è±¡å­˜æ´», å¹¶æŠŠå®ƒåŠ åˆ°æ ‡è®°é˜Ÿåˆ—(è¯¥å¯¹è±¡å˜ä¸ºç°è‰²)
        greyobject(obj, 0, 0, hbits, span, gcw, objIndex)
        // å¦‚æœæ ‡è®°äº†ç¦æ­¢æœ¬åœ°æ ‡è®°é˜Ÿåˆ—åˆ™flushåˆ°å…¨å±€æ ‡è®°é˜Ÿåˆ—
        if gcphase == _GCmarktermination || gcBlackenPromptly {
            // Ps arenâ€™t allowed to cache work during mark
            // termination.
            gcw.dispose()
        }
    }
}
å‚è€ƒé“¾æ¥
https://github.com/golang/go
https://making.pusher.com/golangs-real-time-gc-in-theory-and-practice
https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md
https://golang.org/s/go15gcpacing
https://golang.org/ref/mem
https://talks.golang.org/2015/go-gc.pdf
https://docs.google.com/document/d/1ETuA2IOmnaQ4j81AtTGT40Y4_Jr6_IDASEKg0t0dBR8/edit#heading=h.x4kziklnb8fr
https://go-review.googlesource.com/c/go/+/21503
http://www.cnblogs.com/diegodu/p/5803202.html
http://legendtkl.com/2017/04/28/golang-gc
https://lengzzz.com/note/gc-in-golang</p>

<p>Golangçš„GCå’ŒCoreCLRçš„GCå¯¹æ¯”
å› ä¸ºæˆ‘ä¹‹å‰å·²ç»å¯¹CoreCLRçš„GCåšè¿‡åˆ†æ(çœ‹è¿™ä¸€ç¯‡å’Œè¿™ä¸€ç¯‡), è¿™é‡Œæˆ‘å¯ä»¥ç®€å•çš„å¯¹æ¯”ä¸€ä¸‹CoreCLRå’ŒGOçš„GCå®ç°:</p>

<p>CoreCLRçš„å¯¹è±¡å¸¦æœ‰ç±»å‹ä¿¡æ¯, GOçš„å¯¹è±¡ä¸å¸¦, è€Œæ˜¯é€šè¿‡bitmapåŒºåŸŸè®°å½•å“ªäº›åœ°æ–¹åŒ…å«æŒ‡é’ˆ
CoreCLRåˆ†é…å¯¹è±¡çš„é€Ÿåº¦æ˜æ˜¾æ›´å¿«, GOåˆ†é…å¯¹è±¡éœ€è¦æŸ¥æ‰¾spanå’Œå†™å…¥bitmapåŒºåŸŸ
CoreCLRçš„æ”¶é›†å™¨éœ€è¦åšçš„å·¥ä½œæ¯”GOå¤šå¾ˆå¤š
CoreCLRä¸åŒå¤§å°çš„å¯¹è±¡éƒ½ä¼šæ”¾åœ¨ä¸€ä¸ªsegmentä¸­, åªèƒ½çº¿æ€§æ‰«æ
CoreCLRåˆ¤æ–­å¯¹è±¡å¼•ç”¨è¦è®¿é—®ç±»å‹ä¿¡æ¯, è€Œgoåªéœ€è¦è®¿é—®bitmap
CoreCLRæ¸…æ‰«æ—¶è¦ä¸€ä¸ªä¸ªå»æ ‡è®°ä¸ºè‡ªç”±å¯¹è±¡, è€Œgoåªéœ€è¦åˆ‡æ¢allocBits
CoreCLRçš„åœé¡¿æ—¶é—´æ¯”GOè¦é•¿
è™½ç„¶CoreCLRæ”¯æŒå¹¶è¡ŒGC, ä½†æ˜¯æ²¡æœ‰GOå½»åº•, GOè¿æ‰«ææ ¹å¯¹è±¡éƒ½ä¸éœ€è¦å®Œå…¨åœé¡¿
CoreCLRæ”¯æŒåˆ†ä»£GC
è™½ç„¶Full GCæ—¶CoreCLRçš„æ•ˆç‡ä¸å¦‚GO, ä½†æ˜¯CoreCLRå¯ä»¥åœ¨å¤§éƒ¨åˆ†æ—¶å€™åªæ‰«æç¬¬0å’Œç¬¬1ä»£çš„å¯¹è±¡
å› ä¸ºæ”¯æŒåˆ†ä»£GC, é€šå¸¸CoreCLRèŠ±åœ¨GCä¸Šçš„CPUæ—¶é—´ä¼šæ¯”GOè¦å°‘
CoreCLRçš„åˆ†é…å™¨å’Œæ”¶é›†å™¨é€šå¸¸æ¯”GOè¦é«˜æ•ˆ, ä¹Ÿå°±æ˜¯è¯´CoreCLRä¼šæœ‰æ›´é«˜çš„ååé‡.
ä½†CoreCLRçš„æœ€å¤§åœé¡¿æ—¶é—´ä¸å¦‚GOçŸ­, è¿™æ˜¯å› ä¸ºGOçš„GCæ•´ä¸ªè®¾è®¡éƒ½æ˜¯ä¸ºäº†å‡å°‘åœé¡¿æ—¶é—´.</p>

<p>ç°åœ¨åˆ†å¸ƒå¼è®¡ç®—å’Œæ¨ªå‘æ‰©å±•è¶Šæ¥è¶Šæµè¡Œ,
æ¯”èµ·è¿½æ±‚å•æœºååé‡, è¿½æ±‚ä½å»¶è¿Ÿç„¶åè®©åˆ†å¸ƒå¼è§£å†³ååé‡é—®é¢˜æ— ç–‘æ˜¯æ›´æ˜æ™ºçš„é€‰æ‹©,
GOçš„è®¾è®¡ç›®æ ‡ä½¿å¾—å®ƒæ¯”å…¶ä»–è¯­è¨€éƒ½æ›´é€‚åˆç¼–å†™ç½‘ç»œæœåŠ¡ç¨‹åº.</p>
:ET