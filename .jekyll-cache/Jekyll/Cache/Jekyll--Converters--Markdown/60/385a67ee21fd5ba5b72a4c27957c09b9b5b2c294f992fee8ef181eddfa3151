I"O<p>1.12及之前版本的sync.Pool有三个问题：</p>

<p>每次GC都回收所有对象，如果缓存对象数量太大，会导致STW1阶段的耗时增加。</p>

<p>每次GC都回收所有对象，导致缓存对象命中率下降，New方法的执行造成额外的内存分配消耗。</p>

<p>Pool.Get方法底层有锁，极端情况下，要尝试最多P次抢锁，也获取不到缓存对象，最后得执行New方法返回对象。</p>

<p>这些问题就对sync.Pool的室使用提出了要求，不满足时，性能并不会有大幅提升：</p>

<p>最好是高并发场景。（对应问题3）</p>

<p>最好两次GC之间的间隔足够长。（对应问题1，2）
<!-- more -->
首先sync.Pool 有两种使用方式，使用效果没有区别。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>第一种，实例化的时候，实现New 函数即可：
</code></pre></div></div>

<p>package main</p>

<p>import(<br />
    “fmt”<br />
    “sync”<br />
)</p>

<p>func main() {<br />
    p := &amp;sync.Pool{<br />
        New: func() interface{} {<br />
            return 0<br />
        },<br />
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a := p.Get().(int)  
p.Put(1)  
b := p.Get().(int)  
fmt.Println(a, b)   }  
    第二种，get 取值的时候，判断是否为nil 即可。
</code></pre></div></div>

<p>package main</p>

<p>import(<br />
    “fmt”<br />
    “sync”<br />
)</p>

<p>func main() {<br />
    p := &amp;sync.Pool{}<br />
    a := p.Get()
    if a == nil {
       a = func() interface{} {<br />
            return 0<br />
        }
    }
    p.Put(1)<br />
    b := p.Get().(int)<br />
    fmt.Println(a, b)<br />
}<br />
    这两种实现方式，最后的效果是一样的，也反应了pool 的特性，get 返回值是new 的对象，或者nil。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>然后，pool 底层到底是怎样的数据结构？就是一个metux 和 slice？其实也是类似，只是加了些其他特性而已，下面数据结构：
</code></pre></div></div>

<p>type Pool struct {<br />
    local     unsafe.Pointer // local fixed-size per-P pool, actual type is [P]poolLocal<br />
    localSize uintptr        // size of the local array</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// New optionally specifies a function to generate  
// a value when Get would otherwise return nil.  
// It may not be changed concurrently with calls to Get.  
New func() interface{}   }  
</code></pre></div></div>

<p>// Local per-P Pool appendix.<br />
type poolLocal struct {<br />
    private interface{}   // Can be used only by the respective P.<br />
    shared  []interface{} // Can be used by any P.<br />
    Mutex                 // Protects shared.<br />
    pad     [128]byte     // Prevents false sharing.<br />
}<br />
    这里的local 是个poolLocal 的数组，localsize 是数组的大小。其中，从get 和put 方法看，为每个thread 维护了一个poolLocal 数据结构。不同线程取数据的时候，先判断下hash 到哪个线程去了，分别去对应的poolLocal 中去取数据，这是利用了分段锁的思想。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>具体实现可以看get 方法：
</code></pre></div></div>

<p>func (p *Pool) Get() interface{} {<br />
    if raceenabled {<br />
        if p.New != nil {<br />
            return p.New()<br />
        }<br />
        return nil<br />
    }<br />
    l := p.pin()  // 获取当前线程的poolLocal，也就是p.local[pid]。<br />
    x := l.private  //判断临时变量是否有值，有值即返回
    l.private = nil<br />
    runtime_procUnpin()<br />
    if x != nil {<br />
        return x<br />
    }<br />
    l.Lock()  //临时对象没值到本地的缓存列表中去取
    last := len(l.shared) - 1<br />
    if last &gt;= 0 {<br />
        x = l.shared[last]<br />
        l.shared = l.shared[:last]<br />
    }<br />
    l.Unlock()<br />
    if x != nil {<br />
        return x<br />
    }<br />
    return p.getSlow()  //当本线程的缓存对象已经没有，去其他线程缓存列表中取
}<br />
    这里代码的注释比较详尽了，本来维护一个mutex ，现在变成竞争多个mutex ，降低了锁的竞争。性能自然非常好。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>最后是getSlow 方法，从其他线程的变量中去steal 偷。runtime 也喜欢搞这种事。。。
</code></pre></div></div>

<p>func (p *Pool) getSlow() (x interface{}) {<br />
    // See the comment in pin regarding ordering of the loads.<br />
    size := atomic.LoadUintptr(&amp;p.localSize) // load-acquire<br />
    local := p.local                         // load-consume<br />
    // Try to steal one element from other procs.<br />
    pid := runtime_procPin()<br />
    runtime_procUnpin()<br />
    for i := 0; i &lt; int(size); i++ {  //遍历其他线程的缓存队列
        l := indexLocal(local, (pid+i+1)%int(size))<br />
        l.Lock()<br />
        last := len(l.shared) - 1<br />
        if last &gt;= 0 {<br />
            x = l.shared[last]<br />
            l.shared = l.shared[:last]<br />
            l.Unlock()<br />
            break<br />
        }<br />
        l.Unlock()<br />
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if x == nil &amp;&amp; p.New != nil {  //其他线程没有，那么new 一个
    x = p.New()  
}  
return x   }  
最后，pool 还有个特性是当gc 的时候所有的缓存对象都要被清理，调用的是PoolCleanUp,没什么特别之处。但是这个特性要求了pool 绝对不能做有状态的缓存，类似socket的缓存池。

这里的分段锁，为每个线程bind 一个队列，还考虑到了均衡的情况，
</code></pre></div></div>

<p>sync.Pool设计的目的是用来保存和复用临时对象，以减少内存分配，降低CG压力。</p>

<p>Pool对外暴露的主要有三个接口：</p>

<p>func (p *Pool) Get() interface{}</p>

<p>func (p *Pool) Put(x interface{})</p>

<p>New func() interface{}
Get 返回 Pool 中的任意一个对象。如果 Pool 为空，则调用 New 返回一个新创建的对象。</p>

<p>底层数据结构
sync.Pool 是一个临时对象池。一句话来概括，sync.Pool 管理了一组临时对象，当需要时从池中获取，使用完毕后从再放回池中，以供他人使用。</p>

<p>type Pool struct {
	noCopy noCopy</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>local     unsafe.Pointer // local,固定大小per-P池, 实际类型为 [P]poolLocal
localSize uintptr        // local array 的大小
//  New 方法在 Get 失败的情况下，选择性的创建一个值, 否则返回nil
New func() interface{} }
</code></pre></div></div>

<p>type poolLocal struct {
	poolLocalInternal</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// 将 poolLocal 补齐至两个缓存行的倍数，防止 false sharing,
// 每个缓存行具有 64 bytes，即 512 bit
// 目前我们的处理器一般拥有 32 * 1024 / 64 = 512 条缓存行
pad [128 - unsafe.Sizeof(poolLocalInternal{})%128]byte }
</code></pre></div></div>

<p>// Local per-P Pool appendix.
type poolLocalInternal struct {
	private interface{}   // 只能被局部调度器P使用
	shared  []interface{} // 所有P共享
	Mutex                 // 访问共享数据域的锁
}</p>

<p>一个poolLocal与一个P绑定，也就是说一个P持有一个poolLocal。每个 poolLocal 的大小均为缓存行的偶数倍，包含一个 private 私有对象、shared 共享对象 slice 以及一个 Mutex 并发锁。</p>

<p>Put
Put的过程就是将临时对象放进 Pool 里面。</p>

<p>func (p *Pool) Put(x interface{}) {
	if x == nil {
		return
	}
	// 获取 localPool
	l := p.pin()
	// 优先放入 private
	if l.private == nil {
		l.private = x
		x = nil
	}
	runtime_procUnpin()
	// 如果不能放入 private 则放入 shared
	if x != nil {
		l.Lock()
		l.shared = append(l.shared, x)
		l.Unlock()
	}
}
Put的策略相对简单：</p>

<p>首先获取当前goroutine所运行的P持有的localPool
优先放入 private
如果 private 已经有值，即不能放入则放入 shared
前面还有两个细节：</p>

<p>怎么获取到当前P持有的localPool
runtime_procUnpin() 函数的作用
具体细节在后面分析。</p>

<p>Get
Get操作相对复杂一点，在从池中获取对象的时候，会先从 per-P 的 poolLocal slice 中选取一个 poolLocal。
源码如下：</p>

<p>func (p *Pool) Get() interface{} {
	// 首先获取 poolLocal
	l := p.pin()
	// 先从private取
	x := l.private
	l.private = nil
	runtime_procUnpin()
	// private不存在再从shared里面去
	if x == nil {
		// 加锁，从 shared 获取
		l.Lock()
		// 从 shared 尾部取缓存对象
		last := len(l.shared) - 1
		if last &gt;= 0 {
			x = l.shared[last]
			l.shared = l.shared[:last]
		}
		l.Unlock()
		if x == nil {
			// 如果取不到，则获取新的缓存对象
			x = p.getSlow()
		}
	}
	// 如果 getSlow 还是获取不到，则 New 一个
	if x == nil &amp;&amp; p.New != nil {
		x = p.New()
	}
	return x
}</p>

<p>优先从 private 中选择对象
若取不到，则对 shared slice 加锁，取最后一个
若取不到，则尝试从其他线程中 steal
若还是取不到，则使用 New 方法新建
这里同样涉及到两个细节：</p>

<p>怎么获取到当前P持有的localPool
getSlow() 的steal是怎么实现的
细节
pin()函数获取per-P的localPool
还是先看源码：</p>

<p>// pin函数会将当前 goroutine绑定的P, 禁止抢占(preemption) 并从 poolLocal 池中返回 P 对应的 poolLocal
// Caller must call runtime_procUnpin() when done with the pool.
func (p *Pool) pin() *poolLocal {
	pid := runtime_procPin()
	// 在 pinSlow 中会存储 localSize 后再存储 local，因此这里反过来读取
	// 因为我们已经禁用了抢占，这时不会发生 GC
	// 因此，我们必须观察 local 和 localSize 是否对应
	// 观察到一个全新或很大的的 local 是正常行为
	s := atomic.LoadUintptr(&amp;p.localSize) // load-acquire
	l := p.local                          // load-consume
	// 因为可能存在动态的 P（运行时调整 P 的个数）procresize/GOMAXPROCS
	// 如果 P.id 没有越界，则直接返回
	if uintptr(pid) &lt; s {
		return indexLocal(l, pid)
	}
	// 没有结果时，涉及全局加锁
	// 例如重新分配数组内存，添加到全局列表
	return p.pinSlow()
}</p>

<p>//go:linkname sync_runtime_procPin sync.runtime_procPin
//go:nosplit
func sync_runtime_procPin() int {
	return procPin()
}</p>

<p>//go:nosplit
func procPin() int {
	<em>g</em> := getg()
	mp := <em>g</em>.m</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mp.locks++
return int(mp.p.ptr().id) }
</code></pre></div></div>

<p>根据注释：pin函数首先会调用运行时实现获得当前 P 的 id，然后设置P禁止抢占(避免GC)。然后检查 pid 与 p.localSize 的值 来确保从 p.local 中取值不会发生越界。如果不会发生，则调用 indexLocal() 完成取值。否则还需要继续调用 pinSlow()。</p>

<p>这里调用了 runtime_procPin() 来实现获取runtime的P，并设置禁止抢占，然后返回P的id。</p>

<p>在这个过程中我们可以看到在 runtime 调整 P 的大小的代价。如果此时 P 被调大，而没有对应的 poolLocal 时， 必须在取之前创建好，从而必须依赖全局加锁，这对于以性能著称的池化概念是比较致命的，因此这也是 pinSlow() 函数的由来。</p>

<p>pinSlow()
因为需要对全局进行加锁，pinSlow() 会首先取消 P 的不可抢占，然后使用 allPoolsMu 进行加锁：</p>

<p>var (
	allPoolsMu Mutex
	allPools   []*Pool
)
1
2
3
4
这里可以看到，Pool里面有全局变量持有了所有的Pool, 然后也有一个全局锁来保护数据域的可靠性。</p>

<p>pinSlow源码：</p>

<p>func (p *Pool) pinSlow() *poolLocal {
	// 这时取消 P 的禁止抢占，因为使用 mutex 时候 P 必须可抢占
	runtime_procUnpin()
	allPoolsMu.Lock()
	defer allPoolsMu.Unlock()
	// 当锁住后，再次固定 P 取其 id
	pid := runtime_procPin()
	// 并再次检查是否符合条件，因为可能中途已被其他线程调用
	// 当再次固定 P 时 poolCleanup 不会被调用
	s := p.localSize
	l := p.local
	if uintptr(pid) &lt; s {
		return indexLocal(l, pid)
	}
	// 如果数组为空，新建
	// 将其添加到 allPools，垃圾回收器从这里获取所有 Pool 实例
	if p.local == nil {
		allPools = append(allPools, p)
	}
	// 根据 P 数量创建 slice，如果 GOMAXPROCS 在 GC 间发生变化
	// 我们重新分配此数组并丢弃旧的
	size := runtime.GOMAXPROCS(0)
	local := make([]poolLocal, size)
	atomic.StorePointer(&amp;p.local, unsafe.Pointer(&amp;local[0])) // store-release
	atomic.StoreUintptr(&amp;p.localSize, uintptr(size))         // store-release
	return &amp;local[pid]
}</p>

<p>pinSlow() 会首先取消 P 的不可抢占，然后使用 allPoolsMu 进行加锁。
当完成加锁后，再重新固定 P ，取其 pid。
因为中途可能已经被其他的线程调用，因此这时候需要再次对 pid 进行检查。 如果 pid 在 p.local 大小范围内，则不再此时创建，直接返回。
如果 p.local 为空，则将 p 扔给 allPools 并在垃圾回收阶段回收所有 Pool 实例。
最后再完成对 p.local 的创建（彻底丢弃旧数组）
getSlow() steal from other per-P localPool
现在我们获取到了 poolLocal。Get操作就回到了我们从localPool中取值的过程。在取对象的过程中，我们仍然会面对当前localPool中没有缓存的对象了，也就是既不能从 private 取、也不能从 shared 中取得尴尬境地。这时候就来到了 getSlow()，也就是steal</p>

<p>如果我们在本地的 P 中取不到值，就从别的P那里偷一个，总会比创建一个新的要快。 因此，我们再次固定 P，并取得当前的 P.id 来从其他 P 中偷值，那么我们需要先获取到其他 P 对应的 poolLocal。假设 size 为数组的大小，local 为 p.local，那么尝试遍历其他所有 P：</p>

<p>func (p *Pool) getSlow() (x interface{}) {
	// See the comment in pin regarding ordering of the loads.
	size := atomic.LoadUintptr(&amp;p.localSize) // load-acquire
	local := p.local                         // load-consume
	// Try to steal one element from other procs.
	pid := runtime_procPin()
	runtime_procUnpin()
	for i := 0; i &lt; int(size); i++ {
		// 获取目标 poolLocal, 引入 pid 保证不是自身
		l := indexLocal(local, (pid+i+1)%int(size))
		l.Lock()
		last := len(l.shared) - 1
		if last &gt;= 0 {
			x = l.shared[last]
			l.shared = l.shared[:last]
			l.Unlock()
			break
		}
		l.Unlock()
	}
	return x
}</p>

<p>这里证明一下确实不会发生取到自身的情况：不妨设：pid = (pid+i+1)%size则 pid+i+1 = a<em>size+pid 。
即：a</em>size = i+1 ，其中 a 为整数。由于 i&lt;size ，于是 a<em>size = i+1 &lt; size+1，则：(a-1)</em>size &lt; 1 ==&gt; size &lt; 1 / (a-1)，由于 size 为非负整数，这是不可能的。</p>

<p>Runtime 垃圾回收Hook
前面讲到了sync.Pool 的垃圾回收发生在运行时 GC 开始之前。</p>

<p>我们看看 sync.Pool 的 init 函数：</p>

<p>func init() {
	runtime_registerPoolCleanup(poolCleanup)
}</p>

<p>func runtime_registerPoolCleanup(cleanup func())</p>

<p>//go:linkname sync_runtime_registerPoolCleanup sync.runtime_registerPoolCleanup
func sync_runtime_registerPoolCleanup(f func()) {
	poolcleanup = f
}</p>

<p>func clearpools() {
	// clear sync.Pools
	if poolcleanup != nil {
		poolcleanup()
	}
	……
}</p>

<p>func gcStart(trigger gcTrigger){
	…….
	clearpools()
	…….
}</p>

<p>从链路的追踪可以看到，在开始GC的时候回调用Pool的回收。</p>

<p>下面看看Pool的清理函数poolCleanup()是怎么清理Pool的：</p>

<p>func poolCleanup() {
	// 该函数会注册到运行时 GC 阶段(前)，此时为 STW 状态，不需要加锁
	// 它必须不处理分配且不调用任何运行时函数，防御性的将一切归零，有以下两点原因:
	// 1. 防止整个 Pool 的 false retention
	// 2. 如果 GC 发生在当有 goroutine 与 l.shared 进行 Put/Get 时，它会保留整个 Pool.
	//   那么下个 GC 周期的内存消耗将会翻倍。
	// 遍历所有 Pool 实例，接触相关引用，交由 GC 进行回收
	for i, p := range allPools {
		allPools[i] = nil
		for i := 0; i &lt; int(p.localSize); i++ {
			l := indexLocal(p.local, i)
			l.private = nil
			for j := range l.shared {
				l.shared[j] = nil
			}
			l.shared = nil
		}
		p.local = nil
		p.localSize = 0
	}
	allPools = []*Pool{}
}</p>

<p>实际上就是将所有的对象置为 nil，等着GC做自动回收。
整个设计充分利用了go.runtime的调度器优势：一个P下goroutine竞争的无锁化；</p>

<p>一个goroutine固定在一个局部调度器P上，从当前 P 对应的 poolLocal 取值， 若取不到，则从对应的 shared 数组上取，若还是取不到；则尝试从其他 P 的 shared 中偷。 若偷不到，则调用 New 创建一个新的对象。池中所有临时对象在一次 GC 后会被全部清空。</p>

<p>1、缓存对象的数量和期限
上面我们可以看到 pool 创建的时候是不能指定大小的，所有 sync.Pool 的缓存对象数量是没有限制的（只受限于内存），因此使用 sync.pool 是没办法做到控制缓存对象数量的个数的。另外 sync.pool 缓存对象的期限是很诡异的，先看一下 src/pkg/sync/pool.go 里面的一段实现代码：
func init() { 
    runtime_registerPoolCleanup(poolCleanup) 
}
可以看到 pool 包在 init 的时候注册了一个 poolCleanup 函数，它会清除所有的 pool 里面的所有缓存的对象，该函数注册进去之后会在每次 gc 之前都会调用，因此 sync.Pool 缓存的期限只是两次 gc 之间这段时间。例如我们把上面的例子改成下面这样之后，输出的结果将是 0 0。正因 gc 的时候会清掉缓存对象，也不用担心 pool 会无限增大的问题。</p>

<p>a := p.Get().(int) 
p.Put(1) 
runtime.GC() 
b := p.Get().(int) 
fmt.Println(a, b)
这是很多人错误理解的地方，正因为这样，我们是不可以使用sync.Pool去实现一个socket连接池的。</p>

<p>2、缓存对象的开销
如何在多个 goroutine 之间使用同一个 pool 做到高效呢？官方的做法就是尽量减少竞争，因为 sync.pool 为每个 P（对应 cpu，不了解的童鞋可以去看看 golang 的调度模型介绍）都分配了一个子池，如下图：</p>

<p>当执行一个 pool 的 get 或者 put 操作的时候都会先把当前的 goroutine 固定到某个P的子池上面，然后再对该子池进行操作。每个子池里面有一个私有对象和共享列表对象，私有对象是只有对应的 P 能够访问，因为一个 P 同一时间只能执行一个 goroutine，因此对私有对象存取操作是不需要加锁的。共享列表是和其他 P 分享的，因此操作共享列表是需要加锁的。</p>

<p>获取对象过程是：</p>

<p>1）固定到某个 P，尝试从私有对象获取，如果私有对象非空则返回该对象，并把私有对象置空；</p>

<p>2）如果私有对象是空的时候，就去当前子池的共享列表获取（需要加锁）；</p>

<p>3）如果当前子池的共享列表也是空的，那么就尝试去其他P的子池的共享列表偷取一个（需要加锁）；</p>

<p>4）如果其他子池都是空的，最后就用用户指定的 New 函数产生一个新的对象返回。</p>

<p>可以看到一次 get 操作最少 0 次加锁，最大 N（N 等于 MAXPROCS）次加锁。</p>

<p>归还对象的过程：</p>

<p>1）固定到某个 P，如果私有对象为空则放到私有对象；</p>

<p>2）否则加入到该 P 子池的共享列表中（需要加锁）。</p>

<p>可以看到一次 put 操作最少 0 次加锁，最多 1 次加锁。</p>

<p>由于 goroutine 具体会分配到那个 P 执行是 golang 的协程调度系统决定的，因此在 MAXPROCS&gt;1 的情况下，多 goroutine 用同一个 sync.Pool 的话，各个 P 的子池之间缓存的对象是否平衡以及开销如何是没办法准确衡量的。但如果 goroutine 数目和缓存的对象数目远远大于 MAXPROCS 的话，概率上说应该是相对平衡的。</p>

<p>总的来说，sync.Pool 的定位不是做类似连接池的东西，它的用途仅仅是增加对象重用的几率，减少 gc 的负担，而开销方面也不是很便宜的。</p>

<p>原生sync.Pool的问题是，Pool中的对象会被GC清理掉，这使得sync.Pool只适合做简单地对象池，不适合作连接池。
为何不适合作连接池
对象的数量和期限
pool创建时不能指定大小，没有数量限制。pool中对象会被GC清掉，只存在于两次GC之间。实现是pool的init方法注册了一个poolCleanup()函数，这个方法在GC之前执行，清空pool中的所有缓存对象。
池对象Get/Put开销
为使多协程使用同一个POOL。最基本的想法就是每个协程，加锁去操作共享的POOL，这显然是低效的。而进一步改进，类似于ConcurrentHashMap（JDK7）的分Segment，提高其并发性可以一定程度性缓解。
注意到pool中的对象是无差异性的，加锁或者分段加锁都不是较好的做法。go的做法是为每一个绑定协程的P都分配一个子池。每个子池又分为私有池和共享列表。共享列表是分别存放在各个P之上的共享区域，而不是各个P共享的一块内存。协程拿自己P里的子池对象不需要加锁，拿共享列表中的就需要加锁了。
Get对象过程：</p>

<p>goroutine固定到某一个P后，先从当前子池私区拿。并置私有对象为空。
拿不到再从当前子池共享列表拿，需要加锁。
仍拿不到从其它子池共享列表拿，需要加锁。
仍拿不到，sync.pool.New闭包非空，则New一个对象。
所以最坏的情况下遍历其它P才拿到对象，最大值为MACPROCS。</p>

<p>Put过程：</p>

<p>固定P中私有对象为空，则放到私有对象。
否则放入当前子池的共享列表，加锁实现。
开销为最多一次加锁。</p>

<p>如何解决Get最坏情况遍历所有P才获取得对象呢：</p>

<p>能够设置加锁期间遍历其它P的最大次数，遍历不到就直接创建，减少加锁占用pool的时间。
使各子池共享列表中的对象数量尽量平均化，从而避免最坏的情况发生。</p>

<p>方法1止前sync.pool并没有这样的设置。方法2由于goroutine被分配到哪个P由调度器调度不可控，无法确保其平衡。
由于不可控的GC导致生命周期过短，且池大小不可控，因而不适合作连接池。仅适用于增加对象重用机率，减少GC负担。2</p>

<p>pool在一定的使用条件下提高并发性能，条件1是协程数远大于GOMAXPROCS，条件2是池中对象远大于GOMAXPROCS。归结成一个原因就是使对象在各个P中均匀分布。
关于何时回收Pool
池pool和缓存cache的区别。池的意思是，池内对象是可以互换的，不关心具体值，甚至不需要区分是新建的还是从池中拿出的。缓存指的是KV映射，缓存里的值互不相同，清除机制更为复杂。缓存清除算法如LRU、LIRS缓存算法。
池空间回收的几种方式。一些是GC前回收，一些是基于时钟或弱引用回收。最终确定在GC时回收Pool内对象，即不回避GC。用java的GC解释弱引用。GC的四种引用：强引用、弱引用、软引用、虚引用。虚引用即没有引用，弱引用GC但有空间则保留，软引用GC即清除。ThreadLocal的值为弱引用的例子。
Pool其它场景
regexp包为了保证并发时使用同一个正则，而维护了一组状态机。
fmt包做字串拼接，从sync.pool拿[]byte对象。避免频繁构建再GC效率高很多。
var ppFree = sync.Pool{
    New: func() interface{} { return new(pp) },
}</p>

<p>pool关键作用:
减轻GC的压力。
复用对象内存。有时不一定希望复用内存，单纯是想减轻GC压力也可主动给pool塞对象。
Pool’s purpose is to cache allocated but unused items for later reuse, relieving pressure on the garbage collector. That is, it makes it easy to build efficient, thread-safe free lists. However, it is not suitable for all free lists.</p>

<p>原理简述
sync.Pool就是围绕New字段、Get和Put方法来使用。用过都懂，比较简单就不介绍了。</p>

<p>Go是提供goroutine进行并发编程，在并发环境下，sync.Pool的使用不会造成严重性能问题是它的设计考虑点。</p>

<p>容易想到的方法是Pool对象为每个P都分配一个空间，这样在P上运行的G进行Get和Put操作时，就可以在P本地的空间上读写。这样方法比Pool对象维护一个全局空间有明显好处，全局空间的读写肯定要加锁。</p>

<p>即使每个P都有了自己的本地空间，也不是说就可以完全避免锁使用。不要忘了Pool提供了内存复用功效，每个P上的G都使用的是P本地的空间的话，那内存复用就有局限性，只能局限在一个P上。</p>

<p>而pool提供的内存复用是覆盖所有P。意思是，一个G在执行Get方法时，发生G所在的P上，没有可复用的对象。这时就到别的P那儿去偷。偷这个动作就要加锁了。因为偷取别人可复用对象时候，别人也可能同时在读写。</p>

<p>前面开始说每个P有自己的空间，作用是避免锁，后面又说到别的P上偷对象，又要加锁。是不是矛盾了。</p>

<p>不矛盾，让我们来看看sync.Pool的实现原理。</p>

<p>sync.Pool对象底层两个关键字段，local和localSize，前者是指向一个数组，数组大小存在localSize。localSize的大小跟P个数保持一致。数组每个元素就是代表每个P自己的本地空间，类型是poolLocal。</p>

<p>poolLocal类型有两个关键字段，private和shared：</p>

<p>shared是一个数组，读写要加锁。
private只能存一个对象，读写不加锁。
来理一下在Pool对象上读写的逻辑：</p>

<p>Get操作时，先返回本地P上的private上的对象。
如果private为空，继续从本地P上的shared找，这里要加锁。
如果shared也没有，就到别的P那儿，从shared里偷。
所有其它P都遍历过了，没有任何对象可偷。就返回nil或调用New函数。</p>

<p>Put操作时，优先放private。</p>

<p>private已经被放了，那就放到shared的最后。</p>

<p>ync.Pool的特性
无大小限制。
自动清理，每次GC前会清掉Pool里的所有对象。所以不适用于做连接池。
每个P都会有一个本地的poolLocal，Get和Put优先在当前P的本地poolLocal操作。其次再进行跨P操作。
所以Pool的最大个数是runtime.GOMAXPROCS(0)。
sync.Pool的缺点
pool的Get()并非成本低廉，最坏情况可能会上锁runtime.GOMAXPROCS(0)次。</p>

<p>所以，多Goroutine与多P的情况下，使用Pool的效果才会突显。否则要经历无谓的锁成本。</p>

<p>简单的常用场景
bytes.Buffer作为临时对象放在池子里，这样减轻每次都需要创建的消耗。</p>

<p>1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
type Dao struct {
    bp      sync.Pool
}</p>

<p>func New(c *conf.Config) (d *Dao) {
    d = &amp;Dao{
        bp: sync.Pool{
            New: func() interface{} {
                return &amp;bytes.Buffer{}
            },
        },
    }
    return
}</p>

<p>func (d *Dao) Infoc(args …string) (value string, err error) {
    if len(args) == 0 {
        return
    }</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// fetch a buf from bufpool
buf, ok := d.bp.Get().(*bytes.Buffer)
if !ok {
    return "", ErrType
}

// append first arg
if _, err := buf.WriteString(args[0]); err != nil {
    return "", err
}

for _, arg := range args[1:] {
    // append ,arg
    if _, err := buf.WriteString(defaultSpliter); err != nil {
        return "", err
    }

    if _, err := buf.WriteString(strings.Replace(arg, defaultSpliter, defaultReplacer, -1)); err != nil {
        return "", err
    }
}

value = buf.String()
buf.Reset()
d.bp.Put(buf)
return
</code></pre></div></div>

<p>}
带注释的源码
sync.Pool数据结构</p>

<p>1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
// pool 的数据结构
type Pool struct {
	noCopy noCopy
	// 指向一个数组，个数与P相等，每个元素的类型为poolLocalInternal
	local     unsafe.Pointer
	// local数组的大小
	localSize uintptr
	// 创建pool对象时，用户必须提供的new函数
	New func() interface{}
}</p>

<p>type poolLocalInternal struct {
	// 私有对象，每个P都有，用于不同g执行get和put可以无锁操作
	private interface{}
	// 共享对象数组，每个P都有一个，同一个P上不同g可以多次执行put方法，需要有地方能存储。并且别的p上的g可能过来偷，所以要加锁
	shared  []interface{}
	// 对shared进行加锁，private不用加锁
	Mutex
}</p>

<p>type poolLocal struct {
	poolLocalInternal</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Prevents false sharing on widespread platforms with
// 128 mod (cache line size) = 0 .
pad [128 - unsafe.Sizeof(poolLocalInternal{})%128]byte } Get方法
</code></pre></div></div>

<p>1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
func (p *Pool) Put(x interface{}) {
	if x == nil {
		return
	}
	// …
	// 拿到当前P对应的pool
	l := p.pin()
	if l.private == nil {
		// 私有区有位置的话直接放私有区
		l.private = x
		x = nil
	}
	runtime_procUnpin()
	if x != nil {
		// 否则放在共享区里
		l.Lock()
		l.shared = append(l.shared, x)
		l.Unlock()
	}
  // …
}</p>

<p>func (p *Pool) pin() *poolLocal {
	// 拿到当前P的ID
	pid := runtime_procPin()
	s := atomic.LoadUintptr(&amp;p.localSize)
	l := p.local
	if uintptr(pid) &lt; s {
		// 定义pool对象时，s取值为0。只有经过pinSlow后，p.localSize的值才被设置
		// 如果local数组已经初始化，就可以把对应P的本地pool返回
		return indexLocal(l, pid)
	}
	// 否则就得重建local
	return p.pinSlow()
}</p>

<p>func (p *Pool) pinSlow() *poolLocal {
	runtime_procUnpin()
	// 锁上所有的pool对象
	allPoolsMu.Lock()
	defer allPoolsMu.Unlock()
	pid := runtime_procPin()
	s := p.localSize
	l := p.local
	if uintptr(pid) &lt; s {
    // pinSlow是一个创建local的方法。在获得allPoolsMu锁前，可能被别的P先获取，这种情况下local就已经被初始化了
		// 所以在获得allPoolsMu锁后需要再检查一次uintptr(pid) &lt; s
		return indexLocal(l, pid)
	}
	if p.local == nil {
		allPools = append(allPools, p)
	}
	// local的大小默认就是P的个数
	size := runtime.GOMAXPROCS(0)
	local := make([]poolLocal, size)
	atomic.StorePointer(&amp;p.local, unsafe.Pointer(&amp;local[0])) // 设置local
	atomic.StoreUintptr(&amp;p.localSize, uintptr(size))         // 设置localSize
	return &amp;local[pid]
}
Put方法</p>

<p>1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
func (p *Pool) Put(x interface{}) {
	if x == nil {
		return
	}
	// …
	// 拿到当前P对应的pool
	l := p.pin()
	if l.private == nil {
		// 私有区有位置的话直接放私有区
		l.private = x
		x = nil
	}
	runtime_procUnpin()
	if x != nil {
		// 否则放在共享区里
		l.Lock()
		l.shared = append(l.shared, x)
		l.Unlock()
	}
	// …
}
runtime_procPin和runtime_procUnpin</p>

<p>1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
//go:linkname sync_runtime_procPin sync.runtime_procPin
//go:nosplit
func sync_runtime_procPin() int {
	return procPin()
}</p>

<p>//go:nosplit
func procPin() int {
	<em>g</em> := getg()
	mp := <em>g</em>.m</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mp.locks++
return int(mp.p.ptr().id) }
</code></pre></div></div>

<p>//go:linkname sync_atomic_runtime_procUnpin sync/atomic.runtime_procUnpin
//go:nosplit
func sync_atomic_runtime_procUnpin() {
	procUnpin()
}</p>

<p>//go:nosplit
func procUnpin() {
	<em>g</em> := getg()
	<em>g</em>.m.locks–
}</p>

<p>sync.Pool对象内部为每个P都分配了一个private区和shared区。</p>

<p>private区只能存放一个可复用对象，因为每个P在任意时刻只运行一个G，所以在private区上写入和取出对象是不用加锁的。</p>

<p>shared区可以放多个可复用对象，它本身是slice。进shared区就append，出shared区就slice[:last-1]。但shared区上写入和取出对象要加锁，因为别的G可能过来偷对象。</p>

<p>type poolLocalInternal struct {
  // 私有对象，每个P都有，用于不同G执行get和put可以无锁操作
  private interface{}
  // 共享对象数组，每个P都有一个，同一个P上不同G可以多次执行put方法，需要有地方能存储。并且别的P上的G可能过来偷，所以要加锁
  shared  []interface{}
  // 对shared进行加锁，private不用加锁
  Mutex
}</p>

<p>问题3 就是由于shared区是一个带锁的后进先出队列造成的。每次Pool.Get方法在调用时，执行顺序是：</p>

<p>先看当前P的private区是否为空。</p>

<p>加锁，看当前P的shared区是否为空。</p>

<p>加锁，循环遍历看其他P的shared区是否为空。</p>

<p>只要上面三步任意一步就不为空，就可以把缓存对象返回了。但若都为空，最后就得调用New方法返回对象。</p>

<p>// 遍历一次其他P的共享区，偷一个，每次尝试偷都得上锁
for i := 0; i &lt; int(size); i++ {
  // 定位到某个P上的shared区
  l := indexLocal(local, (pid+i+1)%int(size))
  l.Lock()
  last := len(l.shared) - 1
  if last &gt;= 0 {
    // 如果有缓存对象，就返回，并解锁
    x = l.shared[last]
    l.shared = l.shared[:last]
    l.Unlock()
    break
  }
  // 没有缓存对象，解锁，继续遍历下一个P
  l.Unlock()
}</p>

<p>这一顿的加锁操作和Mutex锁自带的阻塞唤醒开销，Get方法在极端情况下就会有性能问题。</p>

<p>问题1和2 都是由于每次GC时，遍历清空所有缓存对象造成的。</p>

<p>sync.Pool在init()中向runtime注册了一个cleanup方法，它在STW1阶段被调用的。如果它执行过久，就会硬生生延长STW1阶段耗时。</p>

<p>func init() {
  runtime_registerPoolCleanup(poolCleanup)
}
这个cleanup方法干的事情是遍历所有的sync.Pool对象，再遍历每个sync.Pool对象中的每个P的shared区，把shared区每个缓存对象设置为nil。代码中就是三层for循环，简单粗暴时间复杂度高。</p>

<p>func poolCleanup() {
  // …
  for i, p := range allPools {
    // 有多少个Sync.Pool对象，遍历多少次
    allPools[i] = nil
    for i := 0; i &lt; int(p.localSize); i++ {
      // 有多少个P，遍历多少次
      l := indexLocal(p.local, i)
      l.private = nil
      for j := range l.shared {
        // 清空shared区中每个缓存对象
        l.shared[j] = nil
      }
      l.shared = nil
    }
    // …
  }
  // …
}</p>

<p>好消息是1.13beta1已经解决了这三个问题。注意是beta版本，而不是stable版本。</p>

<p>接下来主要看1.13通过什么思路解决这些问题的。</p>

<p>不排除未来的stable版本或1.13的小版本会对这块实现做小改动。</p>

<p>取消每次GC默认对全部对象进行回收
解决问题1和2的思路就是不能每次全部回收。但该回收多少呢？</p>

<p>@aclements 提出了一种思路。这轮在sync.Pool中的对象，最快也在下轮GC才被回收。</p>

<p>https://github.com/golang/go/issues/22950#issuecomment-352935997</p>

<p>还记得上面说过每个P都有private区和shared区吗？现在每个P里两个区合在一起构成数组，给个名字叫local（其实也是源码中的实现）。1.13版本的实现中再引入一个victim，它结构与local一致。</p>

<p>// 1.13版本源码
type Pool struct {
  local     unsafe.Pointer // 实际指向[P]poolLocal
  localSize uintptr        // P的个数</p>

<p>victim     unsafe.Pointer // 指向上轮的local
  victimSize uintptr        // 指向上轮的localSize</p>

<p>New func() interface{}
}</p>

<p>type poolLocal struct {
  poolLocalInternal
  pad [128 - unsafe.Sizeof(poolLocalInternal{})%128]byte
}</p>

<p>type poolLocalInternal struct {
  private interface{}
  shared  poolChain
}</p>

<p>有了victim，Get和Put方法的步骤就有所变化：</p>

<p>Get时，先从local里尝试取出缓存对象（包括所有的P）。如果失败，就尝试从victim里取。</p>

<p>victim里也取对象失败，就调用New方法。</p>

<p>Put时，只放local里。</p>

<p>新的数据结构下，cleanup方法策略也有所变化，改为每次只把victim里的对象回收掉。然后victim再指向当前的local。</p>

<p>var (
  // 所有sync.Pool对象
  allPools []<em>Pool
  // 待回收的所有sync.Pool对象
  oldPools []</em>Pool
)
func poolCleanup() {
  for _, p := range oldPools {
    // 每次只回收victim
    p.victim = nil
    p.victimSize = 0
  }</p>

<p>for _, p := range allPools {
    // victim指向当前的local
    p.victim = p.local
    p.victimSize = p.localSize
    p.local = nil
    p.localSize = 0
  }
  oldPools, allPools = allPools, nil
}</p>

<p>显然这样好处就是这轮的缓存对象在GC时不会立马回收，而是存放起来，滞后一轮。这样下一轮能得到复用机会，提高了缓存对象的命中率。并且回收对象时，由对shared区O(n)的遍历操作，变成O(1)。</p>

<p>从benchmark感受这个优化带来的性能提升：</p>

<h1 id="197">1.9.7</h1>
<p>BenchmarkPoolSTW-8     p96-ns/STW 285485 p50-ns/STW 190467</p>
<h1 id="113beta1">1.13beta1</h1>
<p>BenchmarkPoolSTW-8     p96-ns/STW 7720  p50-ns/STW 4979</p>

<p>1.9.7版本的STW1阶段耗时TP96线是285485ns，而1.13beta1是7720ns。</p>

<p>Benchmark代码参考1.13beat1源码src/sync/pool_test.go.BenchmarkPoolSTW方法</p>

<p>使用无锁队列替换shared区
问题3 是因为在shared的访问加了一把Mutex锁造成的。如果不消除这把锁，引入victim区也是徒劳。因为此时victim的访问也得加锁。</p>

<p>旧实现中shared区是单纯的带锁后进先出队列，1.13beta版本改成了单生产者，多消费者的双端无锁环形队列。</p>

<p>单生产者是指，每个P上运行的G，执行Put方法时，就往队列里存放缓存对象（别的P上运行的G不能往里放），并且只能放在队列头部。由于每个P任意时刻只有一个G被运行，所以存放缓存对象不需要加锁。</p>

<p>多消费者分两种角色，一是在P上运行的G，执行Get方法时，从队列头部取出缓存对象。同上，取对象不用加锁；二是在其他P上运行的G，执行Get方法时，本地没有缓存对象，就到别的P上偷。此时盗窃者G只能从队列尾部取出对象，因为盗窃者可能有多个，所以尾部取数据用CAS来实现无锁。</p>

<p>注意，每个P都持有自己无锁队列，并且队列也可能有多个</p>

<p>每个P持有的循环队列初始化多大呢？增长和收缩策略呢？</p>

<p>shared区改用双向链表，每个链表节点指向一个无锁环形队列。</p>

<p>链表节点必须在头部插入。</p>

<p>当前P上的G取缓存对象时，只从头部链表节点指向的无锁队列里取。取不到，沿着prev指针到下一个无锁队列上重复操作，也没有的话。就到别的P上偷。</p>

<p>盗窃者G在偷缓存对象时，只从尾部链表节点指向的无锁队列里取。取不到，沿着next指针到下一个无锁队列上重复操作，也没有的话。就到别的P上继续偷，直到都偷不着，就调用New方法。</p>

<p>链表首次插入节点时，指向无锁队列初始化大小为8，增长策略为在头部插入新节点，指向的无锁队列大小为旧头部节点指向无锁队列大小的两倍，始终保持2的n次方大小。</p>

<p>假如在链表长度为3的情况下。尾部节点指向的无锁队列里缓存对象被偷光了。那么尾部节点会沿着next指针前移，把旧的无锁队列内存释放掉。此时链表长度变为2，这就是链表的收缩策略。最小时剩下一个节点，不会收缩成空链表。</p>

<p>无锁队列的自身最大的大小是2**30。达到上限时，再执行Put操作就放不进去，也不报错。</p>

<p>1.sync.Pool是什么？
Golang在 1.3 版本的时候，在sync包中加入一个新特性：Pool。 简单的说：就是一个临时对象池。</p>

<p>2.为什么需要sync.Pool？
保存和复用临时对象，减少内存分配，降低GC压力。</p>

<p>（对象越多GC越慢，因为Golang进行三色标记回收的时候，要标记的也越多，自然就慢了）</p>

<p>3.如何使用sync.Pool？
func main() {
	// 初始化一个pool
	pool := &amp;sync.Pool{
		// 默认的返回值设置，不写这个参数，默认是nil
		New: func() interface{} {
			return 0
		},
	}</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// 看一下初始的值，这里是返回0，如果不设置New函数，默认返回nil
init := pool.Get()
fmt.Println(init)

// 设置一个参数1
pool.Put(1)

// 获取查看结果
num := pool.Get()
fmt.Println(num)

// 再次获取，会发现，已经是空的了，只能返回默认的值。
num = pool.Get()
fmt.Println(num) }
</code></pre></div></div>

<p>复制代码
使用较为简单。 总的思路就是：搞一个池子，预先放入临时产生的对象，然后取出使用。</p>

<p>可能有同学问了，这个玩意儿官方出的，那他自己有在用吗？ 答案是有的，其实你也一直在用。</p>

<p>就是fmt包啦，由于fmt总是需要很多[]byte对象，索性就直接建了一个[]byte对象的池子，来走一波代码。</p>

<p>type buffer []byte
// printer状态的结构体（）
type pp struct {
    …
}</p>

<p>// pp的对象池， 《====这里用到了。
var ppFree = sync.Pool{
    New: func() interface{} { return new(pp) },
}</p>

<p>// 每次需要pp结构体的时候，都过sync.Pool进行获取。
func newPrinter() <em>pp {
    p := ppFree.Get().(</em>pp)
    p.panicking = false
    p.erroring = false
    p.fmt.init(&amp;p.buf)
    return p
}
复制代码
4.走一波源码
4.1 基础数据结构
type Pool struct {
	// noCopy，防止当前类型被copy，是一个有意思的字段，后文详说。
	noCopy noCopy</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// [P]poolLocal 数组指针
local     unsafe.Pointer
// 数组大小
localSize uintptr        

// 选填的自定义函数，缓冲池无数据的时候会调用，不设置默认返回nil
New func() interface{} //新建对象函数 }
</code></pre></div></div>

<p>type poolLocalInternal struct {
    // 私有缓存区
	private interface{} <br />
	// 公共缓存区
	shared  []interface{} 
	// 锁
	Mutex             <br />
}</p>

<p>type poolLocal struct {
	// 每个P对应的pool
	poolLocalInternal</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// 这个字段很有意思，是为了防止“false sharing/伪共享”，后文详讲。
pad [128 - unsafe.Sizeof(poolLocalInternal{})%128]byte } 复制代码 来一张全景图，更有利于全局角度看这个结构体：
</code></pre></div></div>

<p>在这里插入图片描述
这边有两个小问题：
noCopy的作用？
poolLocal中pad的作用？
如何确定要获取的数据在哪个poolLocal里头？
带着问题，继续往下看，看完就能懂这两个小问题拉。</p>

<p>4.2 pin
在介绍get/put前，关键的基础函数pin需要先了解一下。 一句话说明用处：确定当前P绑定的localPool对象 （这里的P，是MPG中的P，如果看不懂请点这里：关于goroutine的一些小理解）</p>

<p>func (p *Pool) pin() *poolLocal {
	// 返回当前 P.id &amp;&amp; 设置禁止抢占（避免GC）
	pid := runtime_procPin()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// 根据locaSize来获取当前指针偏移的位置
s := atomic.LoadUintptr(&amp;p.localSize) 
l := p.local         

// 有可能在运行中动调调整P，所以这里进行需要判断是否越界
if uintptr(pid) &lt; s {
    // 没越界，直接返回
	return indexLocal(l, pid)
}

// 越界时，会涉及全局加锁，重新分配poolLocal，添加到全局列表
return p.pinSlow() }
</code></pre></div></div>

<p>var (
	allPoolsMu Mutex
	allPools   []*Pool
)</p>

<p>func (p *Pool) pinSlow() *poolLocal {
	// 取消P的禁止抢占（因为后面要进行metux加锁）
	runtime_procUnpin()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// 加锁
allPoolsMu.Lock()
defer allPoolsMu.Unlock()

// 返回当前 P.id &amp;&amp; 设置禁止抢占（避免GC）
pid := runtime_procPin()

// 再次检查是否符合条件，有可能中途已被其他线程调用
s := p.localSize
l := p.local
if uintptr(pid) &lt; s {
	return indexLocal(l, pid)
}

// 如果数组为空，则新建Pool，将其添加到 allPools，GC以此获取所有 Pool 实例
if p.local == nil {
	allPools = append(allPools, p)
}

// 根据 P 数量创建 slice
size := runtime.GOMAXPROCS(0)
local := make([]poolLocal, size)

 // 将底层数组起始指针保存到 Pool.local，并设置 P.localSize
 // 这里需要关注的是：如果GOMAXPROCS在GC间发生变化，则会重新分配的时候，直接丢弃老的，等待GC回收。
atomic.StorePointer(&amp;p.local, unsafe.Pointer(&amp;local[0]))
atomic.StoreUintptr(&amp;p.localSize, uintptr(size))         

// 返回本次所需的 poolLocal
return &amp;local[pid] }
</code></pre></div></div>

<p>// 根据数据结构的大小来计算指针的偏移量
func indexLocal(l unsafe.Pointer, i int) <em>poolLocal {
	lp := unsafe.Pointer(uintptr(l) + uintptr(i)</em>unsafe.Sizeof(poolLocal{}))
	return (*poolLocal)(lp)
}
流程小记：</p>

<p>禁止抢占GC -&gt; 寻找偏移量 -&gt; 检查越界 -&gt;返回poolLocal
                                   -&gt;加锁重建pool，并添加到allPool
4.3 put
先说结论：优先放入private空间，后面再放入shared空间 现在开始分析：</p>

<p>func (p *Pool) Put(x interface{}) {
	if x == nil {
		return
	}</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// 这段代码，不需要关心，降低竞争的
if race.Enabled {
	if fastrand()%4 == 0 {
		// Randomly drop x on floor.
		return
	}
	race.ReleaseMerge(poolRaceAddr(x))
	race.Disable()
}

// 获取当前的poolLocal
l := p.pin()

// 如果private为nil，则优先进行设置，并标记x
if l.private == nil {
	l.private = x
	x = nil
}
runtime_procUnpin()

// 如果标记x不为nil，则将x设置到shared中
if x != nil {
	l.Lock()
	l.shared = append(l.shared, x)
	l.Unlock()
}

// 设置竞争可用了。
if race.Enabled {
	race.Enable()
} } 复制代码 4.4 get 先说结论：优先从private空间拿，再加锁从shared空间拿，还没有再从其他的PoolLocal的shared空间拿，还没有就直接new一个返回。 现在进行分析：
</code></pre></div></div>

<p>func (p *Pool) Get() interface{} {
    // 竞争相关的设置
	if race.Enabled {
		race.Disable()
	}</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// 获取当前的poolLocal
l := p.pin()

// 从private中获取
x := l.private
l.private = nil
runtime_procUnpin()

// 不存在，则继续从shared空间拿，
if x == nil {
    // 加锁了，防止并发 
	l.Lock()
	last := len(l.shared) - 1
	if last &gt;= 0 {
		x = l.shared[last]
        // 从尾巴开始拿起
		l.shared = l.shared[:last]
	}
	l.Unlock()
	if x == nil {
	    // 从其他的poolLocal中的shared空间看看有没有可返回的。
		x = p.getSlow()
	}
}

// 竞争解除
if race.Enabled {
	race.Enable()
	if x != nil {
		race.Acquire(poolRaceAddr(x))
	}
}

// 如果还是没有的话，就直接new一个了
if x == nil &amp;&amp; p.New != nil {
	x = p.New()
}
return x }
</code></pre></div></div>

<p>func (p *Pool) getSlow() (x interface{}) {
    // 获取poolLocal数组的大小
	size := atomic.LoadUintptr(&amp;p.localSize) // load-acquire
	local := p.local                         // load-consume</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// 尝试从其他procs获取一个P对象
pid := runtime_procPin()
runtime_procUnpin()

for i := 0; i &lt; int(size); i++ {
    // 获取一个poolLocal，注意这里是从当前的local的位置开始获取的，目的是防止取到自身
	l := indexLocal(local, (pid+i+1)%int(size))
	// 加锁从尾部获取shared的数据
	l.Lock()
	last := len(l.shared) - 1
    // 若长度大于1
	if last &gt;= 0 {
		x = l.shared[last]
		l.shared = l.shared[:last]
		l.Unlock()
		break
	}
	l.Unlock()
}
return x }
</code></pre></div></div>

<p>复制代码
5.源码关键点解析
5.1 定时清理
Q：这里的pool的是永久保存的吗？还是？ A：是会进行清理的，时间就是两次GC间隔的时间。</p>

<p>// 注册清理函数，随着runtime进行的，也就是每次GC都会跑一下
func init() {
	runtime_registerPoolCleanup(poolCleanup)
}</p>

<p>// 清理函数也很粗暴，直接遍历全局维护的allPools将private和shared置为nil
func poolCleanup() {
    // 遍历allPools
	for i, p := range allPools {
	    // pool置为nil
		allPools[i] = nil</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    // 遍历localSIze的数量次
	for i := 0; i &lt; int(p.localSize); i++ {
		l := indexLocal(p.local, i)
        // private置为nil
		l.private = nil
        
        // 遍历shared，都置为nil
		for j := range l.shared {
			l.shared[j] = nil
		}
		l.shared = nil
	}
	p.local = nil
	p.localSize = 0
}
 
// allPools重置
allPools = []*Pool{} }
</code></pre></div></div>

<p>所以呢，这也说明为什么sync.Pool不适合放做“数据库连接池”等带持久性质的数据，因为它会定期回收啊～</p>

<p>5.2 为什么获取shared要加锁，而private不用？
我们知道golang是MPG的方式运行的，（关于goroutine的一些小理解）</p>

<p>大概这么个感觉吧：</p>

<p>M——P—– poolLocal  <br />
       |      <br />
       G - G
           |
           G
          …
M——P—– poolLocal<br />
       |
       G—G
           |
           G
          …
复制代码
也就是说，每个P都分配一个localPool，在同一个P下面只会有一个Gouroutine在跑，所以这里的private，在同一时间就只可能被一个Gouroutine获取到。</p>

<p>而shared就不一样了，有可能被其他的P给获取走，在同一时间就只可能被多个Gouroutine获取到，为了保证数据竞争，必须加一个锁来保证只会被一个G拿走。</p>

<p>5.3 noCopy的作用？
防止Pool被拷贝，因为Pool 在Golang是全剧唯一的</p>

<p>这里又衍生一个问题，这里的noCopy如何实现被防止拷贝的？？？</p>

<p>Golang中没有原生的禁止拷贝的方式，所以结构体不希望被拷贝，所以go作者做了这么一个约定：只要包含实现 sync.Locker 这个接口的结构体noCopy，go vet 就可以帮我们进行检查是否被拷贝了。</p>

<p>5.4 pad的作用？
这个挺有意思的，源代码出现这么一个词：false sharing，翻译为“伪共享”。 也就是说这个字段，主要就是用来防止“伪共享”的。</p>

<p>为什么会有false sharing？</p>

<p>简单说明一下：缓存系统中是以缓存行为单位存储的。缓存行通常是 64 字节，当缓存行加载其中1个字节时候，其他的63个也会被加载出来，加锁的话也会加锁整个缓存行，当下图所示x、y变量都在一个缓存行的时候，当进行X加锁的时候，正好另一个独立线程要操作Y，这会儿Y就要等X了，此时就不无法并发了。</p>

<p>由于这里的竞争冲突来源自共享，所以称之为伪共享。
如何防止？</p>

<p>补齐缓存行，让每个数据都是独立的缓存行就不会出现false sharding了。</p>

<p>5.5 怎么确定我的数据应该存储在LocalPool数组的哪个单元？
根据数据结构的大小来计算指针的偏移量，进而算出是LocalPool数组的哪个。</p>

<p>5.6 sync.Pool的设计哲学？
Goroutine能同一时刻在并行的数量有限，是由runtime.GOMAXPROCS(0)设置的，这里的Pool将数据与P进行绑定了，分散在了各个真正并行的线程中，每个线程优先从自己的poolLocal中获取数据，很大程度上降低了锁竞争。</p>

:ET