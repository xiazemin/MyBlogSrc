I"<p>robots.txt是用来告诉搜索引擎网站上哪些内容可以被访问、哪些不能被访问。当搜索引擎访问一个网站的时候，它首先会检查网站是否存在robots.txt，如果有则会根据文件命令访问有权限的文件。</p>

<p>为什么要写robots.txt，主要有四点：</p>

<p>1、保护网站安全</p>

<p>2、节省流量</p>

<p>3、禁止搜索引擎收录部分页面</p>

<p>4、引导蜘蛛爬网站地图
<!-- more --></p>

<p>#robots.txt的写法与步骤</p>

<p>1、定义搜索引擎</p>

<p>用User-agent：来定义搜索引擎，其中*表示所有，Baiduspider表示百度蜘蛛，Googlebot表示谷歌蜘蛛。</p>

<p>也就是说User-agent：*表示定义所有蜘蛛，User-agent：Baiduspider表示定义百度蜘蛛。</p>

<p>2、禁止与允许访问</p>

<p>Disallow: /表示禁止访问，Allow: /表示允许访问。</p>

<p>在写robots.txt时需特别注意的是，/前面有一个英文状态下的空格（必须是英文状态下的空格）。</p>

<p>3、禁止搜索引擎访问网站中的某几个文件夹，以a、b、c为例，写法分别如下：</p>

<p>Disallow: /a/</p>

<p>Disallow: /b/</p>

<p>Disallow: /c/</p>

<p>3、禁止搜索引擎访问文件夹中的某一类文件，以a文件夹中的js文件为例，写法如下：</p>

<p>Disallow: /a/*.js</p>

<p>4、只允许某个搜索引擎访问，以Baiduspider为例，写法如下：</p>

<p>User-agent: Baiduspider</p>

<p>Disallow:</p>

<p>5、禁止访问网站中的动态页面</p>

<p>User-agent: *</p>

<p>Disallow: /<em>?</em></p>

<p>6、只允许搜索引擎访问某类文件，以htm为例，写法如下：</p>

<p>User-agent: *</p>

<p>Allow: .htm$</p>

<p>Disallow: /</p>

<p>7、禁止某个搜索引擎抓取网站上的所有图片，以Baiduspider为例，写法如下：</p>

<p>User-agent: F</p>

<p>Disallow: .jpg$</p>

<p>Disallow: .jpeg$</p>

<p>Disallow: .gif$</p>

<p>Disallow: .png$</p>

<p>Disallow: .bmp$</p>

<p>三、robots.txt文件存放位置</p>

<p>robots.txt文件存放在网站根目录下，并且文件名所有字母都必须小写。</p>

<p>四、特别注意事项</p>

<p>在写robots.txt文件时语法一定要用对，User-agent、Disallow、Allow、Sitemap这些词都必须是第一个字母大写，后面的字母小写，而且在:后面必须带一个英文字符下的空格。</p>

<p>网站上线之前切记写robots.txt文件禁止蜘蛛访问网站，如果不会写就先了解清楚写法之后再写，以免给网站收录带来不必要的麻烦。</p>

<p>robots.txt文件生效时间在几天至一个月之间，站长自身无法控制。但是，站长可以在百度统计中查看网站robots.txt文件是否生效。</p>

<p>#一键部署的shell脚本例子：</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="code"><pre><span class="c">#!/bin/bash  </span>
  <span class="c">#网站根目录定义  </span>
<span class="nv">root_dir</span><span class="o">=(</span><span class="s2">"/var/www/"</span><span class="o">)</span>  
  <span class="c">#构建爬虫规则  </span>
<span class="k">for </span><span class="nb">dir </span><span class="k">in</span> <span class="k">${</span><span class="nv">root_dir</span><span class="p">[*]</span><span class="k">}</span>  
<span class="k">do</span>  
    <span class="c">#删除过期的robots.txt文件  </span>
    <span class="k">if</span> <span class="o">[</span> <span class="nt">-f</span> <span class="nv">$dir</span>/robots.txt <span class="o">]</span><span class="p">;</span> <span class="k">then  
        </span><span class="nb">rm</span> <span class="nt">-r</span> <span class="nv">$dir</span>/robots.txt  
    <span class="k">fi</span>  
  <span class="c">#增加新的爬虫规则  </span>
    <span class="nb">echo</span> <span class="s2">"User-agent: *"</span> <span class="o">&gt;</span><span class="nv">$dir</span>/robots.txt  
    <span class="nb">echo</span> <span class="s2">"Disallow: /"</span> <span class="o">&gt;&gt;</span><span class="nv">$dir</span>/robots.txt  
  <span class="c">#修改权限  </span>
    <span class="nb">chown </span>www-data.www-data <span class="nv">$dir</span>/robots.txt  
<span class="k">done</span>  
</pre></td></tr></tbody></table></code></pre></figure>

<p>#在线生成工具：
http://tool.chinaz.com/robots/</p>
:ET