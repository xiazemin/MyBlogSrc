I"f{<p>先看下consumer端发起调用时的链路流程：</p>

<p>+—————————+            +—————————+            +—————————+            <br />
 |      helloService         |            |      proxy                |            |  InvokerInvocationHandler |            <br />
 |      sayHello             +———-&gt; |      sayHello             +———-&gt; |  invoke                   |            <br />
 |                           |            |                           |            |  proxy method args        |            <br />
 +—————————+            +—————————+            +————-+————-+            <br />
                                                                                                 |                          <br />
                                                                                                 |                          <br />
                                                                                  +———————————+       <br />
                                                                                  |              |                  |       <br />
                                                                                  | +————v————–+   |       <br />
                                                                                  | |  MockClusterInvoker       |   |       <br />
                                                                                  | |  invoke                   |   |       <br />
                                                                                  | |                           |   |       <br />
                                                                                  | +————+————–+   |       <br />
                                                                                  |              |                  |       <br />
                                                                                  |              |                  |       <br />
                                                                                  |              |                  |       <br />
 +—————————+            +—————————+           | +————v————–+   |       <br />
 | Router                    |            | RegistryDirectory         |           | |  FailoverClusterInvoker   |   |       <br />
 | route                     | &lt;———-+ list                      | &lt;———–+  invoke                   |   |       <br />
 | MockInVokersSelector      |            | INVOCATION–&gt;List INVOKER |           | |                           |   |       <br />
 +————+————–+            +—————————+           | +—————————+   |       <br />
              |                                                                   |                                 |       <br />
              |                                                                   +———————————+       <br />
              |                                                                 cluster invoke，分布式调用容错机制也是在这做                    <br />
              |                                                                                                             <br />
              |                                                                                                             <br />
              |                                                                                                             <br />
              |                                                                                                             <br />
              |                                                                                                             <br />
+————-v————-+             +—————————+             +—————————+           <br />
|  RandomLoadBalance        |             |InvokerDelegate            |             | ListenerInvokerWrap       |           <br />
|  select                   +———–&gt; |invoke                     +———–&gt; | invoke                    |           <br />
|  List INVOKER–&gt;INVOKER   |             |                           |             |                           |           <br />
+—————————+             +—————————+             +—————————+</p>

<!-- more -->
<ol>
  <li>引入zookeeper作为注册中心后，服务查找过程
从建立spring到netty client建立连接的调用栈：
NettyClient.doOpen() line: 66
NettyClient(AbstractClient).(URL, ChannelHandler) line: 94
NettyClient.(URL, ChannelHandler) line: 61
NettyTransporter.connect(URL, ChannelHandler) line: 37
Transporter$Adpative.connect(URL, ChannelHandler) line: not available
Transporters.connect(URL, ChannelHandler…) line: 67
HeaderExchanger.connect(URL, ExchangeHandler) line: 37
Exchangers.connect(URL, ExchangeHandler) line: 102
DubboProtocol.initClient(URL) line: 378
DubboProtocol.getSharedClient(URL) line: 344
DubboProtocol.getClients(URL) line: 321
DubboProtocol.refer(Class, URL) line: 303
ProtocolListenerWrapper.refer(Class, URL) line: 65
ProtocolFilterWrapper.refer(Class, URL) line: 62
Protocol$Adpative.refer(Class, URL) line: not available
RegistryDirectory.toInvokers(List) line: 405
RegistryDirectory.refreshInvoker(List) line: 228
RegistryDirectory.notify(List) line: 196
ZookeeperRegistry(AbstractRegistry).notify(URL, NotifyListener, List) line: 449
ZookeeperRegistry(FailbackRegistry).doNotify(URL, NotifyListener, List) line: 273
ZookeeperRegistry(FailbackRegistry).notify(URL, NotifyListener, List) line: 259
ZookeeperRegistry.doSubscribe(URL, NotifyListener) line: 170
ZookeeperRegistry(FailbackRegistry).subscribe(URL, NotifyListener) line: 189
RegistryDirectory.subscribe(URL) line: 134
RegistryProtocol.doRefer(Cluster, Registry, Class, URL) line: 271
RegistryProtocol.refer(Class, URL) line: 254
ProtocolListenerWrapper.refer(Class, URL) line: 63
ProtocolFilterWrapper.refer(Class, URL) line: 60
Protocol$Adpative.refer(Class, URL) line: not available
ReferenceBean(ReferenceConfig).createProxy() line: 394
ReferenceBean(ReferenceConfig).init() line: 303
ReferenceBean(ReferenceConfig).get() line: 138
ReferenceBean.getObject() line: 65
DefaultListableBeanFactory(FactoryBeanRegistrySupport).doGetObjectFromFactoryBean(FactoryBean, String, boolean) line: 142</li>
</ol>

<p>整体来说： 先由注册中心的协议处理器处理注册中心的地址，找到所有provider的地址，创建所有invoker，然后再由invoker在真正调用时发起调用。
注册中心的这个也抽象一种协议，由注册中心结合提供者的协议推导出提供者的协议地址，也就是目标端的地址与端口得知了。
每一个接口在zookeeper上都有节点，节点下面是provider，再下面是所有provider的具体地址。</p>

<ol>
  <li>netty client的基本步骤
创建channelPipelineFactory，并在这个factory中返回加工好的ChannelPipeline，加工过程包括加入编解码器，连接事件处理组成的netty handler实现
（包括连接建立，断开，请求写出去，）</li>
</ol>

<p>writeRequested(netty的)–&gt;调用编码器（编码的这个对象中包括了 需要调用的目标接口名 方法名等信息）
（继承SimpleChannelHandler重写逻辑，可以定制channel的读取与写出逻辑）</p>

<ol>
  <li>
    <p>在使用zookeeper作为注册中心时，如果有provider服务停掉， consumer端如何感知？再次启动刚停掉的provider呢？
provider停掉会触发zk客户端的监听，监听对客户端的invoker列表进行刷新。
再次启动会触发 zk的监听，代码在ZkclientZookeeperClient</p>

    <p>public IZkChildListener createTargetChildListener(String path, final ChildListener listener) {
     return new IZkChildListener() {
         public void handleChildChange(String parentPath, List<String> currentChilds)
                 throws Exception {
             listener.childChanged(parentPath, currentChilds);
         }
     };
 }
然后再触发 com.alibaba.dubbo.registry.support.FailbackRegistry.doNotify(URL, NotifyListener, List)。
com.alibaba.dubbo.registry.integration.RegistryDirectory.refreshInvoker(List)， 这是在zk的event线程完成的。
如果有provider停掉了 走一样的监听逻辑</String></p>
  </li>
</ol>

<p>同时，dubbo支持 定时检查provider的状态并进行重连，具体参见
com.alibaba.dubbo.remoting.transport.AbstractClient.initConnectStatusCheckCommand()
reconnectExecutorService.scheduleWithFixedDelay(connectStatusCheckCommand, reconnect, reconnect, TimeUnit.MILLISECONDS);</p>

<ol>
  <li>
    <p>如果正在发服务的时候，provider停掉了，dubbo是如何处理的？
如果在发服务时，provider停掉了，那么此时会抛出异常，并在FailoverClusterInvoker doInvoke中捕获，
FailoverClusterInvoker支持调用失败时重试(可配置)，此时达到再次重试的目的。</p>
  </li>
  <li>
    <p>client在多次调用时，与provider端的连接是建立几次，在prodvider端服务状态有变化时呢？
NettyClient 的doOpen doConnect均在初始化的时候调用，有几个provider就调用几次，真正rpc调用服务的时候是不会再调用open与connect的。
上面这个说法不严格，因为看他发送消息的代码就知道了，每次发消息时还会检查下：</p>
  </li>
</ol>

<p>public void send(Object message, boolean sent) throws RemotingException {
    if (send_reconnect &amp;&amp; !isConnected()){
        connect();
    }
    Channel channel = getChannel();
    //TODO getChannel返回的状态是否包含null需要改进
    if (channel == null || ! channel.isConnected()) {
      throw new RemotingException(this, “message can not send, because channel is closed . url:” + getUrl());
    }
    channel.send(message, sent);
}</p>
<ol>
  <li>对于多个provider，dubbo默认在哪里选择了一个invoker进行调用的
com.alibaba.dubbo.rpc.cluster.support.AbstractClusterInvoker.select(LoadBalance, Invocation, List<Invoker>, List<Invoker>)。</Invoker></Invoker></li>
</ol>

<p>我们的Hello world应用服务，通过配置服务器Config Server获取到了我们配置的hello信息“hello world”. 但自己的配置文件中必须配置config server的URL（http://localhost:8888）, 如果把config server搬到另外一个独立IP上， 那么作为一个client的hello world应用必须修改自己的bootstrap.yml中的config server的URL地址。这明显是不够方便的。</p>

<p>既然config server已经注册到了eureka服务中心，能否让服务中心自动帮hello world应用找到它需要的config server呢？ 答案是肯定的。我们的hello world应用只需要提供它需要的配置所在在的config server的名字就可以了， 在前面例子中，配置服务的名字就是“config-server”。那我们现在就把之前的服务和应用稍作修改， 来达到自动发现服务的方案。下图是Spring Cloud提供的服务发现机制。Config-server是其中的Service Provider, Config-client是Service Consumer， 它们都注册到服务中心Eureka Server。</p>

<ol>
  <li>将config-server注册到服务中心</li>
</ol>

<p>config-server本身就是一个Spring Boot应用， 可以直接参考Spring Cloud 入门教程(一): 服务注册, 将config-server注册到eureka server中。访问http://localhost:8761, 可以看到我们的config-server已经注册。</p>

<ol>
  <li>修改hello world应用的配置</li>
</ol>

<p>1）.同样，需要将Hello 我让你的应用注册到eureka 服务中心, 配置方法同前面一样， 不在赘述。</p>

<p>2）.修改配置文件，将config-server的URL硬编码机制改成，通过服务中心根据名字自动发现机制， 修改bootstrap.yml</p>

<p>复制代码
 1 eureka:
 2   client:
 3     serviceUrl:
 4       defaultZone: http://localhost:8761/eureka/
 5 spring:
 6   application:
 7     name: config-client
 8   cloud:
 9     config:
10       label: master
11       profile: dev
12 #      uri: http://localhost:8888/
13       discovery:
14          enabled: true
15          serviceId: config-server
16 management:
17   security:
18     enabled: false
19 server:
20   port: 8881
复制代码
我们注释掉了硬编码的config-server的URL配置， 取而代之的是服务注册中心的地址http://localhost:8761/eureka/以及配置服务的名字“config-server”, 同时打开自动发现机制discovery.enable = true. 我们在运行一下hello world应用， 可以发现， GIT里面的内容依然可以访问。此时我们的hello world应用已经完全不知道配置服务的地址，也不知道配置的内容， 所有这些都通过服务注册中心自动发现。</p>

<ol>
  <li>当服务很多时，都需要同时从配置中心读取文件的时候，这时我们可以考虑将配置中心做成一个微服务，并且将其集群化，从而达到高可用</li>
</ol>

<p>https://helpcdn.aliyun.com/document_detail/130159.html</p>

<p>Ingress 介绍</p>

<p>Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；前两种估计都应该很熟悉，具体的可以参考下 这篇文章；下面详细的唠一下这个 Ingress。</p>

<p>Ingress 是个什么玩意？</p>

<p>可能从大致印象上 Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具；那么问题来了，集群内服务想要暴露出去面临着几个问题：</p>

<p>Pod 漂移问题</p>

<p>众所周知 Kubernetes 具有强大的副本控制能力，能保证在任意副本(Pod)挂掉时自动从其他机器启动一个新的，还可以动态扩容等，总之一句话，这个 Pod 可能在任何时刻出现在任何节点上，也可能在任何时刻死在任何节点上；</p>

<p>那么自然随着 Pod 的创建和销毁，Pod IP 肯定会动态变化；那么如何把这个动态的 Pod IP 暴露出去？</p>

<p>这里借助于 Kubernetes 的 Service 机制，Service 可以以标签的形式选定一组带有指定标签的 Pod，并监控和自动负载他们的 Pod IP，那么我们向外暴露只暴露 Service IP 就行了；这就是 NodePort 模式：即在每个节点上开起一个端口，然后转发到内部 Service IP 上，如下图所示：</p>

<p>端口管理问题</p>

<p>采用 NodePort 方式暴露服务面临一个坑爹的问题是，服务一旦多起来，NodePort 在每个节点上开启的端口会及其庞大，而且难以维护；这时候引出的思考问题是 “能不能使用 Nginx 啥的只监听一个端口，比如 80，然后按照域名向后转发？”</p>

<p>这思路很好，简单的实现就是使用 DaemonSet 在每个 node 上监听 80，然后写好规则，因为 Nginx 外面绑定了宿主机 80 端口(就像 NodePort)，本身又在集群内，那么向后直接转发到相应 Service IP 就行了，如下图所示：</p>

<p>域名分配及动态更新问题</p>

<p>从上面的思路，采用 Nginx 似乎已经解决了问题，但是其实这里面有一个很大缺陷：每次有新服务加入怎么改 Nginx 配置？总不能手动改或者来个 Rolling Update 前端 Nginx Pod 吧？这时候 “伟大而又正直勇敢的” Ingress 登场，如果不算上面的 Nginx，Ingress 只有两大组件：Ingress</p>

<p>Controller 和 Ingress。</p>

<p>Ingress 这个玩意，简单的理解就是 你原来要改 Nginx 配置，然后配置各种域名对应哪个 Service，现在把这个动作抽象出来，变成一个 Ingress 对象，你可以用 yml 创建，每次不要去改 Nginx 了，直接改 yml 然后创建/更新就行了；那么问题来了：”Nginx 咋整？”</p>

<p>Ingress Controller 这东西就是解决 “Nginx 咋整” 的；Ingress Controoler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取他，按照他自己模板生成一段 Nginx 配置，再写到 Nginx Pod 里，最后 reload 一下，工作流程如下图：</p>

<p>当然在实际应用中，最新版本 Kubernetes 已经将 Nginx 与 Ingress Controller 合并为一个组件，所以 Nginx 无需单独部署，只需要部署 Ingress Controller 即可。</p>

<p>怼一个 Nginx Ingress</p>

<p>上面啰嗦了那么多，只是为了讲明白 Ingress 的各种理论概念，下面实际部署很简单：</p>

<p>配置 ingress RBAC</p>

<p>cat nginx-ingress-controller-rbac.yml</p>

<p>#apiVersion: v1</p>

<p>#kind: Namespace</p>

<p>#metadata:</p>

<h1 id="name-kube-system">name: kube-system</h1>

<hr />

<p>apiVersion: v1</p>

<p>kind: ServiceAccount</p>

<p>metadata:</p>

<p>name: nginx-ingress-serviceaccount</p>

<p>namespace: kube-system</p>

<hr />

<p>apiVersion: rbac.authorization.k8s.io/v1beta1</p>

<p>kind: ClusterRole</p>

<p>metadata:</p>

<p>name: nginx-ingress-clusterrole</p>

<p>rules:</p>

<ul>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>””</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>
    <p>configmaps</p>
  </li>
  <li>
    <p>endpoints</p>
  </li>
  <li>
    <p>nodes</p>
  </li>
  <li>
    <p>pods</p>
  </li>
  <li>
    <p>secrets</p>
  </li>
</ul>

<p>verbs:</p>

<ul>
  <li>
    <p>list</p>
  </li>
  <li>
    <p>watch</p>
  </li>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>””</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>nodes</li>
</ul>

<p>verbs:</p>

<ul>
  <li>
    <p>get</p>
  </li>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>””</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>services</li>
</ul>

<p>verbs:</p>

<ul>
  <li>
    <p>get</p>
  </li>
  <li>
    <p>list</p>
  </li>
  <li>
    <p>watch</p>
  </li>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>“extensions”</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>ingresses</li>
</ul>

<p>verbs:</p>

<ul>
  <li>
    <p>get</p>
  </li>
  <li>
    <p>list</p>
  </li>
  <li>
    <p>watch</p>
  </li>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>””</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>events</li>
</ul>

<p>verbs:</p>

<ul>
  <li>
    <p>create</p>
  </li>
  <li>
    <p>patch</p>
  </li>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>“extensions”</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>ingresses/status</li>
</ul>

<p>verbs:</p>

<ul>
  <li>update</li>
</ul>

<hr />

<p>apiVersion: rbac.authorization.k8s.io/v1beta1</p>

<p>kind: Role</p>

<p>metadata:</p>

<p>name: nginx-ingress-role</p>

<p>namespace: kube-system</p>

<p>rules:</p>

<ul>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>””</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>
    <p>configmaps</p>
  </li>
  <li>
    <p>pods</p>
  </li>
  <li>
    <p>secrets</p>
  </li>
  <li>
    <p>namespaces</p>
  </li>
</ul>

<p>verbs:</p>

<ul>
  <li>
    <p>get</p>
  </li>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>””</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>configmaps</li>
</ul>

<p>resourceNames:</p>

<h1 id="defaults-to--">Defaults to “-“</h1>

<h1 id="here--">Here: “-“</h1>

<h1 id="this-has-to-be-adapted-if-you-change-either-parameter">This has to be adapted if you change either parameter</h1>

<h1 id="when-launching-the-nginx-ingress-controller">when launching the nginx-ingress-controller.</h1>

<ul>
  <li>“ingress-controller-leader-nginx”</li>
</ul>

<p>verbs:</p>

<ul>
  <li>
    <p>get</p>
  </li>
  <li>
    <p>update</p>
  </li>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>””</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>configmaps</li>
</ul>

<p>verbs:</p>

<ul>
  <li>
    <p>create</p>
  </li>
  <li>
    <p>apiGroups:</p>
  </li>
  <li>
    <p>””</p>
  </li>
</ul>

<p>resources:</p>

<ul>
  <li>endpoints</li>
</ul>

<p>verbs:</p>

<ul>
  <li>
    <p>get</p>
  </li>
  <li>
    <p>create</p>
  </li>
  <li>
    <p>update</p>
  </li>
</ul>

<hr />

<p>apiVersion: rbac.authorization.k8s.io/v1beta1</p>

<p>kind: RoleBinding</p>

<p>metadata:</p>

<p>name: nginx-ingress-role-nisa-binding</p>

<p>namespace: kube-system</p>

<p>roleRef:</p>

<p>apiGroup: rbac.authorization.k8s.io</p>

<p>kind: Role</p>

<p>name: nginx-ingress-role</p>

<p>subjects:</p>

<ul>
  <li>kind: ServiceAccount</li>
</ul>

<p>name: nginx-ingress-serviceaccount</p>

<p>namespace: kube-system</p>

<hr />

<p>apiVersion: rbac.authorization.k8s.io/v1beta1</p>

<p>kind: ClusterRoleBinding</p>

<p>metadata:</p>

<p>name: nginx-ingress-clusterrole-nisa-binding</p>

<p>roleRef:</p>

<p>apiGroup: rbac.authorization.k8s.io</p>

<p>kind: ClusterRole</p>

<p>name: nginx-ingress-clusterrole</p>

<p>subjects:</p>

<ul>
  <li>kind: ServiceAccount</li>
</ul>

<p>name: nginx-ingress-serviceaccount</p>

<p>namespace: kube-system</p>

<p>部署默认后端</p>

<p>我们知道 前端的 Nginx 最终要负载到后端 service 上，那么如果访问不存在的域名咋整？官方给出的建议是部署一个 默认后端，对于未知请求全部负载到这个默认后端上；这个后端啥也不干，就是返回 404，部署如下：</p>

<p>kubectl create -f default-backend.yaml</p>

<p>这个 default-backend.yaml 文件可以在 github Ingress 仓库 找到. 针对官方配置 我们单独添加了 nodeselector 指定，绑定LB地址 以方便DNS 做解析。</p>

<p>cat default-backend.yaml</p>

<p>apiVersion: extensions/v1beta1</p>

<p>kind: Deployment</p>

<p>metadata:</p>

<p>name: default-http-backend</p>

<p>labels:</p>

<p>k8s-app: default-http-backend</p>

<p>namespace: kube-system</p>

<p>spec:</p>

<p>replicas: 1</p>

<p>template:</p>

<p>metadata:</p>

<p>labels:</p>

<p>k8s-app: default-http-backend</p>

<p>spec:</p>

<p>terminationGracePeriodSeconds: 60</p>

<p>containers:</p>

<ul>
  <li>name: default-http-backend</li>
</ul>

<h1 id="any-image-is-permissable-as-long-as">Any image is permissable as long as:</h1>

<h1 id="1-it-serves-a-404-page-at-">1. It serves a 404 page at /</h1>

<h1 id="2-it-serves-200-on-a-healthz-endpoint">2. It serves 200 on a /healthz endpoint</h1>

<p>image: harbor-demo.dianrong.com/kubernetes/defaultbackend:1.0</p>

<p>livenessProbe:</p>

<p>httpGet:</p>

<p>path: /healthz</p>

<p>port: 8080</p>

<p>scheme: HTTP</p>

<p>initialDelaySeconds: 30</p>

<p>timeoutSeconds: 5</p>

<p>ports:</p>

<ul>
  <li>containerPort: 8080</li>
</ul>

<p>resources:</p>

<p>limits:</p>

<p>cpu: 10m</p>

<p>memory: 20Mi</p>

<p>requests:</p>

<p>cpu: 10m</p>

<p>memory: 20Mi</p>

<p>nodeSelector:</p>

<p>kubernetes.io/hostname: 172.16.200.209</p>

<hr />

<p>apiVersion: v1</p>

<p>kind: Service</p>

<p>metadata:</p>

<p>name: default-http-backend</p>

<p>namespace: kube-system</p>

<p>labels:</p>

<p>k8s-app: default-http-backend</p>

<p>spec:</p>

<p>ports:</p>

<ul>
  <li>port: 80</li>
</ul>

<p>targetPort: 8080</p>

<p>selector:</p>

<p>k8s-app: default-http-backend</p>

<p>部署 Ingress Controller</p>

<p>部署完了后端就得把最重要的组件 Nginx+Ingres Controller(官方统一称为 Ingress Controller) 部署上。</p>

<p>kubectl create -f nginx-ingress-controller.yaml</p>

<p>注意： 官方的 Ingress Controller 有个坑，默认注释了hostNetwork 工作方式。以防止端口的在宿主机的冲突。没有绑定到宿主机 80 端口，也就是说前端 Nginx 没有监听宿主机 80 端口(这还玩个卵啊)；所以需要把配置搞下来自己加一下 hostNetwork</p>

<p>cat  nginx-ingress-controller.yaml</p>

<p>apiVersion: extensions/v1beta1</p>

<p>kind: Deployment</p>

<p>metadata:</p>

<p>name: nginx-ingress-controller</p>

<p>labels:</p>

<p>k8s-app: nginx-ingress-controller</p>

<p>namespace: kube-system</p>

<p>spec:</p>

<p>replicas: 1</p>

<p>template:</p>

<p>metadata:</p>

<p>labels:</p>

<p>k8s-app: nginx-ingress-controller</p>

<p>spec:</p>

<h1 id="hostnetwork-makes-it-possible-to-use-ipv6-and-to-preserve-the-source-ip-correctly-regardless-of-docker-configuration">hostNetwork makes it possible to use ipv6 and to preserve the source IP correctly regardless of docker configuration</h1>

<h1 id="however-it-is-not-a-hard-dependency-of-the-nginx-ingress-controller-itself-and-it-may-cause-issues-if-port-10254-already-is-taken-on-the-host">however, it is not a hard dependency of the nginx-ingress-controller itself and it may cause issues if port 10254 already is taken on the host</h1>

<h1 id="that-said-since-hostport-is-broken-on-cni-httpsgithubcomkuberneteskubernetesissues31307-we-have-to-use-hostnetwork-where-cni-is-used">that said, since hostPort is broken on CNI (https://github.com/kubernetes/kubernetes/issues/31307) we have to use hostNetwork where CNI is used</h1>

<h1 id="like-with-kubeadm">like with kubeadm</h1>

<h1 id="hostnetwork-true">hostNetwork: true</h1>

<p>terminationGracePeriodSeconds: 60</p>

<p>hostNetwork: true</p>

<p>serviceAccountName: nginx-ingress-serviceaccount</p>

<p>containers:</p>

<ul>
  <li>image: harbor-demo.dianrong.com/kubernetes/nginx-ingress-controller:0.9.0-beta.1</li>
</ul>

<p>name: nginx-ingress-controller</p>

<p>readinessProbe:</p>

<p>httpGet:</p>

<p>path: /healthz</p>

<p>port: 10254</p>

<p>scheme: HTTP</p>

<p>livenessProbe:</p>

<p>httpGet:</p>

<p>path: /healthz</p>

<p>port: 10254</p>

<p>scheme: HTTP</p>

<p>initialDelaySeconds: 10</p>

<p>timeoutSeconds: 1</p>

<p>ports:</p>

<ul>
  <li>containerPort: 80</li>
</ul>

<p>hostPort: 80</p>

<ul>
  <li>containerPort: 443</li>
</ul>

<p>hostPort: 443</p>

<p>env:</p>

<ul>
  <li>name: POD_NAME</li>
</ul>

<p>valueFrom:</p>

<p>fieldRef:</p>

<p>fieldPath: metadata.name</p>

<ul>
  <li>name: POD_NAMESPACE</li>
</ul>

<p>valueFrom:</p>

<p>fieldRef:</p>

<p>fieldPath: metadata.namespace</p>

<p>args:</p>

<ul>
  <li>
    <p>/nginx-ingress-controller</p>
  </li>
  <li>
    <p>–default-backend-service=$(POD_NAMESPACE)/default-http-backend</p>
  </li>
</ul>

<h1 id="--default-ssl-certificatepod_namespaceingress-secret">- –default-ssl-certificate=$(POD_NAMESPACE)/ingress-secret</h1>

<p>nodeSelector:</p>

<p>kubernetes.io/hostname: 172.16.200.102</p>

<p>部署 Ingress</p>

<p>从上面可以知道 Ingress 就是个规则，指定哪个域名转发到哪个 Service，所以说首先我们得有个 Service，当然 Service 去哪找这里就不管了；这里默认为已经有了两个可用的 Service，以下以 Dashboard 为例。</p>

<p>先写一个 Ingress 文件，语法格式啥的请参考 官方文档，由于我的 Dashboard 都在kube-system 这个命名空间，所以要指定 namespace。</p>

<p>apiVersion: extensions/v1beta1</p>

<p>kind: Ingress</p>

<p>metadata:</p>

<p>name: dashboard-ingress</p>

<p>namespace: kube-system</p>

<p>annotations:</p>

<p>kubernetes.io/ingress.class: “nginx”</p>

<p>spec:</p>

<p>rules:</p>

<ul>
  <li>host: fox-dashboard.dianrong.com</li>
</ul>

<p>http:</p>

<p>paths:</p>

<ul>
  <li>backend:</li>
</ul>

<p>serviceName: kubernetes-dashboard</p>

<p>servicePort: 80</p>

<p>装逼成功截图如下</p>

<p>部署 Ingress TLS</p>

<p>上面已经搞定了 Ingress，下面就顺便把 TLS 怼上；官方给出的样例很简单，大致步骤就两步：创建一个含有证书的 secret、在 Ingress 开启证书；但是我不得不喷一下，文档就提那么一嘴，大坑一堆，比如多域名配置，TLS功能的启动都没。启用tls 需要在 nginx-ingress-controller添加参数，上面的controller以配置好。</p>

<p>–default-ssl-certificate=$(POD_NAMESPACE)/ingress-secret</p>

<p>证书格式转换</p>

<p>创建secret 需要使用你的证书文件，官方要求证书的编码需要使用base64。转换方法如下：</p>

<p>证书转换pem 格式：</p>

<p>openssl x509 -inform DER -in cert/doamin.crt -outform PEM  -out cert/domain.pem</p>

<p>证书编码转换base64</p>

<table>
  <tbody>
    <tr>
      <td>cat domain.crt</td>
      <td>base64 &gt; domain.crt.base64</td>
    </tr>
  </tbody>
</table>

<p>创建 secret ,需要使用base64 编码格式证书。</p>

<p>cat ingress-secret.yml</p>

<p>apiVersion: v1</p>

<p>data:</p>

<p>tls.crt: LS0tLS1CRU</p>

<p>tls.key: LS0tLS1CRU</p>

<p>kind: Secret</p>

<p>metadata:</p>

<p>name: ingress-secret</p>

<p>namespace: kube-system</p>

<p>type: Opaque</p>

<p>其实这个配置比如证书转码啥的没必要手动去做，可以直接使用下面的命令创建，kubectl 将自动为我们完整格式的转换。</p>

<p>kubectl create secret tls ingress-secret –key certs/ttlinux.com.cn-key.pem –cert certs/ttlinux.com.cn.pem</p>

<p>重新部署 Ingress</p>

<p>生成完成后需要在 Ingress 中开启 TLS，Ingress 修改后如下：</p>

<p>cat dashboard-ingress.yml</p>

<p>apiVersion: extensions/v1beta1</p>

<p>kind: Ingress</p>

<p>metadata:</p>

<p>name: dashboard-ingress</p>

<p>namespace: kube-system</p>

<p>annotations:</p>

<p>kubernetes.io/ingress.class: “nginx”</p>

<p>spec:</p>

<p>tls:</p>

<ul>
  <li>
    <p>hosts:</p>
  </li>
  <li>
    <p>fox-dashboard.dianrong.com</p>
  </li>
</ul>

<p>secretName: ingress-secret</p>

<p>rules:</p>

<ul>
  <li>host: fox-dashboard.dianrong.com</li>
</ul>

<p>http:</p>

<p>paths:</p>

<ul>
  <li>backend:</li>
</ul>

<p>serviceName: kubernetes-dashboard</p>

<p>servicePort: 80</p>

<p>注意：一个 Ingress 只能使用一个 secret(secretName 段只能有一个)，也就是说只能用一个证书，更直白的说就是如果你在一个 Ingress 中配置了多个域名，那么使用 TLS 的话必须保证证书支持该 Ingress 下所有域名；并且这个 secretName 一定要放在上面域名列表最后位置，否则会报错 did not find expected key 无法创建；同时上面的 hosts 段下域名必须跟下面的 rules 中完全匹配。</p>

<p>更需要注意一点：之所以这里单独开一段就是因为有大坑；Kubernetes Ingress 默认情况下，当你不配置证书时，会默认给你一个 TLS 证书的，也就是说你 Ingress 中配置错了，比如写了2个 secretName、或者 hosts 段中缺了某个域名，那么对于写了多个 secretName 的情况，所有域名全会走默认证书；对于 hosts 缺了某个域名的情况，缺失的域名将会走默认证书，部署时一定要验证一下证书，不能 “有了就行”；更新 Ingress 证书可能需要等一段时间才会生效。</p>

<p>最后重新部署一下即可：</p>

<p>kubectl delete -f dashboard-ingress.yml</p>

<p>kubectl create -f dashboard-ingress.yml</p>

<p>部署 TLS 后 80 端口会自动重定向到 443，最终访问截图如下：</p>

<p>ingress 高级用法</p>

<p>lvs 反向代理到 物理nginx。完成https拆包，继承nginx所有功能。</p>

<p>nginx 反向代理到ingress-control。 ingress-control 有两种部署方式 。</p>

<p>ingress-control 使用nodePort 方式暴漏服务</p>

<p>ingress-control 使用hostNetwork 方式暴漏服务。</p>

<p>总结分析</p>

<p>ingress-control 在自己的所属的namespace=ingress, 是可以夸不同namespace提供反向代理服.。</p>

<p>如果需要提供夸NS 访问ingress，先给 ingress-control创建RBAC。</p>

<p>ingress-control 使用hostnetwork 模式 性能比使用service nodePort 性能好很多。因为hostnetwork 是直接获取pod 的IP？</p>

:ET