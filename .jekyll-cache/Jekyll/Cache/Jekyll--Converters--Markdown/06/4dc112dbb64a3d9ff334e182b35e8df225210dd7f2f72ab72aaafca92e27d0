I"O<!-- more -->
<div class="container">
		<div class="row">
	  TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
	</div>
	<div class="row">
	<img src="https://xiazemin.github.io/MyBlog/img/TF.png" />
	</div>
	<div class="row">
	<img src="https://xiazemin.github.io/MyBlog/img/IDF.png" />
	</div>
		<img src="https://xiazemin.github.io/MyBlog/img/TF_IDF.png" />
	</div>

<div class="row">
LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。文档到主题服从多项式分布，主题到词服从多项式分布
1.对每一篇文档，从主题分布中抽取一个主题；
2.从上述被抽到的主题所对应的单词分布中抽取一个单词；
3.重复上述过程直至遍历文档中的每一个单词。
先定义一些字母的含义：文档集合D，主题（topic)集合T
D中每个文档d看作一个单词序列&lt;w1,w2,...,wn&gt;，wi表示第i个单词，设d有n个单词。（LDA里面称之为wordbag，实际上每个单词的出现位置对LDA算法无影响）
·D中涉及的所有不同单词组成一个大集合VOCABULARY（简称VOC），LDA以文档集合D作为输入，希望训练出的两个结果向量（设聚成k个topic，VOC中共包含m个词）：
·对每个D中的文档d，对应到不同Topic的概率θd&lt;pt1,...,ptk&gt;，其中，pti表示d对应T中第i个topic的概率。计算方法是直观的，pti=nti/n，其中nti表示d中对应第i个topic的词的数目，n是d中所有词的总数。
·对每个T中的topict，生成不同单词的概率φt&lt;pw1,...,pwm&gt;，其中，pwi表示t生成VOC中第i个单词的概率。计算方法同样很直观，pwi=Nwi/N，其中Nwi表示对应到topict的VOC中第i个单词的数目，N表示所有对应到topict的单词总数。
LDA的核心公式如下：
p(w|d)=p(w|t)*p(t|d)
直观的看这个公式，就是以Topic作为中间层，可以通过当前的θd和φt给出了文档d中出现单词w的概率。其中p(t|d)利用θd计算得到，p(w|t)利用φt计算得到。
实际上，利用当前的θd和φt，我们可以为一个文档中的一个单词计算它对应任意一个Topic时的p(w|d)，然后根据这些结果来更新这个词应该对应的topic。然后，如果这个更新改变了这个单词所对应的Topic，就会反过来影响θd和φt。
</div>

<div class="row">
Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。
</div>
<div class="row">
<img src="https://xiazemin.github.io/MyBlog/img/CBOW.jpeg" />
</div>
<p>Skip-Gram模型的基础形式非常简单，为了更清楚地解释模型，我们先从最一般的基础模型来看Word2Vec（下文中所有的Word2Vec都是指Skip-Gram模型）。</p>
:ET