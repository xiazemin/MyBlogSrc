I"ŞD<p>å®˜ç½‘åˆ›å»ºFlinké¡¹ç›®æœ‰ä¸¤ç§æ–¹å¼ï¼š</p>

<p>https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/java_api_quickstart.html</p>

<p>æ–¹å¼ä¸€ï¼š</p>

<p>mvn archetype:generate \</p>

<p>-DarchetypeGroupId=org.apache.flink \</p>

<p>-DarchetypeArtifactId=flink-quickstart-java\</p>

<p>-DarchetypeVersion=1.6.2</p>

<p>æ–¹å¼äºŒ</p>

<table>
  <tbody>
    <tr>
      <td>$ curlhttps://flink.apache.org/q/quickstart.sh</td>
      <td>bash -s 1.6.2</td>
    </tr>
  </tbody>
</table>

<p>è¿™é‡Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨ç¬¬ä¸€ç§æ–¹å¼åˆ›å»ºFlinké¡¹ç›®ã€‚</p>

<p>æ‰“å¼€ç»ˆç«¯ï¼Œåˆ‡æ¢åˆ°å¯¹åº”çš„ç›®å½•ï¼Œé€šè¿‡mavenåˆ›å»ºflinké¡¹ç›®
<!-- more -->
https://www.jianshu.com/p/fc2006e8e5f9</p>

<p>https://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException</p>

<p>mvn clean package</p>

<p>[ERROR] Failed to execute goal on project ArrayKeysToString: Could not resolve dependencies for project org.flink:ArrayKeysToString:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.apache.flink:flink-table_2.11:jar:1.4.2-1700, org.apache.flink:flink-streaming-java_2.11:jar:1.4.2-1700, org.apache.hive:hive-exec:jar:1.2.1-213: Could not find artifact org.apache.flink:flink-table_2.11:jar:1.4.2-1700 in alimaven (http://maven.aliyun.com/nexus/content/groups/public/) -&gt; [Help 1]</p>

<p>å…·ä½“æ“ä½œæ˜¯æ‰¾åˆ°è¿™ä¸ª<mirrors>çš„æ ‡ç­¾,æŠŠä¸‹é¢è¿™ä¸ªä»£ç åŠ åˆ°é‡Œé¢å°±è¡Œäº†,åŒæ ·æ³¨æ„ä¸è¦åŠ åœ¨æ³¨è§£é‡Œé¢å»äº†</mirrors></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> &lt;mirrors&gt;
&lt;mirror&gt;
  &lt;id&gt;alimaven&lt;/id&gt;
  &lt;name&gt;aliyun maven&lt;/name&gt;
  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;
  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;        
&lt;/mirror&gt;
</code></pre></div></div>
<p>&lt;/mirrors&gt;</p>

<p>è¿™ä¸ªæ˜¯é˜¿é‡Œå·´å·´æä¾›çš„é•œåƒåœ°å€,ä¹Ÿå¯ä»¥è‡ªå·±åœ¨ç½‘ä¸Šæ‰¾åˆ«çš„é•œåƒåœ°å€</p>

<p>https://www.cnblogs.com/hardingworking-miner/p/10155080.html</p>

<p>http://maven.apache.org/guides/mini/guide-mirror-settings.html</p>

<p>https://blog.csdn.net/xl890727/article/details/53942452</p>

<p>Mavené”™è¯¯ï¼šwas cached in the local repository, resolution will not be reattempted until the update</p>

<p>è§£å†³åŠæ³•ï¼šå‘½ä»¤ä¸­å¢åŠ ä¸ªå‚æ•°Uï¼Œå¦‚ä¸‹ï¼šÂ mvn -U clean install -Dmaven.test.skip=true</p>

<p>useÂ -UÂ parameter to force a check for the updated releases and snapshots on remote repositories, and resolve this issue.</p>

<p>https://flink.apache.org/downloads.html
https://www.cnblogs.com/dan-baishucaizi/p/12757367.html</p>
<mirror>
        <id>mapr-public</id>
        <mirrorOf>mapr-releases</mirrorOf>
        <name>mapr-releases</name>
        <url>https://maven.aliyun.com/repository/mapr-public</url>
    </mirror>
<p>å¿«é€Ÿç¼–è¯‘æŒ‡ä»¤
mvn clean package -DskipTests -Dfast</p>

<p>æ”¹ä¸ºå†…ç½‘repoé—®é¢˜è§£å†³äº†</p>

<p>æŠŠjaråŒ…èµ„æºä¸Šä¼ 
ç„¶å
â€“ åˆ›å»º UDF
ADD JAR â€˜hdfs://yarn-hdfs-test/aa-1.0-SNAPSHOT.jarâ€™;
CREATE FUNCTION â€˜addUdfâ€™ AS â€˜com.xx.streamsql.udf.AddUDFâ€™;</p>

<p>åœ¨streamsqlé‡Œé¢å°±å¯ä»¥ä½¿ç”¨addUdfå‡½æ•°äº†</p>

<p>æœ¬æ–‡ä¼šä¸»è¦è®²ä¸‰ç§udfï¼š</p>

<p>ScalarFunction</p>

<p>TableFunction</p>

<p>AggregateFunction</p>

<p>Â  Â  ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°æ˜¯éå¸¸é‡è¦çš„ä¸€ä¸ªç‰¹å¾ï¼Œå› ä¸ºä»–æå¤§åœ°æ‰©å±•äº†æŸ¥è¯¢çš„è¡¨è¾¾èƒ½åŠ›ã€‚æœ¬æ–‡é™¤äº†ä»‹ç»è¿™ä¸‰ç§udfä¹‹å¤–ï¼Œæœ€åä¼šä»‹ç»ä¸€ä¸ªredisä½œä¸ºäº¤äº’æ•°æ®æºçš„udfæ¡ˆä¾‹ã€‚</p>

<p>æ³¨å†Œç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°</p>

<p>Â  Â åœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹ï¼Œç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°åœ¨ä½¿ç”¨ä¹‹å‰æ˜¯å¿…é¡»è¦æ³¨å†Œçš„ã€‚å¯¹äºScalaçš„Table APIï¼Œudfæ˜¯ä¸éœ€è¦æ³¨å†Œçš„ã€‚</p>

<p>Â  Â è°ƒç”¨TableEnvironmentçš„registerFunction()æ–¹æ³•æ¥å®ç°æ³¨å†Œã€‚Udfæ³¨å†ŒæˆåŠŸä¹‹åï¼Œä¼šè¢«æ’å…¥TableEnvironmentçš„function catalogï¼Œè¿™æ ·table APIå’Œsqlå°±èƒ½è§£æä»–äº†ã€‚</p>

<p>1.Scalar Functions æ ‡é‡å‡½æ•°
Â  Â æ ‡é‡å‡½æ•°ï¼Œæ˜¯æŒ‡è¿”å›ä¸€ä¸ªå€¼çš„å‡½æ•°ã€‚æ ‡é‡å‡½æ•°æ˜¯å®ç°å°†0ï¼Œ1ï¼Œæˆ–è€…å¤šä¸ªæ ‡é‡å€¼è½¬åŒ–ä¸ºä¸€ä¸ªæ–°å€¼ã€‚</p>

<p>Â  Â å®ç°ä¸€ä¸ªæ ‡é‡å‡½æ•°éœ€è¦ç»§æ‰¿ScalarFunctionï¼Œå¹¶ä¸”å®ç°ä¸€ä¸ªæˆ–è€…å¤šä¸ªevaluationæ–¹æ³•ã€‚æ ‡é‡å‡½æ•°çš„è¡Œä¸ºå°±æ˜¯é€šè¿‡evaluationæ–¹æ³•æ¥å®ç°çš„ã€‚evaluationæ–¹æ³•å¿…é¡»å®šä¹‰ä¸ºpublicï¼Œå‘½åä¸ºevalã€‚evaluationæ–¹æ³•çš„è¾“å…¥å‚æ•°ç±»å‹å’Œè¿”å›å€¼ç±»å‹å†³å®šç€æ ‡é‡å‡½æ•°çš„è¾“å…¥å‚æ•°ç±»å‹å’Œè¿”å›å€¼ç±»å‹ã€‚evaluationæ–¹æ³•ä¹Ÿå¯ä»¥è¢«é‡è½½å®ç°å¤šä¸ªevalã€‚åŒæ—¶evaluationæ–¹æ³•æ”¯æŒå˜å‚æ•°ï¼Œä¾‹å¦‚ï¼ševal(Stringâ€¦ strs)ã€‚</p>

<p>ä¸‹é¢ç»™å‡ºä¸€ä¸ªæ ‡é‡å‡½æ•°çš„ä¾‹å­ã€‚ä¾‹å­å®ç°çš„æ˜¯ä¸€ä¸ªhashcodeæ–¹æ³•ã€‚</p>

<p>public class HashCode extends ScalarFunction {
private int factor = 12;
public HashCode(int factor) {
    this.factor = factor;
}
public int eval(String s) {
   return s.hashCode() * factor;
}
}
BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);
// register the function
tableEnv.registerFunction(â€œhashCodeâ€, new HashCode(10));
// use the function in Java Table API
myTable.select(â€œstring, string.hashCode(), hashCode(string)â€);
// use the function in SQL API</p>

<p>tableEnv.sqlQuery(â€œSELECT string, HASHCODE(string) FROM MyTableâ€);
Â  Â é»˜è®¤æƒ…å†µä¸‹evaluationæ–¹æ³•çš„è¿”å›å€¼ç±»å‹æ˜¯ç”±flinkç±»å‹æŠ½å–å·¥å…·å†³å®šã€‚å¯¹äºåŸºç¡€ç±»å‹åŠç®€å•çš„POJOSæ˜¯è¶³å¤Ÿçš„ï¼Œä½†æ˜¯æ›´å¤æ‚çš„ç±»å‹ï¼Œè‡ªå®šä¹‰ç±»å‹ï¼Œç»„åˆç±»å‹ï¼Œä¼šæŠ¥é”™ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œè¿”å›å€¼ç±»å‹çš„TypeInformationï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šï¼Œæ–¹æ³•æ˜¯é‡è½½ScalarFunction#getResultType()ã€‚</p>

<p>Â  Â ä¸‹é¢ç»™ä¸€ä¸ªä¾‹å­ï¼Œé€šè¿‡å¤å†™ScalarFunction#getResultType()ï¼Œå°†longå‹çš„è¿”å›å€¼åœ¨ä»£ç ç”Ÿæˆçš„æ—¶å€™ç¿»è¯‘æˆTypes.TIMESTAMPã€‚</p>

<p>public static class TimestampModifier extends ScalarFunction {
public long eval(long t) {
  return t % 1000;
}
public TypeInformation&lt;?&gt; getResultType(signature: Class&lt;?&gt;[]) {
  return Types.TIMESTAMP;
}
}
2.Table Functions è¡¨å‡½æ•°
Â  Â ä¸æ ‡é‡å‡½æ•°ç›¸ä¼¼ä¹‹å¤„æ˜¯è¾“å…¥å¯ä»¥0ï¼Œ1ï¼Œæˆ–è€…å¤šä¸ªå‚æ•°ï¼Œä½†æ˜¯ä¸åŒä¹‹å¤„å¯ä»¥è¾“å‡ºä»»æ„æ•°ç›®çš„è¡Œæ•°ã€‚è¿”å›çš„è¡Œä¹Ÿå¯ä»¥åŒ…å«ä¸€ä¸ªæˆ–è€…å¤šä¸ªåˆ—ã€‚</p>

<p>Â  Â ä¸ºäº†è‡ªå®šä¹‰è¡¨å‡½æ•°ï¼Œéœ€è¦ç»§æ‰¿TableFunctionï¼Œå®ç°ä¸€ä¸ªæˆ–è€…å¤šä¸ªevaluationæ–¹æ³•ã€‚è¡¨å‡½æ•°çš„è¡Œä¸ºå®šä¹‰åœ¨è¿™äº›evaluationæ–¹æ³•å†…éƒ¨ï¼Œå‡½æ•°åä¸ºevalå¹¶ä¸”å¿…é¡»æ˜¯publicã€‚TableFunctionå¯ä»¥é‡è½½å¤šä¸ªevalæ–¹æ³•ã€‚Evaluationæ–¹æ³•çš„è¾“å…¥å‚æ•°ç±»å‹ï¼Œå†³å®šç€è¡¨å‡½æ•°çš„è¾“å…¥ç±»å‹ã€‚Evaluationæ–¹æ³•ä¹Ÿæ”¯æŒå˜å‚ï¼Œä¾‹å¦‚ï¼ševal(Stringâ€¦ strs)ã€‚è¿”å›è¡¨çš„ç±»å‹å–å†³äºTableFunctionçš„åŸºæœ¬ç±»å‹ã€‚Evaluationæ–¹æ³•ä½¿ç”¨collect(T)å‘å°„è¾“å‡ºrowsã€‚</p>

<p>Â  Â åœ¨Table APIä¸­ï¼Œè¡¨å‡½æ•°åœ¨scalaè¯­è¨€ä¸­ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š.join(Expression) æˆ–è€… .leftOuterJoin(Expression)ï¼Œåœ¨javaè¯­è¨€ä¸­ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š.join(String) æˆ–è€….leftOuterJoin(String)ã€‚</p>

<p>Joinæ“ä½œç®—å­ä¼šä½¿ç”¨è¡¨å‡½æ•°(æ“ä½œç®—å­å³è¾¹çš„è¡¨)äº§ç”Ÿçš„æ‰€æœ‰è¡Œè¿›è¡Œ(cross) join å¤–éƒ¨è¡¨(æ“ä½œç®—å­å·¦è¾¹çš„è¡¨)çš„æ¯ä¸€è¡Œã€‚
leftOuterJoinæ“ä½œç®—å­ä¼šä½¿ç”¨è¡¨å‡½æ•°(æ“ä½œç®—å­å³è¾¹çš„è¡¨)äº§ç”Ÿçš„æ‰€æœ‰è¡Œè¿›è¡Œ(cross) join å¤–éƒ¨è¡¨(æ“ä½œç®—å­å·¦è¾¹çš„è¡¨)çš„æ¯ä¸€è¡Œï¼Œå¹¶ä¸”åœ¨è¡¨å‡½æ•°è¿”å›ä¸€ä¸ªç©ºè¡¨çš„æƒ…å†µä¸‹ä¼šä¿ç•™æ‰€æœ‰çš„outer rowsã€‚
åœ¨sqlè¯­æ³•ä¸­ç¨å¾®æœ‰ç‚¹åŒºåˆ«ï¼š</p>

<p>cross joinç”¨æ³•æ˜¯LATERAL TABLE(<TableFunction>)ã€‚
LEFT JOINç”¨æ³•æ˜¯åœ¨joinæ¡ä»¶ä¸­åŠ å…¥ON TRUEã€‚
ä¸‹é¢çš„ä¾‹å­è®²çš„æ˜¯å¦‚ä½•ä½¿ç”¨è¡¨å€¼å‡½æ•°ã€‚</TableFunction></p>

<p>// The generic type â€œTuple2&lt;String, Integer&gt;â€ determines the schema of the returned table as (String, Integer).</p>

<p>public class Split extends TableFunction&lt;Tuple2&lt;String, IntegerÂ» {</p>

<p>private String separator = â€œ â€œ;
  public Split(String separator) {
      this.separator = separator;
  }
  public void eval(String str) {
      for (String s : str.split(separator)) {
          // use collect(â€¦) to emit a row
          collect(new Tuple2&lt;String, Integer&gt;(s, s.length()));
      }
  }
}
BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);
Table myTable = â€¦         // table schema: [a: String]
// Register the function.
tableEnv.registerFunction(â€œsplitâ€, new Split(â€œ#â€));
// Use the table function in the Java Table API. â€œasâ€ specifies the field names of the table.
myTable.join(â€œsplit(a) as (word, length)â€).select(â€œa, word, lengthâ€);</p>

<p>myTable.leftOuterJoin(â€œsplit(a) as (word, length)â€).select(â€œa, word, lengthâ€);</p>

<p>// Use the table function in SQL with LATERAL and TABLE keywords.
// CROSS JOIN a table function (equivalent to â€œjoinâ€ in Table API).
tableEnv.sqlQuery(â€œSELECT a, word, length FROM MyTable, LATERAL TABLE(split(a)) as T(word, length)â€);
// LEFT JOIN a table function (equivalent to â€œleftOuterJoinâ€ in Table API).
tableEnv.sqlQuery(â€œSELECT a, word, length FROM MyTable LEFT JOIN LATERAL TABLE(split(a)) as T(word, length) ON TRUEâ€);
Â  Â éœ€è¦æ³¨æ„çš„æ˜¯PROJOç±»å‹ä¸éœ€è¦ä¸€ä¸ªç¡®å®šçš„å­—æ®µé¡ºåºã€‚æ„å‘³ç€ä½ ä¸èƒ½ä½¿ç”¨asä¿®æ”¹è¡¨å‡½æ•°è¿”å›çš„pojoçš„å­—æ®µçš„åå­—ã€‚</p>

<p>Â  Â é»˜è®¤æƒ…å†µä¸‹TableFunctionè¿”å›å€¼ç±»å‹æ˜¯ç”±flinkç±»å‹æŠ½å–å·¥å…·å†³å®šã€‚å¯¹äºåŸºç¡€ç±»å‹åŠç®€å•çš„POJOSæ˜¯è¶³å¤Ÿçš„ï¼Œä½†æ˜¯æ›´å¤æ‚çš„ç±»å‹ï¼Œè‡ªå®šä¹‰ç±»å‹ï¼Œç»„åˆç±»å‹ï¼Œä¼šæŠ¥é”™ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œè¿”å›å€¼ç±»å‹çš„TypeInformationï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šï¼Œæ–¹æ³•æ˜¯é‡è½½TableFunction#getResultType()ã€‚</p>

<p>ä¸‹é¢çš„ä¾‹å­ï¼Œæˆ‘ä»¬é€šè¿‡å¤å†™TableFunction#getResultType()æ–¹æ³•ä½¿å¾—è¡¨è¿”å›ç±»å‹æ˜¯RowTypeInfo(String, Integer)ã€‚</p>

<p>public class CustomTypeSplit extends TableFunction<Row> {
  public void eval(String str) {
      for (String s : str.split(" ")) {
          Row row = new Row(2);
          row.setField(0, s);
          row.setField(1, s.length);
          collect(row);
      }
  }
  @Override
  public TypeInformation<Row> getResultType() {
      return Types.ROW(Types.STRING(), Types.INT());
  }
}
3.Aggregation Functions èšåˆå‡½æ•°
Â  Â ç”¨æˆ·è‡ªå®šä¹‰èšåˆå‡½æ•°èšåˆä¸€å¼ è¡¨(ä¸€è¡Œæˆ–è€…å¤šè¡Œï¼Œä¸€è¡Œæœ‰ä¸€ä¸ªæˆ–è€…å¤šä¸ªå±æ€§)ä¸ºä¸€ä¸ªæ ‡é‡çš„å€¼ã€‚
[å›¾ç‰‡ä¸Šä¼ å¤±è´¥...(image-f5e972-1542542047386)]
ä¸Šå›¾ä¸­æ˜¯è®²çš„ä¸€å¼ é¥®æ–™çš„è¡¨è¿™ä¸ªè¡¨æœ‰æ˜¯é‚£ä¸ªå­—æ®µäº”è¡Œæ•°æ®ï¼Œç°åœ¨è¦åšçš„æ˜¯æ±‚å‡ºæ‰€æœ‰é¥®æ–™çš„æœ€é«˜ä»·ã€‚</Row></Row></p>

<p>Â  Â èšåˆå‡½æ•°éœ€è¦ç»§æ‰¿AggregateFunctionã€‚èšåˆå‡½æ•°å·¥ä½œæ–¹å¼å¦‚ä¸‹ï¼š</p>

<p>é¦–å…ˆï¼Œéœ€è¦ä¸€ä¸ªaccumulatorï¼Œè¿™ä¸ªæ˜¯ä¿å­˜èšåˆä¸­é—´ç»“æœçš„æ•°æ®ç»“æ„ã€‚è°ƒç”¨AggregateFunctionå‡½æ•°çš„createAccumulator()æ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ªç©ºaccumulator.</p>

<p>éšåï¼Œæ¯ä¸ªè¾“å…¥è¡Œéƒ½ä¼šè°ƒç”¨accumulate()æ–¹æ³•æ¥æ›´æ–°accumulatorã€‚ä¸€æ—¦æ‰€æœ‰çš„è¡Œè¢«å¤„ç†äº†ï¼ŒgetValue()æ–¹æ³•å°±ä¼šè¢«è°ƒç”¨ï¼Œè®¡ç®—å’Œè¿”å›æœ€ç»ˆçš„ç»“æœã€‚</p>

<p>å¯¹äºæ¯ä¸ªAggregateFunctionï¼Œä¸‹é¢ä¸‰ä¸ªæ–¹æ³•éƒ½æ˜¯æ¯”ä¸å¯å°‘çš„ï¼š</p>

<p>createAccumulator()</p>

<p>accumulate()</p>

<p>getValue()
Â  Â flinkçš„ç±»å‹æŠ½å–æœºåˆ¶ä¸èƒ½è¯†åˆ«å¤æ‚çš„æ•°æ®ç±»å‹ï¼Œæ¯”å¦‚ï¼Œæ•°æ®ç±»å‹ä¸æ˜¯åŸºç¡€ç±»å‹æˆ–è€…ç®€å•çš„pojosç±»å‹ã€‚æ‰€ä»¥ï¼Œç±»ä¼¼äºScalarFunction å’ŒTableFunctionï¼ŒAggregateFunctionæä¾›äº†æ–¹æ³•å»æŒ‡å®šè¿”å›ç»“æœç±»å‹çš„TypeInformationï¼Œç”¨çš„æ˜¯AggregateFunction#getResultType()ã€‚Accumulatorç±»å‹ç”¨çš„æ˜¯AggregateFunction#getAccumulatorType()ã€‚</p>

<p>Â  Â é™¤äº†ä¸Šé¢çš„æ–¹æ³•ï¼Œè¿˜æœ‰ä¸€äº›å¯é€‰çš„æ–¹æ³•ã€‚æœ‰äº›æ–¹æ³•æ˜¯è®©ç³»ç»Ÿæ›´åŠ é«˜æ•ˆçš„æ‰§è¡ŒæŸ¥è¯¢ï¼Œå¦å¤–çš„ä¸€äº›åœ¨ç‰¹å®šçš„åœºæ™¯ä¸‹æ˜¯å¿…é¡»çš„ã€‚ä¾‹å¦‚ï¼Œmerge()æ–¹æ³•åœ¨ä¼šè¯ç»„çª—å£ï¼ˆsession group windowï¼‰ä¸Šä¸‹æ–‡ä¸­æ˜¯å¿…é¡»çš„ã€‚å½“ä¸€è¡Œæ•°æ®æ˜¯è¢«è§†ä¸ºè·Ÿä¸¤ä¸ªå›è¯çª—å£ç›¸å…³çš„æ—¶å€™ï¼Œä¸¤ä¸ªä¼šè¯çª—å£çš„accumulatorséœ€è¦è¢«joinã€‚</p>

<p>AggregateFunctionçš„ä¸‹é¢å‡ ä¸ªæ–¹æ³•ï¼Œæ ¹æ®ä½¿ç”¨åœºæ™¯çš„ä¸åŒéœ€è¦è¢«å®ç°ï¼š</p>

<p>retract()ï¼šåœ¨bounded OVERçª—å£çš„èšåˆæ–¹æ³•ä¸­æ˜¯éœ€è¦å®ç°çš„ã€‚
merge()ï¼šåœ¨å¾ˆå¤šbatch èšåˆå’Œä¼šè¯çª—å£èšåˆæ˜¯å¿…é¡»çš„ã€‚
resetAccumulator(): åœ¨å¤§å¤šæ•°batchèšåˆæ˜¯å¿…é¡»çš„ã€‚
AggregateFunctionçš„æ‰€æœ‰æ–¹æ³•éƒ½æ˜¯éœ€è¦è¢«å£°æ˜ä¸ºpublicï¼Œè€Œä¸æ˜¯staticã€‚å®šä¹‰èšåˆå‡½æ•°éœ€è¦å®ç°org.apache.flink.table.functions.AggregateFunctionåŒæ—¶éœ€è¦å®ç°ä¸€ä¸ªæˆ–è€…å¤šä¸ªaccumulateæ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥è¢«é‡è½½ä¸ºä¸åŒçš„æ•°æ®ç±»å‹ï¼Œå¹¶ä¸”æ”¯æŒå˜å‚ã€‚</p>

<p>Â  Â ä¸ºäº†è®¡ç®—åŠ æƒå¹³å‡å€¼ï¼Œç´¯åŠ å™¨éœ€è¦å­˜å‚¨å·²ç´¯ç§¯çš„æ‰€æœ‰æ•°æ®çš„åŠ æƒå’ŒåŠè®¡æ•°ã€‚åœ¨æ —å­ä¸­å®šä¹‰ä¸€ä¸ªWeightedAvgAccumç±»ä½œä¸ºaccumulatorã€‚å°½ç®¡ï¼Œretract(), merge(), å’ŒresetAccumulator()æ–¹æ³•åœ¨å¾ˆå¤šèšåˆç±»å‹æ˜¯ä¸éœ€è¦çš„ï¼Œè¿™é‡Œä¹Ÿç»™å‡ºäº†æ —å­ã€‚</p>

<p>/**</p>
<ul>
  <li>Accumulator for WeightedAvg.
*/
public static class WeightedAvgAccum {
public long sum = 0;
public int count = 0;
}
/**</li>
  <li>Weighted Average user-defined aggregate function.
*/
public static class WeightedAvg extends AggregateFunction&lt;Long, WeightedAvgAccum&gt; {
@Override
public WeightedAvgAccum createAccumulator() {
  return new WeightedAvgAccum();
}
@Override
public Long getValue(WeightedAvgAccum acc) {
    if (acc.count == 0) {
        return null;
    } else {
        return acc.sum / acc.count;
    }
}
public void accumulate(WeightedAvgAccum acc, long iValue, int iWeight) {
    acc.sum += iValue * iWeight;
    acc.count += iWeight;
}
public void retract(WeightedAvgAccum acc, long iValue, int iWeight) {
    acc.sum -= iValue * iWeight;
    acc.count -= iWeight;
}
public void merge(WeightedAvgAccum acc, Iterable<WeightedAvgAccum> it) {
    Iterator<WeightedAvgAccum> iter = it.iterator();
    while (iter.hasNext()) {
        WeightedAvgAccum a = iter.next();
        acc.count += a.count;
        acc.sum += a.sum;
    }
}
public void resetAccumulator(WeightedAvgAccum acc) {
    acc.count = 0;
    acc.sum = 0L;
}
}
// register function
StreamTableEnvironment tEnv = ...
tEnv.registerFunction("wAvg", new WeightedAvg());
// use function</WeightedAvgAccum></WeightedAvgAccum></li>
</ul>

<p>tEnv.sqlQuery(â€œSELECT user, wAvg(points, level) AS avgPoints FROM userScores GROUP BY userâ€);
4.udfçš„æœ€ä½³å®è·µç»éªŒ
4.1 Table APIå’ŒSQL
Â  Â ä»£ç ç”Ÿæˆå™¨å†…éƒ¨ä¼šå°½å¯èƒ½å¤šçš„å°è¯•ä½¿ç”¨åŸç”Ÿå€¼ã€‚ç”¨æˆ·å®šä¹‰çš„å‡½æ•°å¯èƒ½é€šè¿‡å¯¹è±¡åˆ›å»ºã€å¼ºåˆ¶è½¬æ¢(casting)å’Œæ‹†è£…ç®±((un)boxing)å¼•å…¥å¤§é‡å¼€é”€ã€‚å› æ­¤ï¼Œå¼ºçƒˆæ¨èå‚æ•°å’Œè¿”å›å€¼çš„ç±»å‹å®šä¹‰ä¸ºåŸç”Ÿç±»å‹è€Œä¸æ˜¯ä»–ä»¬åŒ…è£…ç±»å‹(boxing class)ã€‚Types.DATE å’ŒTypes.TIMEå¯ä»¥ç”¨intä»£æ›¿ã€‚Types.TIMESTAMPå¯ä»¥ç”¨longä»£æ›¿ã€‚</p>

<p>Â  Â å»ºè®®ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ä½¿ç”¨javaç¼–å†™è€Œä¸æ˜¯scalaç¼–å†™ï¼Œå› ä¸ºscalaçš„ç±»å‹å¯èƒ½ä¼šæœ‰ä¸è¢«flinkç±»å‹æŠ½å–å™¨å…¼å®¹ã€‚</p>

<p>4.2 ç”¨Runtimeé›†æˆUDFs
Â  Â æœ‰æ—¶å€™udféœ€è¦è·å–å…¨å±€runtimeä¿¡æ¯æˆ–è€…åœ¨è¿›è¡Œå®é™…å·¥ä½œä¹‹å‰åšä¸€äº›è®¾ç½®å’Œæ¸…é™¤å·¥ä½œï¼Œæ¯”å¦‚ï¼Œæ‰“å¼€æ•°æ®åº“é“¾æ¥å’Œå…³é—­æ•°æ®åº“é“¾æ¥ã€‚Udfæä¾›äº†open()å’Œclose()æ–¹æ³•ï¼Œå¯ä»¥è¢«å¤å†™ï¼ŒåŠŸèƒ½ç±»ä¼¼Datasetå’ŒDataStream APIçš„RichFunctionæ–¹æ³•ã€‚</p>

<p>Â  Â Open()æ–¹æ³•æ˜¯åœ¨evaluationæ–¹æ³•è°ƒç”¨å‰è°ƒç”¨ä¸€æ¬¡ã€‚Close()æ˜¯åœ¨evaluationæ–¹æ³•æœ€åä¸€æ¬¡è°ƒç”¨åè°ƒç”¨ã€‚Open()æ–¹æ³•æå…±ä¸€ä¸ªFunctionContextï¼ŒFunctionContextåŒ…å«äº†udfæ‰§è¡Œç¯å¢ƒçš„ä¸Šä¸‹æ–‡ï¼Œæ¯”å¦‚ï¼Œmetric groupï¼Œåˆ†å¸ƒå¼ç¼“å­˜æ–‡ä»¶ï¼Œå…¨å±€çš„jobå‚æ•°ã€‚</p>

<p>Â  Â é€šè¿‡è°ƒç”¨FunctionContextçš„ç›¸å…³æ–¹æ³•ï¼Œå¯ä»¥è·å–åˆ°ç›¸å…³çš„ä¿¡æ¯ï¼š</p>

<p>getMetricGroup()å¹¶è¡Œå­ä»»åŠ¡çš„æŒ‡æ ‡ç»„;
getCachedFile(name)åˆ†å¸ƒå¼ç¼“å­˜æ–‡ä»¶çš„æœ¬åœ°å‰¯æœ¬;
getJobParameter(name, defaultValue)ç»™å®škeyå…¨å±€jobå‚æ•°;
Â  Â ç»™å‡ºçš„ä¾‹å­å°±æ˜¯é€šè¿‡FunctionContextåœ¨ä¸€ä¸ªæ ‡é‡å‡½æ•°ä¸­è·å–å…¨å±€jobçš„å‚æ•°ã€‚ä¸»è¦æ˜¯å®ç°è·å–redisçš„é…ç½®ï¼Œç„¶åç®€å†redisé“¾æ¥ï¼Œå®ç°redisçš„äº¤äº’çš„è¿‡ç¨‹ã€‚</p>

<p>import org.apache.flink.table.functions.FunctionContext;
import org.apache.flink.table.functions.ScalarFunction;
import redis.clients.jedis.Jedis;
public class HashCode extends ScalarFunction {
  private int factor = 12;
  Jedis jedis = null;
  public HashCode() {
      super();
  }
  @Override
  public void open(FunctionContext context) throws Exception {
      super.open(context);
      String redisHost = context.getJobParameter(â€œredis.hostâ€,â€localhostâ€);
      int redisPort = Integer.valueOf(context.getJobParameter(â€œredis.portâ€,â€6379â€));
      jedis = new Jedis(redisHost,redisPort);
  }</p>

<p>@Override
  public void close() throws Exception {
      super.close();
      jedis.close();
  }</p>

<p>public HashCode(int factor) {
      this.factor = factor;
  }</p>

<p>public int eval(int s) {
      s = s % 3;
      if(s == 2)
          return Integer.valueOf(jedis.get(String.valueOf(s)));
      else
          return 0;
  }
}</p>

<p>ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);
// set job parameter
Map&lt;String,String&gt; hashmap = new HashMap&lt;&gt;();
       hashmap.put(â€œredis.hostâ€,â€localhostâ€);
       hashmap.put(â€œredis.portâ€,â€6379â€);
       ParameterTool parameter = ParameterTool.fromMap(hashmap);
       exeEnv.getConfig().setGlobalJobParameters(parameter);
// register the function
tableEnv.registerFunction(â€œhashCodeâ€, new HashCode());
// use the function in Java Table API
myTable.select(â€œstring, string.hashCode(), hashCode(string)â€);
// use the function in SQL
tableEnv.sqlQuery(â€œSELECT string, HASHCODE(string) FROM MyTableâ€);</p>

<p>https://www.jianshu.com/p/5dc2cab91c78</p>

<p>å®šä¹‰ UDF ç±»ï¼Œç»§æ‰¿ ScalarFunction
å®ç° eval() æ–¹æ³•ï¼Œå‚æ•°å’Œè¿”å›å€¼æ ¹æ® SQL ä¸­çš„å®é™…éœ€è¦å†³å®šï¼ŒVARCHAR ç±»å‹å¯¹åº” JAVA ä¸­çš„ Stringï¼ŒBIGINT å¯¹åº” long ç­‰ã€‚</p>

:ET