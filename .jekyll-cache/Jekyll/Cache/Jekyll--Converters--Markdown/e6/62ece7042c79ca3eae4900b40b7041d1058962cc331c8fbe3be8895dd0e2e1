I"<p>ES-分词器(Analyzer)
把输入的文本块按照一定的策略进行分解，并建立倒排索引。在Lucene的架构中，这个过程由分析器(analyzer)完成。</p>

<p>主要组成
character filter:接收原字符流，通过添加、删除或者替换操作改变原字符流。例如：去除文本中的html标签，或者将罗马数字转换成阿拉伯数字等。一个字符过滤器可以有零个或者多个。</p>

<p>tokenizer：简单的说就是将一整段文本拆分成一个个的词。例如拆分英文，通过空格能将句子拆分成一个个的词，但是对于中文来说，无法使用这种方式来实现。在一个分词器中,有且只有一个tokenizeer</p>

<p>token filters：将切分的单词添加、删除或者改变。例如将所有英文单词小写，或者将英文中的停词a删除等。在token filters中，不允许将token(分出的词)的position或者offset改变。同时，在一个分词器中，可以有零个或者多个token filters.</p>

<p>文本分词会发生在两个地方：</p>

<p>创建索引:当索引文档字符类型为text时，在建立索引时将会对该字段进行分词。</p>

<p>搜索：当对一个text类型的字段进行全文检索时，会对用户输入的文本进行分词。</p>

<!-- more -->

<p>GET _analyze
{
  “analyzer”: “ik_max_word”,
  “text”: “121221”
}
{
  “tokens” : [
    {
      “token” : “121221”,
      “start_offset” : 0,
      “end_offset” : 6,
      “type” : “ARABIC”,
      “position” : 0
    }
  ]
}```</p>

<p>可以通过_analyzerAPI来测试分词的效果。
POST _analyze
{
  “analyzer”: “standard”,
  “text”: “The quick brown fox”
}</p>

<p>搜索时如何确定分词器
在搜索时，通过下面参数依次检查搜索时使用的分词器：</p>

<p>搜索时指定analyzer参数
创建mapping时指定字段的search_analyzer属性
创建索引时指定setting的analysis.analyzer.default_search
查看创建索引时字段指定的analyzer属性
如果上面几种都未设置，则使用默认的standard分词器。</p>

<p>https://www.jianshu.com/p/bebea42b5040</p>

:ET