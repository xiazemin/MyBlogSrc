---
title: memory 内存管理机制
layout: post
category: linux
author: 夏泽民
---
所有进程（执行的程序）都必须占用一定数量的内存，它或是用来存放从磁盘载入的程序代码，或是存放取自用户输入的数据等等。不过进程对这些内存的管理方式因内存用途不一而不尽相同，有些内存是事先静态分配和统一回收的，而有些却是按需要动态分配和回收的。

对任何一个普通进程来讲，它都会涉及到5种不同的数据段。稍有编程知识的朋友都能想到这几个数据段中包含有“程序代码段”、“程序数据段”、“程序堆栈段”等。不错，这几种数据段都在其中，但除了以上几种数据段之外，进程还另外包含两种数据段。下面我们来简单归纳一下进程对应的内存空间中所包含的5种不同的数据区。

代码段：代码段是用来存放可执行文件的操作指令，也就是说是它是可执行程序在内存中的镜像。代码段需要防止在运行时被非法修改，所以只准许读取操作，而不允许写入（修改）操作——它是不可写的。

数据段：数据段用来存放可执行文件中已初始化全局变量，换句话说就是存放程序静态分配[1]的变量和全局变量。

BSS段[2]：BSS段包含了程序中未初始化的全局变量，在内存中 bss段全部置零。

堆（heap）：堆是用于存放进程运行中被动态分配的内存段，它的大小并不固定，可动态扩张或缩减。当进程调用malloc等函数分配内存时，新分配的内存就被动态添加到堆上（堆被扩张）；当利用free等函数释放内存时，被释放的内存从堆中被剔除（堆被缩减）

栈：栈是用户存放程序临时创建的局部变量，也就是说我们函数括弧“{}”中定义的变量（但不包括static声明的变量，static意味着在数据段中存放变量）。除此以外，在函数被调用时，其参数也会被压入发起调用的进程栈中，并且待到调用结束后，函数的返回值也会被存放回栈中。由于栈的先进先出特点，所以栈特别方便用来保存/恢复调用现场。从这个意义上讲，我们可以把堆栈看成一个寄存、交换临时数据的内存区。
<!-- more -->
从用户向内核看，所使用的内存表象形式会依次经历“逻辑地址”——“线性地址”——“物理地址”几种形式（关于几种地址的解释在前面已经讲述了）。逻辑地址经段机制转化成线性地址；线性地址又经过页机制转化为物理地址。（但是我们要知道Linux系统虽然保留了段机制，但是将所有程序的段地址都定死为0-4G，所以虽然逻辑地址和线性地址是两种不同的地址空间，但在Linux中逻辑地址就等于线性地址，它们的值是一样的）。沿着这条线索，我们所研究的主要问题也就集中在下面几个问题。

1.     进程空间地址如何管理？

2.     进程地址如何映射到物理内存？

3.     物理内存如何被管理？

以及由上述问题引发的一些子问题。如系统虚拟地址分布；内存分配接口；连续内存分配与非连续内存分配等。

 

进程内存空间
Linux操作系统采用虚拟内存管理技术，使得每个进程都有各自互不干涉的进程地址空间。该空间是块大小为4G的线性虚拟空间，用户所看到和接触到的都是该虚拟地址，无法看到实际的物理内存地址。利用这种虚拟地址不但能起到保护操作系统的效果（用户不能直接访问物理内存），而且更重要的是，用户程序可使用比实际物理内存更大的地址空间（具体的原因请看硬件基础部分）。

在讨论进程空间细节前，这里先要澄清下面几个问题：

l         第一、4G的进程地址空间被人为的分为两个部分——用户空间与内核空间。用户空间从0到3G（0xC0000000），内核空间占据3G到4G。用户进程通常情况下只能访问用户空间的虚拟地址，不能访问内核空间虚拟地址。只有用户进程进行系统调用（代表用户进程在内核态执行）等时刻可以访问到内核空间。

l         第二、用户空间对应进程，所以每当进程切换，用户空间就会跟着变化；而内核空间是由内核负责映射，它并不会跟着进程改变，是固定的。内核空间地址有自己对应的页表（init_mm.pgd），用户进程各自有不同的页表。

l         第三、每个进程的用户空间都是完全独立、互不相干的。不信的话，你可以把上面的程序同时运行10次（当然为了同时运行，让它们在返回前一同睡眠100秒吧），你会看到10个进程占用的线性地址一模一样

进程内存管理
进程内存管理的对象是进程线性地址空间上的内存镜像，这些内存镜像其实就是进程使用的虚拟内存区域（memory region）。进程虚拟空间是个32或64位的“平坦”（独立的连续区间）地址空间（空间的具体大小取决于体系结构）。要统一管理这么大的平坦空间可绝非易事，为了方便管理，虚拟空间被划分为许多大小可变的(但必须是4096的倍数)内存区域，这些区域在进程线性地址中像停车位一样有序排列。这些区域的划分原则是“将访问属性一致的地址空间存放在一起”，所谓访问属性在这里无非指的是“可读、可写、可执行等”。

如果你要查看某个进程占用的内存区域，可以使用命令cat /proc/<pid>/maps获得（pid是进程号，你可以运行上面我们给出的例子——./example &;pid便会打印到屏幕），你可以发现很多类似于下面的数字信息。

由于程序example使用了动态库，所以除了example本身使用的的内存区域外，还会包含那些动态库使用的内存区域（区域顺序是：代码段、数据段、bss段）。

我们下面只抽出和example有关的信息，除了前两行代表的代码段和数据段外，最后一行是进程使用的栈空间。

-------------------------------------------------------------------------------

08048000 - 08049000 r-xp 00000000 03:03 439029                               /home/mm/src/example

08049000 - 0804a000 rw-p 00000000 03:03 439029                               /home/mm/src/example

……………

bfffe000 - c0000000 rwxp ffff000 00:00 0

----------------------------------------------------------------------------------------------------------------------

每行数据格式如下：

（内存区域）开始－结束 访问权限  偏移  主设备号：次设备号 i节点  文件。

注意，你一定会发现进程空间只包含三个内存区域，似乎没有上面所提到的堆、bss等，其实并非如此，程序内存段和进程地址空间中的内存区域是种模糊对应，也就是说，堆、bss、数据段（初始化过的）都在进程空间中由数据段内存区域表示。

在Linux内核中对应进程内存区域的数据结构是: vm_area_struct, 内核将每个内存区域作为一个单独的内存对象管理，相应的操作也都一致。采用面向对象方法使VMA结构体可以代表多种类型的内存区域－－比如内存映射文件或进程的用户空间栈等，对这些区域的操作也都不尽相同。

vm_area_strcut结构比较复杂，关于它的详细结构请参阅相关资料。我们这里只对它的组织方法做一点补充说明。vm_area_struct是描述进程地址空间的基本管理单元，对于一个进程来说往往需要多个内存区域来描述它的虚拟空间，如何关联这些不同的内存区域呢？大家可能都会想到使用链表，的确vm_area_struct结构确实是以链表形式链接，不过为了方便查找，内核又以红黑树（以前的内核使用平衡树）的形式组织内存区域，以便降低搜索耗时。并存的两种组织形式，并非冗余：链表用于需要遍历全部节点的时候用，而红黑树适用于在地址空间中定位特定内存区域的时候。内核为了内存区域上的各种不同操作都能获得高性能，所以同时使用了这两种数据结构。

进程地址空间的管理模型
<img src="{{site.url}}{{site.baseurl}}/img/memory_model.gif"/>

进程的地址空间对应的描述结构是“内存描述符结构”,它表示进程的全部地址空间，——包含了和进程地址空间有关的全部信息，其中当然包含进程的内存区域。

进程内存的分配与回收
创建进程fork()、程序载入execve()、映射文件mmap()、动态内存分配malloc()/brk()等进程相关操作都需要分配内存给进程。不过这时进程申请和获得的还不是实际内存，而是虚拟内存，准确的说是“内存区域”。进程对内存区域的分配最终都会归结到do_mmap（）函数上来（brk调用被单独以系统调用实现，不用do_mmap()），

内核使用do_mmap()函数创建一个新的线性地址区间。但是说该函数创建了一个新VMA并不非常准确，因为如果创建的地址区间和一个已经存在的地址区间相邻，并且它们具有相同的访问权限的话，那么两个区间将合并为一个。如果不能合并，那么就确实需要创建一个新的VMA了。但无论哪种情况， do_mmap()函数都会将一个地址区间加入到进程的地址空间中－－无论是扩展已存在的内存区域还是创建一个新的区域。

同样，释放一个内存区域应使用函数do_ummap()，它会销毁对应的内存区域。

如何由虚变实！
    从上面已经看到进程所能直接操作的地址都为虚拟地址。当进程需要内存时，从内核获得的仅仅是虚拟的内存区域，而不是实际的物理地址，进程并没有获得物理内存（物理页面——页的概念请大家参考硬件基础一章），获得的仅仅是对一个新的线性地址区间的使用权。实际的物理内存只有当进程真的去访问新获取的虚拟地址时，才会由“请求页机制”产生“缺页”异常，从而进入分配实际页面的例程。

该异常是虚拟内存机制赖以存在的基本保证——它会告诉内核去真正为进程分配物理页，并建立对应的页表，这之后虚拟地址才实实在在地映射到了系统的物理内存上。（当然，如果页被换出到磁盘，也会产生缺页异常，不过这时不用再建立页表了）

这种请求页机制把页面的分配推迟到不能再推迟为止，并不急于把所有的事情都一次做完（这种思想有点像设计模式中的代理模式（proxy））。之所以能这么做是利用了内存访问的“局部性原理”，请求页带来的好处是节约了空闲内存，提高了系统的吞吐率。要想更清楚地了解请求页机制，可以看看《深入理解linux内核》一书。

这里我们需要说明在内存区域结构上的nopage操作。当访问的进程虚拟内存并未真正分配页面时，该操作便被调用来分配实际的物理页，并为该页建立页表项。在最后的例子中我们会演示如何使用该方法。

 

系统物理内存管理 
虽然应用程序操作的对象是映射到物理内存之上的虚拟内存，但是处理器直接操作的却是物理内存。所以当应用程序访问一个虚拟地址时，首先必须将虚拟地址转化成物理地址，然后处理器才能解析地址访问请求。地址的转换工作需要通过查询页表才能完成，概括地讲，地址转换需要将虚拟地址分段，使每段虚地址都作为一个索引指向页表，而页表项则指向下一级别的页表或者指向最终的物理页面。

每个进程都有自己的页表。进程描述符的pgd域指向的就是进程的页全局目录。下面我们借用《linux设备驱动程序》中的一幅图大致看看进程地址空间到物理页之间的转换关系。
	<img src="{{site.url}}{{site.baseurl}}/img/memory_model_pgd.jpg"/>
	
	上面的过程说起来简单，做起来难呀。因为在虚拟地址映射到页之前必须先分配物理页——也就是说必须先从内核中获取空闲页，并建立页表。下面我们介绍一下内核管理物理内存的机制。

 

物理内存管理（页管理）
Linux内核管理物理内存是通过分页机制实现的，它将整个内存划分成无数个4k（在i386体系结构中）大小的页，从而分配和回收内存的基本单位便是内存页了。利用分页管理有助于灵活分配内存地址，因为分配时不必要求必须有大块的连续内存[3]，系统可以东一页、西一页的凑出所需要的内存供进程使用。虽然如此，但是实际上系统使用内存时还是倾向于分配连续的内存块，因为分配连续内存时，页表不需要更改，因此能降低TLB的刷新率（频繁刷新会在很大程度上降低访问速度）。

鉴于上述需求，内核分配物理页面时为了尽量减少不连续情况，采用了“伙伴”关系来管理空闲页面。伙伴关系分配算法大家应该不陌生——几乎所有操作系统方面的书都会提到,我们不去详细说它了，如果不明白可以参看有关资料。这里只需要大家明白Linux中空闲页面的组织和管理利用了伙伴关系，因此空闲页面分配时也需要遵循伙伴关系，最小单位只能是2的幂倍页面大小。内核中分配空闲页面的基本函数是get_free_page/get_free_pages，它们或是分配单页或是分配指定的页面（2、4、8…512页）。

 注意：get_free_page是在内核中分配内存，不同于malloc在用户空间中分配，malloc利用堆动态分配，实际上是调用brk()系统调用，该调用的作用是扩大或缩小进程堆空间（它会修改进程的brk域）。如果现有的内存区域不够容纳堆空间，则会以页面大小的倍数为单位，扩张或收缩对应的内存区域，但brk值并非以页面大小为倍数修改，而是按实际请求修改。因此Malloc在用户空间分配内存可以以字节为单位分配,但内核在内部仍然会是以页为单位分配的。

   另外,需要提及的是，物理页在系统中由页结构struct page描述，系统中所有的页面都存储在数组mem_map[]中，可以通过该数组找到系统中的每一页（空闲或非空闲）。而其中的空闲页面则可由上述提到的以伙伴关系组织的空闲页链表（free_area[MAX_ORDER]）来索引。
<img src="{{site.url}}{{site.baseurl}}/img/memory_model_brk.gif"/>

内核内存使用
Slab

    所谓尺有所长，寸有所短。以页为最小单位分配内存对于内核管理系统中的物理内存来说的确比较方便，但内核自身最常使用的内存却往往是很小（远远小于一页）的内存块——比如存放文件描述符、进程描述符、虚拟内存区域描述符等行为所需的内存都不足一页。这些用来存放描述符的内存相比页面而言，就好比是面包屑与面包。一个整页中可以聚集多个这些小块内存；而且这些小块内存块也和面包屑一样频繁地生成/销毁。

  为了满足内核对这种小内存块的需要，Linux系统采用了一种被称为slab分配器的技术。Slab分配器的实现相当复杂，但原理不难，其核心思想就是“存储池[4]”的运用。内存片段（小块内存）被看作对象，当被使用完后，并不直接释放而是被缓存到“存储池”里，留做下次使用，这无疑避免了频繁创建与销毁对象所带来的额外负载。

Slab技术不但避免了内存内部分片（下文将解释）带来的不便（引入Slab分配器的主要目的是为了减少对伙伴系统分配算法的调用次数——频繁分配和回收必然会导致内存碎片——难以找到大块连续的可用内存），而且可以很好地利用硬件缓存提高访问速度。

    Slab并非是脱离伙伴关系而独立存在的一种内存分配方式，slab仍然是建立在页面基础之上，换句话说，Slab将页面（来自于伙伴关系管理的空闲页面链表）撕碎成众多小内存块以供分配，slab中的对象分配和销毁使用kmem_cache_alloc与kmem_cache_free。

 

Kmalloc

Slab分配器不仅仅只用来存放内核专用的结构体，它还被用来处理内核对小块内存的请求。当然鉴于Slab分配器的特点，一般来说内核程序中对小于一页的小块内存的请求才通过Slab分配器提供的接口Kmalloc来完成（虽然它可分配32 到131072字节的内存）。从内核内存分配的角度来讲，kmalloc可被看成是get_free_page（s）的一个有效补充，内存分配粒度更灵活了。

有兴趣的话，可以到/proc/slabinfo中找到内核执行现场使用的各种slab信息统计，其中你会看到系统中所有slab的使用信息。从信息中可以看到系统中除了专用结构体使用的slab外，还存在大量为Kmalloc而准备的Slab（其中有些为dma准备的）。

 

内核非连续内存分配（Vmalloc）

 

伙伴关系也好、slab技术也好，从内存管理理论角度而言目的基本是一致的，它们都是为了防止“分片”，不过分片又分为外部分片和内部分片之说，所谓内部分片是说系统为了满足一小段内存区（连续）的需要，不得不分配了一大区域连续内存给它，从而造成了空间浪费；外部分片是指系统虽有足够的内存，但却是分散的碎片，无法满足对大块“连续内存”的需求。无论何种分片都是系统有效利用内存的障碍。slab分配器使得一个页面内包含的众多小块内存可独立被分配使用，避免了内部分片，节约了空闲内存。伙伴关系把内存块按大小分组管理，一定程度上减轻了外部分片的危害，因为页框分配不在盲目，而是按照大小依次有序进行，不过伙伴关系只是减轻了外部分片，但并未彻底消除。你自己比划一下多次分配页面后，空闲内存的剩余情况吧。

所以避免外部分片的最终思路还是落到了如何利用不连续的内存块组合成“看起来很大的内存块”——这里的情况很类似于用户空间分配虚拟内存，内存逻辑上连续，其实映射到并不一定连续的物理内存上。Linux内核借用了这个技术，允许内核程序在内核地址空间中分配虚拟地址，同样也利用页表（内核页表）将虚拟地址映射到分散的内存页上。以此完美地解决了内核内存使用中的外部分片问题。内核提供vmalloc函数分配内核虚拟内存，该函数不同于kmalloc，它可以分配较Kmalloc大得多的内存空间（可远大于128K，但必须是页大小的倍数），但相比Kmalloc来说,Vmalloc需要对内核虚拟地址进行重映射，必须更新内核页表，因此分配效率上要低一些（用空间换时间）

与用户进程相似,内核也有一个名为init_mm的mm_strcut结构来描述内核地址空间，其中页表项pdg=swapper_pg_dir包含了系统内核空间（3G-4G）的映射关系。因此vmalloc分配内核虚拟地址必须更新内核页表，而kmalloc或get_free_page由于分配的连续内存，所以不需要更新内核页表。
<img src="{{site.url}}{{site.baseurl}}/img/memory_model_vmalloc.gif"/>

vmalloc分配的内核虚拟内存与kmalloc/get_free_page分配的内核虚拟内存位于不同的区间，不会重叠。因为内核虚拟空间被分区管理，各司其职。进程空间地址分布从０到３G(其实是到PAGE_OFFSET,在0x86中它等于0xC0000000)，从3G到vmalloc_start这段地址是物理内存映射区域（该区域中包含了内核镜像、物理页面表mem_map等等）比如我使用的系统内存是64M(可以用free看到)，那么(3G——3G+64M)这片内存就应该映射到物理内存，而vmalloc_start位置应在3G+64M附近（说"附近"因为是在物理内存映射区与vmalloc_start期间还会存在一个8M大小的gap来防止跃界）,vmalloc_end的位置接近4G(说"接近"是因为最后位置系统会保留一片128k大小的区域用于专用页面映射，还有可能会有高端内存映射区，这些都是细节，这里我们不做纠缠)。

由get_free_page或Kmalloc函数所分配的连续内存都陷于物理映射区域，所以它们返回的内核虚拟地址和实际物理地址仅仅是相差一个偏移量（PAGE_OFFSET），你可以很方便的将其转化为物理内存地址，同时内核也提供了virt_to_phys（）函数将内核虚拟空间中的物理映射区地址转化为物理地址。要知道，物理内存映射区中的地址与内核页表是有序对应的，系统中的每个物理页面都可以找到它对应的内核虚拟地址（在物理内存映射区中的）。

而vmalloc分配的地址则限于vmalloc_start与vmalloc_end之间。每一块vmalloc分配的内核虚拟内存都对应一个vm_struct结构体（可别和vm_area_struct搞混，那可是进程虚拟内存区域的结构），不同的内核虚拟地址被4k大小的空闲区间隔，以防止越界——见下图）。与进程虚拟地址的特性一样，这些虚拟地址与物理内存没有简单的位移关系，必须通过内核页表才可转换为物理地址或物理页。它们有可能尚未被映射，在发生缺页时才真正分配物理页面。

实例
内存映射(mmap)是Linux操作系统的一个很大特色，它可以将系统内存映射到一个文件（设备）上，以便可以通过访问文件内容来达到访问内存的目的。这样做的最大好处是提高了内存访问速度，并且可以利用文件系统的接口编程（设备在Linux中作为特殊文件处理）访问内存，降低了开发难度。许多设备驱动程序便是利用内存映射功能将用户空间的一段地址关联到设备内存上，无论何时，只要内存在分配的地址范围内进行读写，实际上就是对设备内存的访问。同时对设备文件的访问也等同于对内存区域的访问，也就是说，通过文件操作接口可以访问内存。Linux中的X服务器就是一个利用内存映射达到直接高速访问视频卡内存的例子。

熟悉文件操作的朋友一定会知道file_operations结构中有mmap方法，在用户执行mmap系统调用时，便会调用该方法来通过文件访问内存——不过在调用文件系统mmap方法前，内核还需要处理分配内存区域（vma_struct）、建立页表等工作。对于具体映射细节不作介绍了，需要强调的是,建立页表可以采用remap_page_range方法一次建立起所有映射区的页表，或利用vma_struct的nopage方法在缺页时现场一页一页的建立页表。第一种方法相比第二种方法简单方便、速度快， 但是灵活性不高。一次调用所有页表便定型了，不适用于那些需要现场建立页表的场合——比如映射区需要扩展或下面我们例子中的情况。

 

我们这里的实例希望利用内存映射，将系统内核中的一部分虚拟内存映射到用户空间，以供应用程序读取——你可利用它进行内核空间到用户空间的大规模信息传输。因此我们将试图写一个虚拟字符设备驱动程序，通过它将系统内核空间映射到用户空间——将内核虚拟内存映射到用户虚拟地址。从上一节已经看到Linux内核空间中包含两种虚拟地址：一种是物理和逻辑都连续的物理内存映射虚拟地址；另一种是逻辑连续但非物理连续的vmalloc分配的内存虚拟地址。我们的例子程序将演示把vmalloc分配的内核虚拟地址映射到用户地址空间的全过程。

程序里主要应解决两个问题：

第一是如何将vmalloc分配的内核虚拟内存正确地转化成物理地址？

因为内存映射先要获得被映射的物理地址，然后才能将其映射到要求的用户虚拟地址上。我们已经看到内核物理内存映射区域中的地址可以被内核函数virt_to_phys转换成实际的物理内存地址，但对于vmalloc分配的内核虚拟地址无法直接转化成物理地址，所以我们必须对这部分虚拟内存格外“照顾”——先将其转化成内核物理内存映射区域中的地址，然后在用virt_to_phys变为物理地址。

转化工作需要进行如下步骤：

a)         找到vmalloc虚拟内存对应的页表，并寻找到对应的页表项。

b)        获取页表项对应的页面指针

c)        通过页面得到对应的内核物理内存映射区域地址。

第二是当访问vmalloc分配区时，如果发现虚拟内存尚未被映射到物理页，则需要处理“缺页异常”。因此需要我们实现内存区域中的nopaga操作，以能返回被映射的物理页面指针，在我们的实例中就是返回上面过程中的内核物理内存映射区域中的地址。由于vmalloc分配的虚拟地址与物理地址的对应关系并非分配时就可确定，必须在缺页现场建立页表，因此这里不能使用remap_page_range方法，只能用vma的nopage方法一页一页的建立。

 

程序组成

map_driver.c，它是以模块形式加载的虚拟字符驱动程序。该驱动负责将一定长的内核虚拟地址(vmalloc分配的)映射到设备文件上。其中主要的函数有——vaddress_to_kaddress（）负责对vmalloc分配的地址进行页表解析,以找到对应的内核物理映射地址（kmalloc分配的地址）；map_nopage()负责在进程访问一个当前并不存在的VMA页时，寻找该地址对应的物理页，并返回该页的指针。

test.c 它利用上述驱动模块对应的设备文件在用户空间读取读取内核内存。结果可以看到内核虚拟地址的内容（ok!），被显示在了屏幕上。

 

执行步骤

编译map_driver.c为map_driver.o模块,具体参数见Makefile

加载模块 ：insmod map_driver.o

生成对应的设备文件

1 在/proc/devices下找到map_driver对应的设备命和设备号：grep mapdrv /proc/devices

2 建立设备文件mknod  mapfile c 254 0  （在我的系统里设备号为254）

    利用maptest读取mapfile文件，将取自内核的信息打印到屏幕上。
   
   
   一 物理内存和虚拟内存

         我们知道，直接从物理内存读写数据要比从硬盘读写数据要快的多，因此，我们希望所有数据的读取和写入都在内存完成，而内存是有限的，这样就引出了物理内存与虚拟内存的概念。
物理内存就是系统硬件提供的内存大小，是真正的内存，相对于物理内存，在linux下还有一个虚拟内存的概念，虚拟内存就是为了满足物理内存的不足而提出的策略，它是利用磁盘空间虚拟出的一块逻辑内存，用作虚拟内存的磁盘空间被称为交换空间（Swap Space）。
         作为物理内存的扩展，linux会在物理内存不足时，使用交换分区的虚拟内存，更详细的说，就是内核会将暂时不用的内存块信息写到交换空间，这样以来，物理内存得到了释放，这块内存就可以用于其它目的，当需要用到原始的内容时，这些信息会被重新从交换空间读入物理内存。
         linux的内存管理采取的是分页存取机制，为了保证物理内存能得到充分的利用，内核会在适当的时候将物理内存中不经常使用的数据块自动交换到虚拟内存中，而将经常使用的信息保留到物理内存。
         要深入了解linux内存运行机制，需要知道下面提到的几个方面：
         首先，Linux系统会不时的进行页面交换操作，以保持尽可能多的空闲物理内存，即使并没有什么事情需要内存，Linux也会交换出暂时不用的内存页面。这可以避免等待交换所需的时间。
         其次，linux进行页面交换是有条件的，不是所有页面在不用时都交换到虚拟内存，linux内核根据”最近最经常使用“算法，仅仅将一些不经常使用的页面文件交换到虚拟内存，有时我们会看到这么一个现象：linux物理内存还有很多，但是交换空间也使用了很多。其实，这并不奇怪，例如，一个占用很大内存的进程运行时，需要耗费很多内存资源，此时就会有一些不常用页面文件被交换到虚拟内存中，但后来这个占用很多内存资源的进程结束并释放了很多内存时，刚才被交换出去的页面文件并不会自动的交换进物理内存，除非有这个必要，那么此刻系统物理内存就会空闲很多，同时交换空间也在被使用，就出现了刚才所说的现象了。关于这点，不用担心什么，只要知道是怎么一回事就可以了。
         最后，交换空间的页面在使用时会首先被交换到物理内存，如果此时没有足够的物理内存来容纳这些页面，它们又会被马上交换出去，如此以来，虚拟内存中可能没有足够空间来存储这些交换页面，最终会导致linux出现假死机、服务异常等问题，linux虽然可以在一段时间内自行恢复，但是恢复后的系统已经基本不可用了。
因此，合理规划和设计linux内存的使用，是非常重要的.

二 内存的监控
         作为一名linux系统管理员，监控内存的使用状态是非常重要的，通过监控有助于了解内存的使用状态，比如内存占用是否正常，内存是否紧缺等等，监控内存最常使用的命令有free、top等，下面是某个系统free的输出：
[haixigov@WEBServer ~]$ free
             total        used         free      shared   buffers   cached
Mem:         16402432    16360492      41940        0     465404   12714880
-/+ buffers/cache:        3180208   13222224
Swap:        8193108        264      8192844
我们解释下输出结果中每个选项的含义：
首先是第一行：
 total：物理内存的总大小。 
 used：已经使用的物理内存多小。 
 free：空闲的物理内存值。 
 shared：多个进程共享的内存值。
 buffers/cached：磁盘缓存的大小。 
第二行Mem：代表物理内存使用情况。
第三行(-/+ buffers/cached)：代表磁盘缓存使用状态。
第四行：Swap表示交换空间内存使用状态。
free命令输出的内存状态，可以通过两个角度来查看：一个是从内核的角度来看，一个是从应用层的角度来看的。 
 

1．从内核的角度来查看内存的状态
就是内核目前可以直接分配到，不需要额外的操作，即为上面free命令输出中第二行Mem项的值，可以看出，此系统物理内存有16G，空闲的内存只有41940K，也就是40M多一点，我们来做一个这样的计算：
16402432－16360492＝41940  
其实就是总的物理内存减去已经使用的物理内存得到的就是空闲的物理内存大小，注意这里的可用内存值41940并不包含处于buffers和cached状态的内存大小。
如果你认为这个系统空闲内存太小，那你就错了，实际上，内核完全控制着内存的使用情况，linux会在需要内存的时候，或在系统运行逐步推进时，将buffers和cached状态的内存变为free状态的内存，以供系统使用。 
 

2．从应用层的角度来看系统内存的使用状态
也就是linux上运行的应用程序可以使用的内存大小，即free命令第三行“(-/+ buffers/cached)”的输出，可以看到，此系统已经使用的内存才3180208K，而空闲的内存达到13222224K，继续做这样一个计算：
41940＋（465404＋12714880）＝13222224
通过这个等式可知，应用程序可用的物理内存值是Mem项的free值加上buffers和cached值之和，也就是说，这个free值是包括buffers和cached项大小的，
对于应用程序来说，buffers/cached占有的内存是可用的，因为buffers/cached是为了提高文件读取的性能，当应用程序需要用到内存的时候，buffers/cached会很快地被回收，以供应用程序使用。
 

3．buffers与cached的异同
　在 Linux 操作系统中，当应用程序需要读取文件中的数据时，操作系统先分配一些内存，将数据从磁盘读入到这些内存中，然后再将数据分发给应用程序；当需要往文件中写数据时，操作系统先分配内存接收用户数据，然后再将数据从内存写到磁盘上。然而，如果有大量数据需要从磁盘读取到内存或者由内存写入磁盘时，系统的读写性能就变得非常低下，因为无论是从磁盘读数据，还是写数据到磁盘，都是一个很消耗时间和资源的过程，在这种情况下，linux引入了buffers和cached机制。
buffers与cached都是内存操作，用来保存系统曾经打开过的文件以及文件属性信息，这样当操作系统需要读取某些文件时，会首先在buffers与cached内存区查找，如果找到，直接读出传送给应用程序，如果没有找到需要数据，才从磁盘读取，这就是操作系统的缓存机制，通过缓存，大大提高了操作系统的性能。但buffers与cached缓冲的内容却是不同的。
buffers是用来缓冲块设备做的，它只记录文件系统的元数据（metadata）以及 tracking in-flight pages，而cached是用来给文件做缓冲。更通俗一点说：buffers主要用来存放目录里面有什么内容，文件的属性以及权限等等。而cached直接用来记忆我们打开过的文件和程序。
为了验证我们的结论是否正确，可以通过vi打开一个非常大的文件，看看cached的变化，然后再次vi这个文件，感觉一下两次打开的速度有何异同，是不是第二次打开的速度明显快于第一次呢？
接着执行下面的命令：
 find /* -name  *.conf
         看看buffers的值是否变化，然后重复执行find命令，看看两次显示速度有何不同。
         Linux操作系统的内存运行原理，很大程度上是根据服务器的需求来设计的，例如系统的缓冲机制会把经常使用到的文件和数据缓存在cached中，linux总是在力求缓存更多的数据和信息，这样再次需要这些数据时可以直接从内存中取，而不需要有一个漫长的磁盘操作，这种设计思路提高了系统的整体性能。

三 交换空间swap的使用
        虽然现在的内存已经变得非常廉价，但是swap仍然有很大的使用价值，合理的规划和使用swap分区，对系统稳定运行至关重要。Linux下可以使用文件系统中的一个常规文件或者一个独立分区作为交换空间使用。同时linux允许使用多个交换分区或者交换文件。
 

1．创建swap交换空间
         创建交换空间所需的交换文件是一个普通的文件，但是，创建交换文件与创建普通文件不同，必须通过dd命令来完成，同时这个文件必须位于本地硬盘上，不能在网络文件系统（NFS）上创建swap交换文件。例如：
[root@localhost ~]# dd if=/dev/zero of=/data/swapfile bs=1024 count=65536
65536+0 records in
65536+0 records out
这样就创建一个有连续空间的交换文件，大小为60M左右，关于dd命令做简单的讲述:
if＝输入文件，或者设备名称。
of＝输出文件或者设备名称。
ibs=bytes 表示一次读入bytes 个字节(即一个块大小为 bytes 个字节)。
obs=bytes 表示一次写bytes 个字节(即一个块大小为 bytes 个字节)。
bs＝bytes，同时设置读写块的大小，以bytes为单位，此参数可代替 ibs 和 obs。
count=blocks 仅拷贝blocks个块。
skip=blocks 表示从输入文件开头跳过 blocks 个块后再开始复制。
seek=blocks表示从输出文件开头跳过 blocks 个块后再开始复制。(通常只有当输出文件是磁盘或磁带时才有效)
这里的输入设备/dev/zero代表一个输出永远为0的设备文件，使用它作输入可以得到全为空的文件。
 

2．激活和使用swap
首先通过mkswap命令指定作为交换空间的设备或者文件：
[root@localhost ~]#mkswap  /data/swapfile
Setting up swapspace version 1, size = 67104 kB
[root@localhost backup]# free
             total       used       free     shared    buffers     cached
Mem:       2066632    1998188      68444          0      26160    1588044
-/+ buffers/cache:     383984    1682648
Swap:      4088500     101036    3987464
从上面输出可知，我们指定了一个67104 kB的交换空间，而此时新建的交换空间还未被使用，下面简单介绍下mkswap命令，mkswap的一般使用格式为：
mkswap [参数] [设备名称或文件][交换区大小]
参数：
-c：建立交换区前，先检查是否有损坏的区块。
-v0：建立旧式交换区，此为预设值。
-v1：建立新式交换区。
交换区大小：指定交换区的大小，单位为1024字节。
设置交换分区后，接着通过swapon命令激活swap：
[root@localhost ~]#/usr/sbin/swapon /data/swapfile
[root@localhost backup]# free
             total       used       free     shared    buffers     cached
Mem:       2066632    1997668      68964          0      27404    1588880
-/+ buffers/cache:     381384    1685248
Swap:      4154028     100976    4053052
         通过free命令可以看出，swap大小已经由4088500k变为4154028k，相差的值是60M左右，刚好等于我们增加的一个交换文件大小，这说明新增的交换分区已经可以使用了，但是如果linux重启，那么新增的swap空间将变得不可用，因此需要在/etc/fstab中添加自动加载设置：
 /data/swapfile  none  swap  sw 0 0
如此以来，linux在重启后就可以实现自动加载swap分区了。其实linux在启动过程中会执行“swapon -a”命令，此命令会加载列在/etc/fstab中的所有交换空间。
 

3．移除swap
通过swapoff即可移除一个交换空间
[root@localhost ~]#/usr/sbin/swapoff /data/swapfile
其实也可以通过“swapoff -a”移除在/etc/fstab中定义的所有交换空间，这里的“swapoff -a”与上面提到的“swapon -a”对应。执行“swapoff -a”后，free命令输出如下：
[root@localhost backup]# free
             total       used       free     shared    buffers     cached
Mem:       2066632    2048724      17908          0      30352    1642748
-/+ buffers/cache:     375624    1691008
Swap:            0          0          0

系统中内存使用情况 cat /proc/meminfo

进程的内存使用情况：/proc/28040/status

查询进程 cpu 和内存使用占比 top

虚拟内存统计 vmstat

查询内存总使用率 free

伙伴系统
申请算法：
申请 2^i 个页块存储空间，如果 2^i 对应的块链表有空闲页块，则分配给应用
如果没有空闲页块，则查找 2^(i 1) 对应的块链表是否有空闲页块，如果有，则分配 2^i 块链表节点给应用，另外 2^i 块链表节点插入到 2^i 对应的块链表中
如果 2^(i 1) 块链表中没有空闲页块，则重复步骤 2，直到找到有空闲页块的块链表
如果仍然没有，则返回内存分配失败
2)    回收算法
释放 2^i 个页块存储空间，查找 2^i 个页块对应的块链表，是否有与其物理地址是连续的页块，如果没有，则无需合并

slab 算法——基本原理

它的基本思想是将内核中经常使用的对象放到高速缓存中，并且由系统保持为初始的可利用状态。比如进程描述符，内核中会频繁对此数据进行申请和释放
小内存分配

缺页异常

通过 get_free_pages 申请一个或多个物理页面
换算 addr 在进程 pdg 映射中所在的 pte 地址
将 addr 对应的 pte 设置为物理页面的首地址
系统调用：Brk—申请内存小于等于 128kb，do_map—申请内存大于 128kb


mmap

https://www.cnblogs.com/huxiao-tee/p/4660352.html

映射--缺页--swap cache--磁盘IO加载--系统自动进行脏页回

https://cloud.tencent.com/developer/article/1084101

内存管理，红黑树和链表两种形式来组织管理，红黑树的节点嵌入链表节点vm_area_struct中



mm_users：进程引用计数，vfork和clone时+1；

对于fork，则会分配新的空间，然后拷贝mm_struct

task_struct结构体是进程描述符，属于进程管理（PCB），其中，mm（memory manage）表示内存管理，它指向mm_struct结构体，它描述linux下进程的虚拟地址空间，它又包含两个重要字段：pgd、mmap，其中，pgd指向第一级页表的基址，而mmap指向一个vm_area_struct(区域结构)的链表，vm_start,vm_end分别表示数据的起始地址，vm_prot描述的是这个区域包含的所有页的读写许可权限；vm_flags描述这个区域是和别的进行共享的，还是该进程私有的

struct mm_struct {
	struct vm_area_struct * mmap;		/* list of VMAs */
	struct rb_root mm_rb;
	struct vm_area_struct * mmap_cache;	/* last find_vma result */
	unsigned long (*get_unmapped_area) (struct file *filp,
				unsigned long addr, unsigned long len,
				unsigned long pgoff, unsigned long flags);
	void (*unmap_area) (struct mm_struct *mm, unsigned long addr);
	unsigned long mmap_base;		/* base of mmap area */
	unsigned long task_size;		/* size of task vm space */
	unsigned long cached_hole_size; 	/* if non-zero, the largest hole below free_area_cache */
	unsigned long free_area_cache;		/* first hole of size cached_hole_size or larger */
	pgd_t * pgd;
	atomic_t mm_users;			/* How many users with user space? */
	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
	int map_count;				/* number of VMAs */
	struct rw_semaphore mmap_sem;
	spinlock_t page_table_lock;		/* Protects page tables and some counters */
 
	struct list_head mmlist;		/* List of maybe swapped mm's.	These are globally strung
						 * together off init_mm.mmlist, and are protected
						 * by mmlist_lock
						 */
 
	/* Special counters, in some configurations protected by the
	 * page_table_lock, in other configurations by being atomic.
	 */
	mm_counter_t _file_rss;
	mm_counter_t _anon_rss;
 
	unsigned long hiwater_rss;	/* High-watermark of RSS usage */
	unsigned long hiwater_vm;	/* High-water virtual memory usage */
 
	unsigned long total_vm, locked_vm, shared_vm, exec_vm;
	unsigned long stack_vm, reserved_vm, def_flags, nr_ptes;
	unsigned long start_code, end_code, start_data, end_data;
	unsigned long start_brk, brk, start_stack;
	unsigned long arg_start, arg_end, env_start, env_end;
 
	unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */
 
	cpumask_t cpu_vm_mask;
 
	/* Architecture-specific MM context */
	mm_context_t context;
 
	/* Swap token stuff */
	/*
	 * Last value of global fault stamp as seen by this process.
	 * In other words, this value gives an indication of how long
	 * it has been since this task got the token.
	 * Look at mm/thrash.c
	 */
	unsigned int faultstamp;
	unsigned int token_priority;
	unsigned int last_interval;
 
	unsigned long flags; /* Must use atomic bitops to access the bits */
 
	/* coredumping support */
	int core_waiters;
	struct completion *core_startup_done, core_done;
 
	/* aio bits */
	rwlock_t		ioctx_list_lock;
	struct kioctx		*ioctx_list;
};
struct page {
	unsigned long flags;		/* Atomic flags, some possibly
					 * updated asynchronously */
	atomic_t _count;		/* Usage count, see below. */
	union {
		atomic_t _mapcount;	/* Count of ptes mapped in mms,
					 * to show when page is mapped
					 * & limit reverse map searches.
					 */
		unsigned int inuse;	/* SLUB: Nr of objects */
	};
	union {
	    struct {
		unsigned long private;		/* Mapping-private opaque data:
					 	 * usually used for buffer_heads
						 * if PagePrivate set; used for
						 * swp_entry_t if PageSwapCache;
						 * indicates order in the buddy
						 * system if PG_buddy is set.
						 */
		struct address_space *mapping;	/* If low bit clear, points to
						 * inode address_space, or NULL.
						 * If page mapped as anonymous
						 * memory, low bit is set, and
						 * it points to anon_vma object:
						 * see PAGE_MAPPING_ANON below.
						 */
	    };
#if NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS
	    spinlock_t ptl;
#endif
	    struct kmem_cache *slab;	/* SLUB: Pointer to slab */
	    struct page *first_page;	/* Compound tail pages */
	};
	union {
		pgoff_t index;		/* Our offset within mapping. */
		void *freelist;		/* SLUB: freelist req. slab lock */
	};
	struct list_head lru;		/* Pageout list, eg. active_list
					 * protected by zone->lru_lock !
					 */
	/*
	 * On machines where all RAM is mapped into kernel address space,
	 * we can simply calculate the virtual address. On machines with
	 * highmem some memory is mapped into kernel virtual memory
	 * dynamically, so we need a place to store that address.
	 * Note that this field could be 16 bits on x86 ... ;)
	 *
	 * Architectures with slow multiplication can define
	 * WANT_PAGE_VIRTUAL in asm/page.h
	 */
#if defined(WANT_PAGE_VIRTUAL)
	void *virtual;			/* Kernel virtual address (NULL if
					   not kmapped, ie. highmem) */
#endif /* WANT_PAGE_VIRTUAL */
｝；



虚拟内存和物理内存的关系
  进程X                                                                      进程Y
+-------+                                                                  +-------+
| VPFN7 |--+                                                               | VPFN7 |
+-------+  |       进程X的                                 进程Y的           +-------+
| VPFN6 |  |      Page Table                              Page Table     +-| VPFN6 |
+-------+  |      +------+                                +------+       | +-------+
| VPFN5 |  +----->| .... |---+                    +-------| .... |<---+  | | VPFN5 |
+-------+         +------+   |        +------+    |       +------+    |  | +-------+
| VPFN4 |    +--->| .... |---+-+      | PFN4 |    |       | .... |    |  | | VPFN4 |
+-------+    |    +------+   | |      +------+    |       +------+    |  | +-------+
| VPFN3 |--+ |    | .... |   | | +--->| PFN3 |<---+  +----| .... |<---+--+ | VPFN3 |
+-------+  | |    +------+   | | |    +------+       |    +------+    |    +-------+
| VPFN2 |  +-+--->| .... |---+-+-+    | PFN2 |<------+    | .... |    |    | VPFN2 |
+-------+    |    +------+   | |      +------+            +------+    |    +-------+
| VPFN1 |    |               | +----->| FPN1 |                        +----| VPFN1 |
+-------+    |               |        +------+                             +-------+
| VPFN0 |----+               +------->| PFN0 |                             | VPFN0 |
+-------+                             +------+                             +-------+
 虚拟内存                               物理内存                               虚拟内存


PFN(the page frame number)： 页编号


当一个程序开始运行时，需要先到内存中读取该进程的指令，获取指令是用到的就是虚拟地址，该地址是程序链接时确定的，为了获取到实际的指令和数据，cpu需要借助进程的页表(page table)将虚拟地址转换为物理地址，页表里面的数据由操作系统维护。注意linux内核代码访问内存用的都是实际的物理地址，不存在虚拟地址到物理地址的转换，只有应用程序才需要。 为了方便转换，Linux将虚拟内存和物理内存的page都拆分为固定大小的页，一般是4k，每个页都会分配一个唯一的编号，就是页编号PFN。
从上面的图可以看出，虚拟内存和物理内存的page之间通过page table进行映射。进程X和Y的虚拟内存是相互独立的，他们的页表也是相互独立的，不同进程共享物理内存。进程可以随便访问自己的虚拟地址空间，而页表和物理内存由内核维护，当进程需要访问内存时，cpu会根据进程的页表将虚拟地址翻译成物理地址，然后进行访问。：并不是每个虚拟地址空间的page都有对应的Page Table相关联，只有虚拟地址被分配给进程后，也即进程调用类似malloc函数之后，系统才会为相应的虚拟地址在Page Table中添加记录，如果进程访问一个没有和Page Table关联的虚拟地址，系统将会抛出SIGSEGV信号，导致进程退出，这也是为什么我们访问野指针时会经常出现segmentfault的原因。换句话说，虽然每个进程都有4G（32位系统）的虚拟地址空间，但只有向系统申请了的那些地址空间才能用，访问未分配的地址空间将会出segmentfault错误。Linux会将虚拟地址0不映射到任何地方，这样我们访问空指针就一定会报segmentfault错误。

虚拟内存的优点
更大的地址空间：并且是连续的，使得程序编写、链接更加简单
进程隔离：不同进程的虚拟地址之间没有关系，所以一个进程的操作不会对其它进程造成影响
数据保护：每块虚拟内存都有相应的读写属性，这样就能保护程序的代码段不被修改，数据块不能被执行等，增加了系统的安全性
内存映射：有了虚拟内存之后，可以直接映射磁盘上的文件（可执行文件或动态库）到虚拟地址空间，这样可以做到物理内存延时分配，只有在需要读相应的文件的时候，才将它真正的从磁盘上加载到内存中来，而在内存吃紧的时候又可以将这部分内存清空掉，提高物理内存利用效率，并且所有这些对应用程序来说是都透明的
共享内存：比如动态库，只要在内存中存储一份就可以了，然后将它映射到不同进程的虚拟地址空间中，让进程觉得自己独占了这个文件。进程间的内存共享也可以通过映射同一块物理内存到进程的不同虚拟地址空间来实现共享
其它：有了虚拟地址空间后，交换空间和COW（copy on write）等功能都能很方便的实现
page table
page table可以简单的理解为一个memory mapping的链表（当然实际结构很复杂），里面的每个memory mapping都将一块虚拟地址映射到一个特定的资源（物理内存或者外部存储空间）。每个进程拥有自己的page table，和其它进程的page table没有关系。

memory mapping
每个memory mapping就是对一段虚拟内存的描述，包括虚拟地址的起始位置，长度，权限(比如这段内存里的数据是否可读、写、执行), 以及关联的资源(如物理内存page，swap空间上的page，磁盘上的文件内容等)。当进程申请内存时，系统将返回虚拟内存地址，同时为相应的虚拟内存创建memory mapping并将它放入page table，但这时系统不一定会分配相应的物理内存，系统一般会在进程真正访问这段内存的时候才会分配物理内存并关联到相应的memory mapping，这就是所谓的延时分配/按需分配。每个memory mapping都有一个标记，用来表示所关联的物理资源类型，一般分两大类，那就是anonymous和file backed。

file backed这种类型表示对应的物理资源存放在磁盘上的文件中，它所包含的信息包括文件的位置、offset、rwx权限等。当进程第一次访问对应的虚拟page的时候，由于在memory mapping中找不到对应的物理内存，CPU会报page fault中断，然后操作系统就会处理这个中断并将文件的内容加载到物理内存中，然后更新memory mapping，这样下次CPU就能访问这块虚拟地址了。以这种方式加载到内存的数据一般都会放到page cache中。一般程序的可执行文件，动态库都是以这种方式映射到进程的虚拟地址空间的。
anonymous类型： 程序自己用到的数据段和堆栈空间，以及通过mmap分配的共享内存，它们在磁盘上找不到对应的文件，所以这部分内存页被叫做anonymous page。anonymous page和file backed最大的差别是当内存吃紧时，系统会直接删除掉file backed对应的物理内存，因为下次需要的时候还能从磁盘加载到内存，但anonymous page不能被删除，只能被swap out。
shared： 不同进程的page table里面的多个memory mapping可以映射到相同的物理地址，通过虚拟地址可以访问到相同的内容，当一个进程修改内存的内容后，在另一个进程可以立即读取到，这种方式一般用来实现进程间高速的共享数据。当标记为shared的memory mapping被删除回收时，需要更新物理page上的引用计数，当物理page的计数变0后被回收。
copy on write： 它是基于shared技术，当读这种类型的内存时，系统不需要做任何特殊的操作，而当要写这块内存时，系统将会生成一块新的内存并拷贝原来内存中的数据到新内存中，然后将新内存关联到相应的memory mapping，接着执行写操作，linux很多功能都依赖于copy on write技术来提高性能，最常见的是fork。
我们来总结下内存的使用过程：
进程向系统发出内存申请请求
系统会检查进程的虚拟地址空间是否用完，如果有剩余，给进程分配虚拟地址
系统为这块虚拟地址创建相应的memory mapping，并把它放进进程的page table
系统返回虚拟地址给进程，进程开始访问虚拟地址
cpu根据虚拟地址在该进程的page table找到对应的memory mapping，但是该mapping没有和物理内存关联，于是产生缺页中断
操作系统收到缺页中断后，分配真正的物理内存并将它关联到对应的memory mapping
中断处理完成后，cpu就可以访问内存了。
当然缺页中断不是每次都会发生，只有系统觉得有必要延迟分配内存的时候才用的着， 也就是说上面第三步很多时候系统会分配真正的物理内存并关联memory mapping
其他概念
操作系统只要实现了虚拟内存和物理内存之间的映射关系就能正常工作了，但要使得内存访问更高效，还有很多需要考虑，下面我们来介绍一下与之相关的一些其他概念及其作用：

MMU（Memory Management Unit）
MMU是cpu的一个用来将进程的虚拟地址转换为物理地址的模块，它的输入是进程的page table和虚拟地址，输出是物理地址。将虚拟地址转换成物理地址的速度直接影响着系统的速度，所有cpu包含了这个硬件模块用来加速。

TLB (Translation Lookaside Buffer)
上面介绍到，MMU的输入是page table，而page table又存在内存里，和cpu的cache相比，内存的速度很慢，为了进一步加快虚拟地址到物理地址的转换速度，Linux发明了TLB，它存在于cpu的L1cache里面，用来缓存已经找到的虚拟地址和物理地址的映射，这样下次转换前先排查一下TLB，如果已经在里面了就不需要使用MMU进行转换了。

按需分配物理页

由于实际情况下，物理内存要比虚拟内存少很多，所以操作系统必须很小心的分配物理内存，以使内存的使用率达到最大化。一个节约物理内存的办法就是只加载当前正在使用的虚拟page对应的数据到内存。比如，一个很大的数据库程序，如果你只是用了查询操作，那么负责插入删除等部分的代码段就没必要加载到内存中，这样就能节约很多物理内存，这种方法就叫做物理内存页按需分配，也可以称作延时加载。

当CPU访问一个虚拟内存页的时候，如果这个虚拟内存页对应的数据还没加载到物理内存中，则CPU就会通知操作系统发生了page fault，然后由操作系统负责将数据加载进物理内存。由于将数据加载进内存比较耗时，所以CPU不会等在那里，而是去调度其它进程，当它下次再调度到该进程时，数据已经在物理内存上了。

Linux主要使用这种方式来加载可执行文件和动态库，当程序被内核开始调度执行时，内核将进程的可执行文件和动态库映射到进程的虚拟地址空间，并只加载马上要用到的那小部分数据到物理内存中，其它的部分只有当CPU访问到它们时才去加载。

访问控制
page table里面的每条虚拟内存到物理内存的映射记录（memory mapping）都包含一份控制信息，当进程要访问一块虚拟内存时，系统可以根据这份控制信息来检查当前的操作是否是合法的。

为什么需要做这个检查呢？比如有些内存里面放的是程序的可执行代码，那么就不应该去修改它；有些内存里面存放的是程序运行时用到的数据，那么这部分内存只能被读写，不应该被执行；有些内存里面存放的是内核的代码，那么在用户态就不应该去执行它；有了这些检查之后会大大增强系统的安全性

huge pages
由于CPU的cache有限，所以TLB里面缓存的数据也有限，而采用了huge page后，由于每页的内存变大（比如由原来的4K变成了4M），虽然TLB里面的纪录数没变，但这些纪录所能覆盖的地址空间变大，相当于同样大小的TLB里面能缓存的映射范围变大，从而减少了调用MMU的次数，加快了虚拟地址到物理地址的转换速度。

Caches
为了提高系统性能，Linux使用了一些跟内存管理相关的cache，并且尽量将空闲的内存用于这些cache。这些cache都是系统全局共享的：

Buffer Cache：用来缓冲块设备上的数据，比如磁盘，当读写块设备时，系统会将相应的数据存放到这个cache中，等下次再访问时，可以直接从cache中拿数据，从而提高系统效率。它里面的数据结构是一个块设备ID和block编号到具体数据的映射，只要根据块设备ID和块的编号，就能找到相应的数据。
Page Cache： 这个cache主要用来加快读写磁盘上文件的速度。它里面的数据结构是文件ID和offset到文件内容的映射，根据文件ID和offset就能找到相应的数据
从上面的定义可以看出，page cache和buffer cache有重叠的地方，不过实际情况是buffer cache只缓存page cache不缓存的那部分内容，比如磁盘上文件的元数据。所以一般情况下和page cache相比，Buffer Cache的大小基本可以忽略不计。

小结
学习了以上内容，我们结合top命令来看看各个字段表示的含义：

top - 22:56:37 up 5 days, 11:28,  1 user,  load average: 0.06, 0.05, 0.01
Tasks: 186 total,   1 running, 185 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.3 us,  0.3 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :   997596 total,   134328 free,   132816 used,   730452 buff/cache
KiB Swap:  1046524 total,  1017976 free,    28548 used.   635824 avail Mem 
KiB Mem代表物理内存，KiB Swap表示交换空间，他们的单位都是KiB（1k）。buff/cached代表了buff和cache总共用了多少，buff代表buffer cache占了多少空间，由于它主要用来缓存磁盘上文件的元数据，所以一般都比较小，跟cache比可以忽略不计；cache代表page cache和其它一些占用空间比较小且大小比较固定的cache的总和，基本上cache就约等于page cache，page cache的准确值可以通过查看/proc/meminf中的Cached得到。由于page cache是用来缓存磁盘上文件内容的，所以占有空间很大，Linux一般会尽可能多的将空闲物理内存用于page cache。
