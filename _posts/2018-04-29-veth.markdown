---
title: veth
layout: post
category: linux
author: 夏泽民
---
 Linux container 中用到一个叫做veth的东西，这是一种新的设备，专门为 container 所建。veth 从名字上来看是 Virtual ETHernet 的缩写，它的作用很简单，就是要把从一个 network namespace 发出的数据包转发到另一个 namespace。veth 设备是成对的，一个是 container 之中，另一个在 container 之外，即在真实机器上能看到的。 
  VETH设备总是成对出现，送到一端请求发送的数据总是从另一端以请求接受的形式出现。创建并配置正确后，向其一端输入数据，VETH会改变数据的方向并将其送入内核网络子系统，完成数据的注入，而在另一端则能读到此数据。（Namespace，其中往veth设备上任意一端上RX到的数据，都会在另一端上以TX的方式发送出去）veth工作在L2数据链路层，veth-pair设备在转发数据包过程中并不串改数据包内容。 
<!-- more -->
  显然，仅有veth-pair设备，容器是无法访问网络的。因为容器发出的数据包，实质上直接进入了veth1设备的协议栈里。如果容器需要访问网络，需要使用bridge等技术，将veth1接收到的数据包通过某种方式转发出去 
创建veth的命令如下：
ip link add name veth0 type veth0 peer name veth1
veth设备特点
veth和其它的网络设备都一样，一端连接的是内核协议栈
veth设备是成对出现的，另一端两个设备彼此相连
一个设备收到协议栈的数据发送请求后，会将数据发送到另一个设备上去
常用命令
创建network namespace
 # ip netns add sunldnamespace01
 # ip netns list
sunldnamespace01
创建veth
 # ip link add sunldveth01 type veth peer name sunldveth02
 # ip link list
显示信息如下：
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:2d:d4:23 brd ff:ff:ff:ff:ff:ff
20: sunldveth02: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether c6:bb:c0:d0:54:71 brd ff:ff:ff:ff:ff:ff
21: sunldveth01: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether da:a1:36:d1:3b:36 brd ff:ff:ff:ff:ff:ff
添加网卡到namespace
 # ip link set sunldveth01 netns sunldnamespace01
查看当前namespace中的veth，只有sunldveth02
 # ip link list
显示信息如下
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:2d:d4:23 brd ff:ff:ff:ff:ff:ff
3: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
    link/ether ba:d9:d4:48:55:65 brd ff:ff:ff:ff:ff:ff
20: sunldveth02: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether c6:bb:c0:d0:54:71 brd ff:ff:ff:ff:ff:ff
通过命令查看sunldnamespace01中的veth
 # ip netns exec sunldnamespace01 ip link list
显示信息如下
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
21: sunldveth01: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether da:a1:36:d1:3b:36 brd ff:ff:ff:ff:ff:ff
配置network namespace的网口
 # ip netns exec sunldnamespace01 ifconfig sunldveth01 100.2.96.2/16 up
 # ip netns exec sunldnamespace01 ip addr list
显示信息如下
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
21: sunldveth01: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000
    link/ether da:a1:36:d1:3b:36 brd ff:ff:ff:ff:ff:ff
    inet 100.2.96.2/16 brd 100.2.255.255 scope global sunldveth01
       valid_lft forever preferred_lft forever
开启空间脚本
 #可以使用这条命令开启一个 ns0 的 shell 
ip netns exec ns0 sh
network namespace
创建network namespace
 # ip netns add blue
 # ip netns list
blue
添加网口到namespace
先创建veth
 # ip link add veth0 type veth peer name veth1
在当前namespace可以看到veth0和veth1
 # ip link list
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:b2:cf:72 brd ff:ff:ff:ff:ff:ff
3: veth1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether ae:0d:00:e1:11:38 brd ff:ff:ff:ff:ff:ff
4: veth0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 42:e7:50:d4:bb:c5 brd ff:ff:ff:ff:ff:ff
将veth1加到namespace “blue”
 # ip link set veth1 netns blue
此时，当前namepapce只能看到veth0。
通过如下命令可以查看blue namespace的网口
 # ip netns exec blue ip link list
配置network namespace的网口
通过ip netns exec可以配置namespace的网口
 # ip netns exec blue ifconfig veth1 172.17.42.2/16 up
network namespace的网口与物理网卡的通信通过bridge来实现。
 # add the namespaces
ip netns add ns1
ip netns add ns2
 # create the veth pair
ip link add tap1 type veth peer name tap2
 # move the interfaces to the namespaces
ip link set tap1 netns ns1
ip link set tap2 netns ns2
 # bring up the links
ip netns exec ns1 ip link set dev tap1 up
ip netns exec ns2 ip link set dev tap2 up

 RJ45口的双绞线可以做物理层自环，1/3，2/6短接即可，这样一台机器的一块网卡自己就可以既发又收了，但是你能对比头发略粗的光纤做什么呢？真实的做法当然是用软件解决了，在Linux上可以使用netns来解决，即net namespace。
       netns是一个很好玩的东西，它可以让你在一台机器上模拟多个网络设备，这样做的意义是非同一般的：
1.使用netns可以充分利用闲置的处理器资源，特别是你的多块网卡性能压不满CPU的时候；
2.使用netns可以将不同类型的网络应用隔离，针对每一类实施不同的策略；
3.使用netns有点玩虚拟化的意思，不过比虚拟机更灵活。
一个net namespace有自己独立的路由表，iptables策略，设备管理机构，和其它的netns完全隔离，比如你将eth0加入了netns1，那么netns2中的应用程序就看不到eth0，网卡设备管理只是netns中的一个元素，还有很多，比如你在netns1中配置的iptables策略对netns2中的数据包没有任何影响。总之，如果你懂Linux内核源码，那么只要附着有net结构体字段的那些结构，比如skb，net_device，都和netns有关。
       那么我应该怎么做自环呢？我的设备有4个网卡，我希望1和4之间通信，通过2和3转发，它的逻辑拓扑如下：
PC1/eth0----PC2/eth1(forward)PC2/eth2----PC3/eth3
很简单，将eth0和eth3设置在两个不同的netns，然后用线缆连接eth0和eth1，同样连接eth2和eth3，最后将eth0和eth1的IP地址设置在一个网段，将eth2和eth3的IP地址设置在另一个不同的网段即可。光说不练假把式，具体应该怎么做呢？同样很简单：
1.添加两个netns
ip netns add t1
ip netns add t2
2.将eth0加入t1，并且设置IP地址
ip link set eth0 netns t1
此时再ifconfig就看不到eth0了，你甚至执行ls /sys/class/net也看不到eth0了，只有执行ip netns exec t1 ls /sys/class/net才能看到。
ip netns exec t1 ifconfig eth0 192.168.1.200/24
3.将eth3加入t2，并且设置IP地址
ip link set eth3 netns t2
此时ifconfig就看不到eth3了，你甚至执行ls /sys/class/net也看不到eth3了，只有执行ip netns exec t2 ls /sys/class/net才能看到。
ip netns exec t1 ifconfig eth3 172.16.1.200/24
4.设置eth1和eth2的地址
ifconfig eth1 192.168.1.1/24
ifconfig eth2 172.16.1.1/24
5.设置两个netns的默认路由
ip netns exec t1 route add default gw 192.168.1.1
ip netns exec t2 route add default gw 172.16.1.1
6.测试
在netns t1中ping netns t2中的eth3地址
ip netns exec t1 ping 172.16.1.200
上述配置之后，从eth0发出的包会通过网线到达eth1(而不是走local路由表的loopback)，然后经过eth1的forward从eth2发出。经由网线到达目的地eth3杯接收。整个过程中就一台机器，展示出的效果好像三台机器的样子。有了这个机制，是不是再也不用为搭建测试环境而发愁了呢？
       除了自环测试之外，netns还可以用于设置策略路由，这种策略路由不需要ip rule。试想一种场景，你同时运行了P1和P2两个程序，本机所在的局域网有两个出口到达外网，你希望P1通过gw1和外界通信，P2通过gw2和外界通信，约束条件是你的机器只有一张网卡eth0，怎么办呢？通过iptables为P1和P2的数据包打上不同的mark，然后通过ip rule设置策略路由无疑可以解决，另外直接在P1和P2应用程序中用setsockopt也是可以设置ipmark的，这就不需要iptables了。然而这一切都过时了，2014年我需要一种不同的方式。
       我不知道怎么表达我思考的过程，但是给出一个操作序列是简单的事情，因为照着这么做确实可以满足需求，然后看到这篇文章的人照着操作步骤倒推回去，就可以得到一个思考过程。首先你要明白的是Linux内核支持一种虚拟网卡类型，即veth，一般而言veth是成对的，从一个veth发出的数据包可以直接到达它的peer veth，感兴趣的可以看Linux内核的drivers/net/veth.c，和drivers/net/tun.c没什么不同，更简单些罢了。第一步要做的就是建立一对veth：
ip link add veth1 type veth peer name veth2
此时系统中除了eth0之外又多了两块网卡，所有的网卡为lo，eth0，veth1，veth2。中间隐含着一个事实，即veth1和veth2之间有一条虚拟的链路将两块网卡连接起来，就好像一条双绞线连接的两块物理网卡一样。我现在希望P1的数据包通过veth1发出，然后自然而然地就能发到veth2，但是随后怎么通过eth0发到物理线路呢？太简单，太简单，使用bridge吧：
brctl addbr br0
brctl addif br0 eth0 veth2
同时，veth1和br0所在的局域网设置在一个IP网段中，这下子就全通了，该二层网络的逻辑拓扑为：
veth1----veth2(bridge)eth0----gw(1,2)
怎么设置netns我本来不想说了，但是由于小小暂时不跟我玩了，我还是写完吧。首先将veth1设置到netns1(具体怎么创建netns，不再赘述)并设置路由：
ip link set veth1 netns netns1
ip netns exec netns1 route add default gw $gw1
route add default gw $gw2
这就完了？是的，完事了。事实上，保留br0的默认netns即可，没有必要创建netns2了。接下来需要做的就是启动P1和P2了：
ip netns exec netns1 P1
P2
好了，一切结束。
       我始终都觉得，在Linux上一般都是不用修改源码就能解决问题，可是我还是喜欢修改代码，原因何在？很简单，源码很容易获得，并且源码很容易修改，我走火入魔般地写了大量的Netfilter扩展以及做了大量的nf_conntrack修改，甚至还添加了一些该死的socket filter...虽然这些行为都是自娱自乐型的，并没有被应用在工作中，但是这些行为说明我不是网络管理员，而是一名程序员，哈哈，自封的资深软件工程师(我还是觉得这些成果能被应用)。然而，做一名技术精湛的网络管理人员的难度却远远超过做程序员的难度。这不，又一次遇到了OpenVPN的多实例问题，我觉得，单纯的程序员搞不定它，单纯的网管也不行。
       TAP模式的多实例已经被我用Linux Bridge完美蹂躏了，但是TUN模式的多实例问题仍然没有完美的方案，虽然修改tun驱动，使用broadcast mode bonding+tun filter可以解决，但是我还是觉得那是一种走火入魔的方式，因此就算在公司我也没能将整个调试测试进行下去，结果落了个不了了之，事实上，是我太不喜欢那种方式。tun的IP filter是我改出来的方案，并非标准的，能不能使用标准的方式进行寻址呢？使用netns，答案就是肯定的。
       假设在GW上启动了2个OpenVPN实例ovpn1和ovpn2，虚拟网卡分别为tun1和tun2，在client-connect脚本中得知ovpn2负责N1，ovpn2负责N2。现在问题的关键是，GW后方出发的数据包如何知道是将数据包发送到tun1还是tun2，这个判断能不能自动进行？如果使用netns，那就是可以的，我可以将2个tun分别设置在不同的netns，然后每一个netns对应一个同处一个netns的veth虚拟网卡，这些veth的peer们处在另外一个netns中，这样就可以实现IP层TUN模式虚拟网卡到以太网的TAP模式虚拟网卡的适配。最后将这些peer们Bridge成一个br0，那么TUN模式的OpenVPN就能和TAP模式的OpenVPN采用同一种方式处理了。
       不管怎样，当你玩弄netns的时候，你要知道你并不是在玩弄冷酷无情的虚拟化操作系统，也不是真的模拟了两台物理上相互隔离的机器，因为虽然两个程序的网络是隔离的，但是文件系统却是共享的。你要时刻准备着，使用网络隔离和使用内存，文件系统共享相结合。将一台机器既可以作为多台机器使用，又可以作为一台机器共享资源！
       不管怎样，当你玩弄netns的时候，你要知道你并不是在玩弄冷酷无情的虚拟化操作系统，也不是真的模拟了两台物理上相互隔离的机器，因为虽然两个程序的网络是隔离的，但是文件系统却是共享的。你要时刻准备着，使用网络隔离和使用内存，文件系统共享相结合。将一台机器既可以作为多台机器使用，又可以作为一台机器共享资源！
       理解了上述的例子和最后的总结，那么我来发问，单网卡或者没有网卡怎么玩自环？这个需求可能就是为了测试一下协议栈而已。略去思考的过程，很简单，多加一个层次。比如你有一台机器一块网卡也没有，那么你只需要下面的命令就可以在你的机器上实现IP转发或者bridge转发了：
ip link add v1 type veth peer name vp1
ip link add v2 type veth peer name vp2
brctl addbr br0
brctl addif vp1 vp2
ifconfig vp1 up
ifconfig vp2 up
sysctl -w net.ipv4.ip_forward=1
ip netns add t1
ip netns add t2
ip link set v1 netns t1
ip link set v2 netns t2
ip netns exec t1 ifconfig v1 1.1.1.1/24
ip netns exec t2 ifconfig v2 1.1.1.2/24
ip netns exec t1 ping 1.1.1.2

在Linux虚拟化技术中，网络层面，通常除了网桥或虚拟交换机技术外。
还有一个重要的就是namespace和veth pair。
net namespace主要是隔离网络设备本身，例如在Linux 中有多个容器，每个容器对应各自的namespace，我们可以把不同的网络设备指派给不同的容器。
veth pair你可以理解为使用网线连接好的两个接口，把两个端口放到两个namespace中，那么这两个namespace就能打通。
如果要把namespace和本地网络打通，也可以创建veth设备，把两端分别放入本地和namespace。
