---
title: MemStats
layout: post
category: golang
author: 夏泽民
---
runtime.MemStats这个结构体包含的字段比较多，但是大多都很有用：
1、Alloc uint64 //golang语言框架堆空间分配的字节数
2、TotalAlloc uint64 //从服务开始运行至今分配器为分配的堆空间总 和，只有增加，释放的时候不减少
3、Sys uint64 //服务现在系统使用的内存
4、Lookups uint64 //被runtime监视的指针数
5、Mallocs uint64 //服务malloc heap objects的次数
6、Frees uint64 //服务回收的heap objects的次数
7、HeapAlloc uint64 //服务分配的堆内存字节数
8、HeapSys uint64 //系统分配的作为运行栈的内存
9、HeapIdle uint64 //申请但是未分配的堆内存或者回收了的堆内存（空闲）字节数
10、HeapInuse uint64 //正在使用的堆内存字节数
10、HeapReleased uint64 //返回给OS的堆内存，类似C/C++中的free。
11、HeapObjects uint64 //堆内存块申请的量
12、StackInuse uint64 //正在使用的栈字节数
13、StackSys uint64 //系统分配的作为运行栈的内存
14、MSpanInuse uint64 //用于测试用的结构体使用的字节数
15、MSpanSys uint64 //系统为测试用的结构体分配的字节数
16、MCacheInuse uint64 //mcache结构体申请的字节数(不会被视为垃圾回收)
17、MCacheSys uint64 //操作系统申请的堆空间用于mcache的字节数
18、BuckHashSys uint64 //用于剖析桶散列表的堆空间
19、GCSys uint64 //垃圾回收标记元信息使用的内存
20、OtherSys uint64 //golang系统架构占用的额外空间
21、NextGC uint64 //垃圾回收器检视的内存大小
22、LastGC uint64 // 垃圾回收器最后一次执行时间。
23、PauseTotalNs uint64 // 垃圾回收或者其他信息收集导致服务暂停的次数。
24、PauseNs [256]uint64 //一个循环队列，记录最近垃圾回收系统中断的时间
25、PauseEnd [256]uint64 //一个循环队列，记录最近垃圾回收系统中断的时间开始点。
26、NumForcedGC uint32 //服务调用runtime.GC()强制使用垃圾回收的次数。
27、GCCPUFraction float64 //垃圾回收占用服务CPU工作的时间总和。如果有100个goroutine，垃圾回收的时间为1S,那么就占用了100S。
28、BySize //内存分配器使用情况
<!-- more -->
用Golang这样带GC的语言编写长期运行的网络服务有一个很大的挑战，那就是内存管理。

为了理解Golang的内存管理有必要对run-time源码进行深挖。有两个进程区分应用程序不再使用的内存，当它们看起来不会再使用，就把它们归还到操作系统（在Golang源码里称为scavenging ）。

这里有一个简单的程序制造了大量的垃圾（garbage），每秒钟创建一个 5,000,000 到 10,000,000 bytes 的数组。程序维持了20个这样的数组，其他的则被丢弃。程序这样设计是为了模拟一种非常常见的情况：随着时间的推移，程序中的不同部分申请了内存，有一些被保留，但大部分不再重复使用。在Go语言网络编程中，用goroutines 来处理网络连接和网络请求时（network connections or requests），通常goroutines都会申请一块内存（比如slice来存储收到的数据）然后就不再使用它们了。随着时间的推移，会有大量的内存被网络连接（network connections）使用，连接累积的垃圾come and gone。

package main
 
import (  
    "fmt" 
    "math/rand" 
    "runtime" 
    "time"
)  
 
func makeBuffer() []byte {  
    return make([]byte, rand.Intn(5000000)+5000000)  
}
 
func main() {  
    pool := make([][]byte, 20)
 
    var m runtime.MemStats  
    makes := 0  
    for {  
        b := makeBuffer()
        makes += 1
        i := rand.Intn(len(pool))
        pool[i] = b
 
        time.Sleep(time.Second)
 
        bytes := 0
 
        for i := 0; i < len(pool); i++ {
            if pool[i] != nil {
                bytes += len(pool[i])
            }
        }
 
        runtime.ReadMemStats(&m)
        fmt.Printf("%d,%d,%d,%d,%d,%d\n", m.HeapSys, bytes, m.HeapAlloc,
            m.HeapIdle, m.HeapReleased, makes)
    }
}
程序使用 runtime.ReadMemStats函数来获取堆的使用信息。它打印了四个值，

HeapSys：程序向应用程序申请的内存

HeapAlloc：堆上目前分配的内存

HeapIdle：堆上目前没有使用的内存

HeapReleased：回收到操作系统的内存

GC在Golang中运行的很频繁（参见GOGC环境变量（GOGC environment variable ）来理解怎样控制垃圾回收操作），因此在运行中由于一些内存被标记为”未使用“，堆上的内存大小会发生变化：这会导致HeapAlloc和HeapIdle发生变化。Golang中的scavenger 会释放那些超过5分钟仍然没有再使用的内存，因此HeapReleased不会经常变化

下面这张图是上面的程序运行了10分钟以后的情况：



(在这张和后续的图中，左轴以是以byte为单位的内存大小，右轴是程序执行次数)

红线展示了pool中byte buffers的数量。20个 buffers 很快达到150,000,000 bytes。最上方的蓝色线表示程序从操作系统申请的内存。稳定在375,000,000 bytes。因此程序申请了2.5倍它所需的空间！

当GC发生时，HeapIdle和HeapAlloc发生跳变。橘色的线是makeBuffer()发送的次数。

这种过度的内存申请是有GC的程序的通病，参见这篇paper

Quantifying the Performance of Garbage Collection vs. Explicit Memory Management

程序不断执行，idle memory（即HeapIdle）会被重用，但很少归还到操作系统。

 

解决此问题的一个办法是在程序中手动进行内存管理。例如，

程序可以这样重写：
package main
 
import (
    "fmt"
    "math/rand"
    "runtime"
    "time"
)
 
func makeBuffer() []byte {
    return make([]byte, rand.Intn(5000000)+5000000)
}
 
func main() {
    pool := make([][]byte, 20)
 
    buffer := make(chan []byte, 5)
 
    var m runtime.MemStats
    makes := 0
    for {
        var b []byte
        select {
        case b = <-buffer:
        default:
            makes += 1
            b = makeBuffer()
        }
 
        i := rand.Intn(len(pool))
        if pool[i] != nil {
            select {
            case buffer <- pool[i]:
                pool[i] = nil
            default:
            }
        }
 
        pool[i] = b
 
        time.Sleep(time.Second)
 
        bytes := 0
        for i := 0; i < len(pool); i++ {
            if pool[i] != nil {
                bytes += len(pool[i])
            }
        }
 
        runtime.ReadMemStats(&m)
        fmt.Printf("%d,%d,%d,%d,%d,%d\n", m.HeapSys, bytes, m.HeapAlloc,
            m.HeapIdle, m.HeapReleased, makes)
    }
}
下面这张图是上面的程序运行了10分钟以后的情况：



这张图展示了完全不同的情况。实际使用的buffer几乎等于从操作系统中申请的内存。同时GC几乎没有工作可做。堆上只有很少的HeapIdle最终需要归还到操作系统。

这段程序中内存回收机制的关键操作就是一个缓冲的channel ——buffer，在上面的代码中，buffer是一个可以存储5个[]byte slice的容器。当程序需要空间时，首先会使用select从buffer中读取：

select {

case b = <- buffer:

default :

makes += 1

b = makeBuffer()

}

这永远不会阻塞因为如果channel中有数据，就会被读出，如果channel是空的（意味着接收会阻塞），则会创建一个。

使用类似的非阻塞机制将slice回收到buffer：

select {

case buffer <- pool[i]:

pool[i] = nil

 default:

}

如果buffer 这个channel满了，则以上的写入过程会阻塞，这种情况下default触发。这种简单的机制可以用于安全的创建一个共享池，甚至可通过channel传递实现多个goroutines之间的完美、安全共享。

在我们的实际项目中运用了相似的技术，实际使用中（简单版本）的回收器（recycler ）展示在下面，有一个goroutine 处理buffers的构造并在多个goroutine之间共享。get(获取一个新buffer)和give(回收一个buffer到pool)这两个channel被所有goroutines使用。

回收器对收回的buffer保持连接，并定期的丢弃那些过于陈旧可能不会再使用的buffer（在示例代码中这个周期是一分钟）。这让程序可以自动应对爆发性的buffers需求。

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
package main
 
import (
    "container/list"
    "fmt"
    "math/rand"
    "runtime"
    "time"
)
 
var makes int
var frees int
 
func makeBuffer() []byte {
    makes += 1
    return make([]byte, rand.Intn(5000000)+5000000)
}
 
type queued struct {
    when time.Time
    slice []byte
}
 
func makeRecycler() (get, give chan []byte) {
    get = make(chan []byte)
    give = make(chan []byte)
 
    go func() {
        q := new(list.List)
        for {
            if q.Len() == 0 {
                q.PushFront(queued{when: time.Now(), slice: makeBuffer()})
            }
 
            e := q.Front()
 
            timeout := time.NewTimer(time.Minute)
            select {
            case b := <-give:
                timeout.Stop()
                q.PushFront(queued{when: time.Now(), slice: b})
 
           case get <- e.Value.(queued).slice:
               timeout.Stop()
               q.Remove(e)
 
           case <-timeout.C:
               e := q.Front()
               for e != nil {
                   n := e.Next()
                   if time.Since(e.Value.(queued).when) > time.Minute {
                       q.Remove(e)
                       e.Value = nil
                   }
                   e = n
               }
           }
       }
 
    }()
 
    return
}
 
func main() {
    pool := make([][]byte, 20)
 
    get, give := makeRecycler()
 
    var m runtime.MemStats
    for {
        b := <-get
        i := rand.Intn(len(pool))
        if pool[i] != nil {
            give <- pool[i]
        }
 
        pool[i] = b
 
        time.Sleep(time.Second)
 
        bytes := 0
        for i := 0; i < len(pool); i++ {
            if pool[i] != nil {
                bytes += len(pool[i])
            }
        }
 
        runtime.ReadMemStats(&m)
        fmt.Printf("%d,%d,%d,%d,%d,%d,%d\n", m.HeapSys, bytes, m.HeapAlloc
             m.HeapIdle, m.HeapReleased, makes, frees)
    }
}
执行程序10分钟，图像会类似于第二幅：



这些技术可以用于程序员知道某些内存可以被重用，而不用借助于GC，可以显著的减少程序的内存使用，同时可以使用在其他数据类型而不仅是[]byte slice，任意类型的Go type（用户定义的或许不行（user-defined or not））都可以用类似的手段回收。

以linux服务为例子，正常情况下，要获取服务内存信息可以通过相关的命令，例如top、ps等命令。这些监控内存使用情况的方法，一般需要编写脚本，执行脚本后将执行结果发送给对应的监控服务，从而达到监控的效果。但是golang自带的包却有一个runtime包，可以轻松获取服务运行时候的各种包括内存使用情况的信息。
使用linux命令，一般情况下只能看服务使用了多少内存。但是服务内存具体的使用情况缺无法获取。golang的runtime包可以做到获取服务总共使用主机多少内存，也可以获取服务已经申请了多少内存，以及内存的分布。个人觉得golang的runtime包功能很强大，可以获取很多服务运行信息，后续要仔细学习。这里先重点来看下跟内存相关的接口，本博客是基于GO1.9 windows64版本的代码和运行环境进行分析。
runtime中和内存使用情况相关的结构体为runtime.MemStats，这个结构定义了golang运行过程中所有内存相关的信息，在源代码中定义如下：

// A MemStats records statistics about the memory allocator. 记录内存分配器的信息
type MemStats struct {
    // General statistics.

    // Alloc is bytes of allocated heap objects.
    // 堆空间分配的字节数
    // This is the same as HeapAlloc (see below).
    Alloc uint64

    // TotalAlloc is cumulative bytes allocated for heap objects.
    //
    // TotalAlloc increases as heap objects are allocated, but
    // unlike Alloc and HeapAlloc, it does not decrease when
    // objects are freed. 从服务开始运行至今分配器为分配的堆空间总和
    TotalAlloc uint64

    // Sys is the total bytes of memory obtained from the OS.
    //
    // Sys is the sum of the XSys fields below. Sys measures the
    // virtual address space reserved by the Go runtime for the
    // heap, stacks, and other internal data structures. It's
    // likely that not all of the virtual address space is backed
    // by physical memory at any given moment, though in general
    // it all was at some point. 服务现在使用的内存
    Sys uint64

    // Lookups is the number of pointer lookups performed by the
    // runtime.
    //
    // This is primarily useful for debugging runtime internals. 被runtime监视的指针数
    Lookups uint64

    // Mallocs is the cumulative count of heap objects allocated. 服务malloc的次数
    // The number of live objects is Mallocs - Frees.
    Mallocs uint64

    // Frees is the cumulative count of heap objects freed. 服务回收的heap objects
    Frees uint64

    // Heap memory statistics.
    //
    // Interpreting the heap statistics requires some knowledge of
    // how Go organizes memory. Go divides the virtual address
    // space of the heap into "spans", which are contiguous
    // regions of memory 8K or larger. A span may be in one of
    // three states:
    //
    // An "idle" span contains no objects or other data. The
    // physical memory backing an idle span can be released back
    // to the OS (but the virtual address space never is), or it
    // can be converted into an "in use" or "stack" span.
    //
    // An "in use" span contains at least one heap object and may
    // have free space available to allocate more heap objects.
    //
    // A "stack" span is used for goroutine stacks. Stack spans
    // are not considered part of the heap. A span can change
    // between heap and stack memory; it is never used for both
    // simultaneously.

    // HeapAlloc is bytes of allocated heap objects.
    //
    // "Allocated" heap objects include all reachable objects, as
    // well as unreachable objects that the garbage collector has
    // not yet freed. Specifically, HeapAlloc increases as heap
    // objects are allocated and decreases as the heap is swept
    // and unreachable objects are freed. Sweeping occurs
    // incrementally between GC cycles, so these two processes
    // occur simultaneously, and as a result HeapAlloc tends to
    // change smoothly (in contrast with the sawtooth that is
    // typical of stop-the-world garbage collectors).
    //服务分配的堆内存
    HeapAlloc uint64

    // HeapSys is bytes of heap memory obtained from the OS.
    //
    // HeapSys measures the amount of virtual address space
    // reserved for the heap. This includes virtual address space
    // that has been reserved but not yet used, which consumes no
    // physical memory, but tends to be small, as well as virtual
    // address space for which the physical memory has been
    // returned to the OS after it became unused (see HeapReleased
    // for a measure of the latter).
    //
    // HeapSys estimates the largest size the heap has had.
    //系统分配的堆内存
    HeapSys uint64

    // HeapIdle is bytes in idle (unused) spans.
    //
    // Idle spans have no objects in them. These spans could be
    // (and may already have been) returned to the OS, or they can
    // be reused for heap allocations, or they can be reused as
    // stack memory.
    //
    // HeapIdle minus HeapReleased estimates the amount of memory
    // that could be returned to the OS, but is being retained by
    // the runtime so it can grow the heap without requesting more
    // memory from the OS. If this difference is significantly
    // larger than the heap size, it indicates there was a recent
    // transient spike in live heap size.
    //申请但是为分配的堆内存，（或者回收了的堆内存）
    HeapIdle uint64

    // HeapInuse is bytes in in-use spans.
    //
    // In-use spans have at least one object in them. These spans
    // can only be used for other objects of roughly the same
    // size.
    //
    // HeapInuse minus HeapAlloc esimates the amount of memory
    // that has been dedicated to particular size classes, but is
    // not currently being used. This is an upper bound on
    // fragmentation, but in general this memory can be reused
    // efficiently.
    //正在使用的堆内存
    HeapInuse uint64

    // HeapReleased is bytes of physical memory returned to the OS.
    //
    // This counts heap memory from idle spans that was returned
    // to the OS and has not yet been reacquired for the heap.
    //返回给OS的堆内存，类似C/C++中的free。
    HeapReleased uint64

    // HeapObjects is the number of allocated heap objects.
    //
    // Like HeapAlloc, this increases as objects are allocated and
    // decreases as the heap is swept and unreachable objects are
    // freed.
    //堆内存块申请的量
    HeapObjects uint64

    // Stack memory statistics.
    //
    // Stacks are not considered part of the heap, but the runtime
    // can reuse a span of heap memory for stack memory, and
    // vice-versa.

    // StackInuse is bytes in stack spans.
    //
    // In-use stack spans have at least one stack in them. These
    // spans can only be used for other stacks of the same size.
    //
    // There is no StackIdle because unused stack spans are
    // returned to the heap (and hence counted toward HeapIdle).
    //正在使用的栈
    StackInuse uint64

    // StackSys is bytes of stack memory obtained from the OS.
    //
    // StackSys is StackInuse, plus any memory obtained directly
    // from the OS for OS thread stacks (which should be minimal).
    //系统分配的作为运行栈的内存
    StackSys uint64

    // Off-heap memory statistics.
    //
    // The following statistics measure runtime-internal
    // structures that are not allocated from heap memory (usually
    // because they are part of implementing the heap). Unlike
    // heap or stack memory, any memory allocated to these
    // structures is dedicated to these structures.
    //
    // These are primarily useful for debugging runtime memory
    // overheads.

    // MSpanInuse is bytes of allocated mspan structures. 用于测试用的结构体使用的字节数
    MSpanInuse uint64

    // MSpanSys is bytes of memory obtained from the OS for mspan
    // structures. 系统为测试用的结构体分配的字节数
    MSpanSys uint64

    // MCacheInuse is bytes of allocated mcache structures. mcache结构体申请的字节数
    MCacheInuse uint64

    // MCacheSys is bytes of memory obtained from the OS for
    // mcache structures. 操作系统申请的堆空间用于mcache的量
    MCacheSys uint64

    // BuckHashSys is bytes of memory in profiling bucket hash tables.用于剖析桶散列表的堆空间
    BuckHashSys uint64

    // GCSys is bytes of memory in garbage collection metadata. 垃圾回收标记元信息使用的内存
    GCSys uint64

    // OtherSys is bytes of memory in miscellaneous off-heap
    // runtime allocations.  golang系统架构占用的额外空间
    OtherSys uint64

    // Garbage collector statistics.

    // NextGC is the target heap size of the next GC cycle.
    //
    // The garbage collector's goal is to keep HeapAlloc ≤ NextGC.
    // At the end of each GC cycle, the target for the next cycle
    // is computed based on the amount of reachable data and the
    // value of GOGC. 垃圾回收器检视的内存大小
    NextGC uint64

    // LastGC is the time the last garbage collection finished, as
    // nanoseconds since 1970 (the UNIX epoch).
    // 垃圾回收器最后一次执行时间。
    LastGC uint64

    // PauseTotalNs is the cumulative nanoseconds in GC
    // stop-the-world pauses since the program started.
    //
    // During a stop-the-world pause, all goroutines are paused
    // and only the garbage collector can run. 
    // 垃圾回收或者其他信息收集导致服务暂停的次数。
    PauseTotalNs uint64

    // PauseNs is a circular buffer of recent GC stop-the-world
    // pause times in nanoseconds.
    //
    // The most recent pause is at PauseNs[(NumGC+255)%256]. In
    // general, PauseNs[N%256] records the time paused in the most
    // recent N%256th GC cycle. There may be multiple pauses per
    // GC cycle; this is the sum of all pauses during a cycle. 一个循环队列，记录最近垃圾回收系统中断的时间
    PauseNs [256]uint64

    // PauseEnd is a circular buffer of recent GC pause end times,
    // as nanoseconds since 1970 (the UNIX epoch).
    //
    // This buffer is filled the same way as PauseNs. There may be
    // multiple pauses per GC cycle; this records the end of the
    // last pause in a cycle. 一个循环队列，记录最近垃圾回收系统中断的时间开始点。
    PauseEnd [256]uint64

    // NumGC is the number of completed GC cycles.
    //垃圾回收的内存大小
    NumGC uint32

    // NumForcedGC is the number of GC cycles that were forced by
    // the application calling the GC function.
    //服务调用runtime.GC()强制使用垃圾回收的次数。
    NumForcedGC uint32

    // GCCPUFraction is the fraction of this program's available
    // CPU time used by the GC since the program started.
    //
    // GCCPUFraction is expressed as a number between 0 and 1,
    // where 0 means GC has consumed none of this program's CPU. A
    // program's available CPU time is defined as the integral of
    // GOMAXPROCS since the program started. That is, if
    // GOMAXPROCS is 2 and a program has been running for 10
    // seconds, its "available CPU" is 20 seconds. GCCPUFraction
    // does not include CPU time used for write barrier activity.
    //
    // This is the same as the fraction of CPU reported by
    // GODEBUG=gctrace=1.
    //垃圾回收占用服务CPU工作的时间总和。如果有100个goroutine，垃圾回收的时间为1S,那么久占用了100S
    GCCPUFraction float64

    // EnableGC indicates that GC is enabled. It is always true,
    // even if GOGC=off.
    //是否启用GC
    EnableGC bool

    // DebugGC is currently unused.
    DebugGC bool

    // BySize reports per-size class allocation statistics.
    //
    // BySize[N] gives statistics for allocations of size S where
    // BySize[N-1].Size < S ≤ BySize[N].Size.
    //
    // This does not report allocations larger than BySize[60].Size.
    //内存分配器使用情况
    BySize [61]struct {
        // Size is the maximum byte size of an object in this
        // size class.
        Size uint32

        // Mallocs is the cumulative count of heap objects
        // allocated in this size class. The cumulative bytes
        // of allocation is Size*Mallocs. The number of live
        // objects in this size class is Mallocs - Frees.
        Mallocs uint64

        // Frees is the cumulative count of heap objects freed
        // in this size class.
        Frees uint64
    }
}
runtime.MemStats这个结构体包含的字段比较多，但是大多都很有用，去掉那些注释来看各个属性，会发现各个属性都是很有价值的：
1、Alloc uint64 //golang语言框架堆空间分配的字节数
2、TotalAlloc uint64 //从服务开始运行至今分配器为分配的堆空间总 和，只有增加，释放的时候不减少
3、Sys uint64 //服务现在系统使用的内存
4、Lookups uint64 //被runtime监视的指针数
5、Mallocs uint64 //服务malloc的次数
6、Frees uint64 //服务回收的heap objects的字节数
7、HeapAlloc uint64 //服务分配的堆内存字节数
8、HeapSys uint64 //系统分配的作为运行栈的内存
9、HeapIdle uint64 //申请但是未分配的堆内存或者回收了的堆内存（空闲）字节数
10、HeapInuse uint64 //正在使用的堆内存字节数
10、HeapReleased uint64 //返回给OS的堆内存，类似C/C++中的free。
11、HeapObjects uint64 //堆内存块申请的量
12、StackInuse uint64 //正在使用的栈字节数
13、StackSys uint64 //系统分配的作为运行栈的内存
14、MSpanInuse uint64 //用于测试用的结构体使用的字节数
15、MSpanSys uint64 //系统为测试用的结构体分配的字节数
16、MCacheInuse uint64 //mcache结构体申请的字节数(不会被视为垃圾回收)
17、MCacheSys uint64 //操作系统申请的堆空间用于mcache的字节数
18、BuckHashSys uint64 //用于剖析桶散列表的堆空间
19、GCSys uint64 //垃圾回收标记元信息使用的内存
20、OtherSys uint64 //golang系统架构占用的额外空间
21、NextGC uint64 //垃圾回收器检视的内存大小
22、LastGC uint64 // 垃圾回收器最后一次执行时间。
23、PauseTotalNs uint64 // 垃圾回收或者其他信息收集导致服务暂停的次数。
24、PauseNs [256]uint64 //一个循环队列，记录最近垃圾回收系统中断的时间
25、PauseEnd [256]uint64 //一个循环队列，记录最近垃圾回收系统中断的时间开始点。
26、NumForcedGC uint32 //服务调用runtime.GC()强制使用垃圾回收的次数。
27、GCCPUFraction float64 //垃圾回收占用服务CPU工作的时间总和。如果有100个goroutine，垃圾回收的时间为1S,那么久占用了100S。
28、BySize //内存分配器使用情况
以上是我个人对runtime.MemStats各个属性的理解，有理解错的地方还请发表评论或者联系下我共同探讨一下。

从runtime.MemStats的属性可以看到，golang中的runtime包其实是一个带有一点维护性质的功能包。开发者可以获取大量golang服务运行时的信息，查看runtime.MenStats的方法也很简单。直接调用runtime.ReadMemStats方法即可获取调用点服务运行信息。一下是我测试过程中获取到的runtime.MenStats，内容如下：

memstat: {
Alloc:69257680 //golang语言框架堆空间分配的字节数 大概68M
TotalAlloc:79489528 //从服务开始运行至今分配器为分配的堆空间总 和，只有增加，释放的时候不减少。大约79M
Sys:1345724664 //服务现在系统使用的内存, 大约1345M
Lookups:3 //被runtime监视的指针数
Mallocs:307494 //服务malloc的次数
Frees:9105  //服务回收的heap objects的字节数 free次数
HeapAlloc:69257680 //golang语言框架堆空间分配的字节数 大概68M
HeapSys:71434240  //系统分配的堆内存 大概71M
HeapIdle:974848 //申请但是未分配的堆内存或者回收了的堆内存（空闲）字节数 大概1M
HeapInuse:70459392 //正在使用的堆内存字节数 大概70M
HeapReleased:0 //返回给OS的堆内存，
HeapObjects:298389  //堆内存块申请的量
StackInuse:1220804608 //正在使用的栈字节数 约1220M
StackSys:1220804608 //系统分配的作为运行栈的内存 约1220M
MSpanInuse:6924360  //用于测试用的结构体使用的字节数 不受GC控制， 约7M
MSpanSys:6979584 //系统为测试用的结构体分配的字节数 约7M
MCacheInuse:6816 //mcache结构体申请的字节数(不会被视为垃圾回收) 约6K
MCacheSys:16384 //操作系统申请的堆空间用于mcache的字节数,约16K
BuckHashSys:1468496 //用于剖析桶散列表的堆空间 约14K
GCSys:40984576 //垃圾回收标记元信息使用的内存 约40M
OtherSys:4036776 //golang系统架构占用的额外空间 约4M
NextGC:135394784  //垃圾回收器检视的内存大小 约135M
LastGC:1506577064496115700 //最后一次GC的时间戳
PauseTotalNs:1834800 //系统暂停的时间，大约1.8毫秒
PauseNs:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 833700 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 131600 0 69500 299900 500100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] //最近垃圾回收消耗情况
PauseEnd:[1506576868237062600 1506576873250702600 1506576878266318900 1506576883290562700 1506576888313706000 1506576893339216000 1506576898380539200 1506576903430807600 1506576908483751100 1506576913540053700 1506576918589605600 1506576923651466900 1506576928716329900 1506576933785270400 1506576938872682700 1506576943987556700 1506576949080171300 1506576954205844600 1506576959319027700 1506576964454667000 1506576969604832300 1506576974795338100 1506576979945880300 1506576985117374500 1506576990330379400 1506576995548568900 1506577000766977100 1506577005980367800 1506577011190038400 1506577016427160200 1506577021671897800 1506577026958806600 1506577032285905300 1506577037561334600 1506577042926588300 1506577048190473200 1506577053579177600 1506577059147393600 1506577064496115700 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] //垃圾回收调用的时间点
NumGC:39 //垃圾回收调用次数
NumForcedGC:39 
GCCPUFraction:-1.325626798158314e-06 //调用GC消耗的性能
EnableGC:true 
DebugGC:false 
BySize:[
{Size:0 Mallocs:0 Frees:0} 
{Size:8 Mallocs:45 Frees:41} 
{Size:16 Mallocs:4316 Frees:4273} 
{Size:32 Mallocs:118 Frees:58} 
{Size:48 Mallocs:192 Frees:1} 
{Size:64 Mallocs:149078 Frees:52} 
{Size:80 Mallocs:6 Frees:1} 
{Size:96 Mallocs:4 Frees:0} 
{Size:112 Mallocs:1 Frees:1} 
{Size:128 Mallocs:41 Frees:41} 
{Size:144 Mallocs:0 Frees:0} 
{Size:160 Mallocs:1 Frees:1} 
{Size:176 Mallocs:0 Frees:0} 
{Size:192 Mallocs:50 Frees:50} 
{Size:208 Mallocs:2 Frees:0} 
{Size:224 Mallocs:0 Frees:0} 
{Size:240 Mallocs:0 Frees:0} 
{Size:256 Mallocs:48 Frees:41} 
{Size:288 Mallocs:0 Frees:0} 
{Size:320 Mallocs:1 Frees:0} 
{Size:352 Mallocs:0 Frees:0} 
{Size:384 Mallocs:149024 Frees:0} 
{Size:416 Mallocs:1 Frees:0} 
{Size:448 Mallocs:0 Frees:0} 
{Size:480 Mallocs:3 Frees:0} 
{Size:512 Mallocs:80 Frees:80} 
{Size:576 Mallocs:0 Frees:0} 
{Size:640 Mallocs:0 Frees:0} 
{Size:704 Mallocs:0 Frees:0} 
{Size:768 Mallocs:0 Frees:0} 
{Size:896 Mallocs:7 Frees:0} 
{Size:1024 Mallocs:41 Frees:41} 
{Size:1152 Mallocs:1 Frees:0} 
{Size:1280 Mallocs:8 Frees:8} 
{Size:1408 Mallocs:0 Frees:0} 
{Size:1536 Mallocs:0 Frees:0} 
{Size:1792 Mallocs:10 Frees:8} 
{Size:2048 Mallocs:33 Frees:33} 
{Size:2304 Mallocs:8 Frees:8} 
{Size:2688 Mallocs:31 Frees:31} 
{Size:3072 Mallocs:8 Frees:8} 
{Size:3200 Mallocs:0 Frees:0} 
{Size:3456 Mallocs:31 Frees:31} 
{Size:4096 Mallocs:10 Frees:10} 
{Size:4864 Mallocs:35 Frees:31} 
{Size:5376 Mallocs:0 Frees:0} 
{Size:6144 Mallocs:39 Frees:39} 
{Size:6528 Mallocs:0 Frees:0} 
{Size:6784 Mallocs:0 Frees:0} 
{Size:6912 Mallocs:0 Frees:0} 
{Size:8192 Mallocs:3 Frees:2} 
{Size:9472 Mallocs:0 Frees:0} 
{Size:9728 Mallocs:0 Frees:0} 
{Size:10240 Mallocs:2 Frees:2} 
{Size:10880 Mallocs:0 Frees:0} 
{Size:12288 Mallocs:0 Frees:0} 
{Size:13568 Mallocs:2 Frees:2} 
{Size:14336 Mallocs:1 Frees:0} 
{Size:16384 Mallocs:0 Frees:0} 
{Size:18432 Mallocs:2 Frees:2} 
{Size:19072 Mallocs:0 Frees:0}
] //具体内存分配情况 
}
在windows下我们能看到的只有cpu占用率和内存使用的大小，其截图如下：


从上面一个实际的例子可以看出使用golang的runtime.MemStats确实是可以获取大量golang服务的运行信息。但是我这边没有测试调用CGO的情况，后续测试一下然后补充一下。测试结果发现runtime.MemStats.Sys是准确的，而且还能获取其他具体的内存分配及垃圾回收相关等信息。从获取的信息来看，服务使用的堆空间很少，但是使用到的栈确高达1220M。该服务是这样的，一直创建goroutine，然后让goroutine无限循环睡眠1秒钟。再获取到runtime.MemStats信息附近发现有获取当前goroutine数量的信息。日志打印内容如下：

goroutine number is:  150001
1
计算一下：150000 * 8K = 1200M。这个和runtime.MemStats获取到的栈大小非常接近。
从上面介绍可以看到，golang服务可以通过runtime.ReadMemStats方法获取服务运行期间内存使用情况和垃圾回收等相关信息，比起各种内存监控工具要详细很多，而且是以golang的方式获取内存数据。因为在golang中，我们只获取内存使用总量和增长趋势往往可以确定的事情很少，但是由runtime.MemStats获取到的信息确是非常有价值的。在服务后台开发过程中，有一个非常重要的话就是“无监控，不服务”，这是服务后台开发的基本。无论是架构的演进方向和服务可靠性控制，以及部分性能优化的数据来源都需要以监控数据作为参考。而golang内存信息的监控个人觉得必须通过对应时刻runtime.MemStats的为标准。当然使用runtime.ReadMemStats会短暂的暂停服务中的所有goroutine，然后收集调用时刻的MemStats。从源代码来看，暂停所有goroutine的时间仅仅是使用memcopy拷贝一个MemStats的时间。代码如下：

func ReadMemStats(m *MemStats) {
    stopTheWorld("read mem stats")

    systemstack(func() {
        readmemstats_m(m)
    })

    startTheWorld()
}

func readmemstats_m(stats *MemStats) {
    updatememstats()

    // The size of the trailing by_size array differs between
    // mstats and MemStats. NumSizeClasses was changed, but we
    // cannot change MemStats because of backward compatibility.
    memmove(unsafe.Pointer(stats), unsafe.Pointer(&memstats), sizeof_C_MStats)

    // memstats.stacks_sys is only memory mapped directly for OS stacks.
    // Add in heap-allocated stack memory for user consumption.
    stats.StackSys += stats.StackInuse
}
从GO1.9源代码和实际生产环境测试情况看，个人的观点是，即使在业务高峰时期，不应该将runtime.ReadMemStats作为性能优化的优化点。因为在在业务高峰时段获取到的服务状态信息往往是最有用的

https://prometheus.io/

如何利用golang自带的profile工具进行应用程序的性能调优，前一段时间我做的日志分析系统在线上遇到了一个问题，就是分任务的系统down机了，日志处理延迟了10几个小时，这个时候任务分发系统重启之后开始分发任务，但是一下子就承受了十几个并发任务，导致内存消耗过快，直接吃掉了16G的内存，这可急坏了我啊。所以赶紧开始做性能优化。

性能优化我主要从以下几个方面进行了测试和调优：

CPU Profiling
Mem Profiling
GC & HEAP
我采用了如下的profile工具代码：

package main

 

import (

"fmt"

"log"

"os"

"runtime"

"runtime/debug"

"runtime/pprof"

"strconv"

"sync/atomic"

"syscall"

"time"

)

 

var heapProfileCounter int32

var startTime = time.Now()

var pid int

 

func init() {

pid = os.Getpid()

}

 

func StartCPUProfile() {

f, err := os.Create("cpu-" + strconv.Itoa(pid) + ".pprof")

if err != nil {

log.Fatal(err)

}

pprof.StartCPUProfile(f)

}

 

func StopCPUProfile() {

pprof.StopCPUProfile()

}

 

func StartBlockProfile(rate int) {

runtime.SetBlockProfileRate(rate)

}

 

func StopBlockProfile() {

filename := "block-" + strconv.Itoa(pid) + ".pprof"

f, err := os.Create(filename)

if err != nil {

log.Fatal(err)

}

if err = pprof.Lookup("block").WriteTo(f, 0); err != nil {

log.Fatalf(" can't write %s: %s", filename, err)

}

f.Close()

}

 

func SetMemProfileRate(rate int) {

runtime.MemProfileRate = rate

}

 

func GC() {

runtime.GC()

}

 

func DumpHeap() {

filename := "heap-" + strconv.Itoa(pid) + "-" + strconv.Itoa(int(atomic.AddInt32(&heapProfileCounter, 1))) + ".pprof"

f, err := os.Create(filename)

if err != nil {

fmt.Fprintf(os.Stderr, "testing: %s", err)

return

}

if err = pprof.WriteHeapProfile(f); err != nil {

fmt.Fprintf(os.Stderr, "testing: can't write %s: %s", filename, err)

}

f.Close()

}

 

func showSystemStat(interval time.Duration, count int) {

 

usage1 := &syscall.Rusage{}

var lastUtime int64

var lastStime int64

 

counter := 0

for {

 

//http://man7.org/linux/man-pages/man3/vtimes.3.html

syscall.Getrusage(syscall.RUSAGE_SELF, usage1)

 

utime := usage1.Utime.Sec*1000000000 + usage1.Utime.Usec

stime := usage1.Stime.Sec*1000000000 + usage1.Stime.Usec

userCPUUtil := float64(utime-lastUtime) * 100 / float64(interval)

sysCPUUtil := float64(stime-lastStime) * 100 / float64(interval)

memUtil := usage1.Maxrss * 1024

 

lastUtime = utime

lastStime = stime

 

if counter > 0 {

fmt.Printf("cpu: %3.2f%% us  %3.2f%% sy, mem:%s \n", userCPUUtil, sysCPUUtil, toH(uint64(memUtil)))

}

 

counter += 1

if count >= 1 && count < counter {

return

}

time.Sleep(interval)

}

 

}

 

func ShowSystemStat(seconds int) {

go func() {

interval := time.Duration(seconds) * time.Second

showSystemStat(interval, 0)

}()

}

 

func PrintSystemStats() {

interval := time.Duration(1) * time.Second

showSystemStat(interval, 1)

}

 

func ShowGCStat() {

go func() {

var numGC int64

 

interval := time.Duration(100) * time.Millisecond

gcstats := &debug.GCStats{PauseQuantiles: make([]time.Duration, 100)}

memStats := &runtime.MemStats{}

for {

debug.ReadGCStats(gcstats)

if gcstats.NumGC > numGC {

runtime.ReadMemStats(memStats)

 

printGC(memStats, gcstats)

numGC = gcstats.NumGC

}

time.Sleep(interval)

}

}()

}

 

func PrintGCSummary() {

memStats := &runtime.MemStats{}

runtime.ReadMemStats(memStats)

gcstats := &debug.GCStats{PauseQuantiles: make([]time.Duration, 100)}

debug.ReadGCStats(gcstats)

 

printGC(memStats, gcstats)

}

 

func printGC(memStats *runtime.MemStats, gcstats *debug.GCStats) {

 

if gcstats.NumGC > 0 {

lastPause := gcstats.Pause[0]

elapsed := time.Now().Sub(startTime)

overhead := float64(gcstats.PauseTotal) / float64(elapsed) * 100

allocatedRate := float64(memStats.TotalAlloc) / elapsed.Seconds()

 

fmt.Printf("NumGC:%d Pause:%s Pause(Avg):%s Overhead:%3.2f%% Alloc:%s Sys:%s Alloc(Rate):%s/s Histogram:%s %s %s \n",

gcstats.NumGC,

toS(lastPause),

toS(avg(gcstats.Pause)),

overhead,

toH(memStats.Alloc),

toH(memStats.Sys),

toH(uint64(allocatedRate)),

toS(gcstats.PauseQuantiles[94]),

toS(gcstats.PauseQuantiles[98]),

toS(gcstats.PauseQuantiles[99]))

} else {

// while GC has disabled

elapsed := time.Now().Sub(startTime)

allocatedRate := float64(memStats.TotalAlloc) / elapsed.Seconds()

 

fmt.Printf("Alloc:%s Sys:%s Alloc(Rate):%s/s\n",

toH(memStats.Alloc),

toH(memStats.Sys),

toH(uint64(allocatedRate)))

}

}

 

func avg(items []time.Duration) time.Duration {

var sum time.Duration

for _, item := range items {

sum += item

}

return time.Duration(int64(sum) / int64(len(items)))

}

 

// human readable format

func toH(bytes uint64) string {

switch {

case bytes < 1024:

return fmt.Sprintf("�", bytes)

case bytes < 1024*1024:

return fmt.Sprintf("%.2fK", float64(bytes)/1024)

case bytes < 1024*1024*1024:

return fmt.Sprintf("%.2fM", float64(bytes)/1024/1024)

default:

return fmt.Sprintf("%.2fG", float64(bytes)/1024/1024/1024)

}

}

 

// short string format

func toS(d time.Duration) string {

 

u := uint64(d)

if u < uint64(time.Second) {

switch {

case u == 0:

return "0"

case u < uint64(time.Microsecond):

return fmt.Sprintf("%.2fns", float64(u))

case u < uint64(time.Millisecond):

return fmt.Sprintf("%.2fus", float64(u)/1000)

default:

return fmt.Sprintf("%.2fms", float64(u)/1000/1000)

}

} else {

switch {

case u < uint64(time.Minute):

return fmt.Sprintf("%.2fs", float64(u)/1000/1000/1000)

case u < uint64(time.Hour):

return fmt.Sprintf("%.2fm", float64(u)/1000/1000/1000/60)

default:

return fmt.Sprintf("%.2fh", float64(u)/1000/1000/1000/60/60)

}

}
}



runtime 包 提供了运行时与系统的交互,比如控制协程函数，触发垃圾立即回收等等底层操作,下面我们就运行时能做的所有事情逐个进行说明与代码演示

1.获取GOROOT环境变量
2.获取GO的版本号
3.获取本机CPU个数
4.设置最大可同时执行的最大CPU数
5.设置cup profile 记录的速录
6.查看cup profile 下一次堆栈跟踪数据
7.立即执行一次垃圾回收
8.给变量绑定方法,当垃圾回收的时候进行监听
9.查看内存申请和分配统计信息
10.查看程序正在使用的字节数
11.查看程序正在使用的对象数
12.获取调用堆栈列表
13.获取内存profile记录历史
14.执行一个断点
15.获取程序调用go协程的栈踪迹历史
16.获取当前函数或者上层函数的标识号、文件名、调用方法在当前文件中的行号
17.获取与当前堆栈记录相关链的调用栈踪迹
18.获取一个标识调用栈标识符pc对应的调用栈
19.获取调用栈所调用的函数的名字
20.获取调用栈所调用的函数的所在的源文件名和行号
21.获取该调用栈的调用栈标识符
22.获取当前进程执行的cgo调用次数
23.获取当前存在的go协程数
24.终止掉当前的go协程
25.让其他go协程优先执行,等其他协程执行完后,在执行当前的协程
26.获取活跃的go协程的堆栈profile以及记录个数
27.将调用的go协程绑定到当前所在的操作系统线程，其它go协程不能进入该线程
28.解除go协程与操作系统线程的绑定关系
29.获取线程创建profile中的记录个数
30.控制阻塞profile记录go协程阻塞事件的采样率
31.返回当前阻塞profile中的记录个数
1.获取GOROOT环境变量
func GOROOT() string
GOROOT返回Go的根目录。如果存在GOROOT环境变量，返回该变量的值；否则，返回创建Go时的根目录

package main
import (
    "fmt"
  "runtime"
)
func main() {
fmt.Println(runtime.GOROOT())
}
image.png
2.获取GO的版本号
func Version() string
返回Go的版本字符串。它要么是递交的hash和创建时的日期；要么是发行标签如"go1.3"

package main

import (
    "fmt"
  "runtime"
)

func main() {
fmt.Println(runtime.Version())
}
image.png
3.获取本机CPU个数
func NumCPU() int
NumCPU返回本地机器的逻辑CPU个数

package main

import (
  "fmt"
  "runtime"
)

func main() {
fmt.Println(runtime.NumCPU())
}
image.png
4.设置最大可同时执行的最大CPU数
func GOMAXPROCS(n int) int

GOMAXPROCS设置可同时执行的最大CPU数，并返回先前的设置。 若 n < 1，它就不会更改当前设置。本地机器的逻辑CPU数可通过 NumCPU 查询。本函数在调度程序优化后会去掉

使用默认的cup数量 我的电脑是4核的

package main

import (
  "fmt"
  "time"
  )
func main() {

   //runtime.GOMAXPROCS(1)
    startTime := time.Now()
    var s1 chan  int64 = make(chan int64)
    var s2 chan  int64 = make(chan int64)
    var s3 chan  int64 = make(chan int64)
    var s4 chan  int64 = make(chan int64)
    go calc(s1)
    go calc(s2)
    go calc(s3)
    go calc(s4)
    <-s1
    <-s2
    <-s3
    <-s4
    endTime := time.Now()
    fmt.Println(endTime.Sub(startTime))
    
}
func calc(s  chan int64) {
   var count int64 = 0
  for i := 0 ;i < 1000000000;i++ {
    count += int64(i)
  }
  s <- count
}
image.png
下面我们将cup数量设置成1

package main
import (
  "fmt"
  "time"
  "runtime"
)

func main() {
   runtime.GOMAXPROCS(1)
    startTime := time.Now()
    var s1 chan  int64 = make(chan int64)
    var s2 chan  int64 = make(chan int64)
    var s3 chan  int64 = make(chan int64)
    var s4 chan  int64 = make(chan int64)
    go calc(s1)
    go calc(s2)
    go calc(s3)
    go calc(s4)
    <-s1
    <-s2
    <-s3
    <-s4
    endTime := time.Now()
    fmt.Println(endTime.Sub(startTime))

}
func calc(s  chan int64) {
   var count int64 = 0
  for i := 0 ;i < 1000000000;i++ {
    count += int64(i)
  }
  s <- count
}
image.png

很明显速度慢了很多

5.设置cup profile 记录的速录
func SetCPUProfileRate(hz int)
SetCPUProfileRate设置CPU profile记录的速率为平均每秒hz次。如果hz<=0，SetCPUProfileRate会关闭profile的记录。如果记录器在执行，该速率必须在关闭之后才能修改。

绝大多数使用者应使用runtime/pprof包或testing包的-test.cpuprofile选项而非直接使用SetCPUProfileRate

6.查看cup profile 下一次堆栈跟踪数据
func CPUProfile() []byte
目前已废弃

7.立即执行一次垃圾回收
func GC()
GC执行一次垃圾回收

看一下代码

package main

import (
   "runtime"
  "time"
)
type Student struct {
  name string

}
func main() {
  var i *Student = new(Student)
  runtime.SetFinalizer(i, func(i interface{}) {
   println("垃圾回收了")
  })
  runtime.GC()
  time.Sleep(time.Second)
}
image.png
我们创建了一个指针类型的变量Student 当我们调用runtime.GC的时候,内存立即会回收，你可以把runtime.GC()屏蔽掉,程序就不在执行了

8.给变量绑定方法,当垃圾回收的时候进行监听
func SetFinalizer(x, f interface{})

注意x必须是指针类型,f 函数的参数一定要和x保持一致,或者写interface{},不然程序会报错

示例如下

package main

import (
   "runtime"
  "time"
)
type Student struct {
  name string

}
func main() {
  var i *Student = new(Student)
  runtime.SetFinalizer(i, func(i *Student) {
   println("垃圾回收了")
  })
  runtime.GC()
  time.Sleep(time.Second)
}
image.png
9.查看内存申请和分配统计信息
func ReadMemStats(m *MemStats)
我们可以获得下面的信息

type MemStats struct {
    // 一般统计
    Alloc      uint64 // 已申请且仍在使用的字节数
    TotalAlloc uint64 // 已申请的总字节数（已释放的部分也算在内）
    Sys        uint64 // 从系统中获取的字节数（下面XxxSys之和）
    Lookups    uint64 // 指针查找的次数
    Mallocs    uint64 // 申请内存的次数
    Frees      uint64 // 释放内存的次数
    // 主分配堆统计
    HeapAlloc    uint64 // 已申请且仍在使用的字节数
    HeapSys      uint64 // 从系统中获取的字节数
    HeapIdle     uint64 // 闲置span中的字节数
    HeapInuse    uint64 // 非闲置span中的字节数
    HeapReleased uint64 // 释放到系统的字节数
    HeapObjects  uint64 // 已分配对象的总个数
    // L低层次、大小固定的结构体分配器统计，Inuse为正在使用的字节数，Sys为从系统获取的字节数
    StackInuse  uint64 // 引导程序的堆栈
    StackSys    uint64
    MSpanInuse  uint64 // mspan结构体
    MSpanSys    uint64
    MCacheInuse uint64 // mcache结构体
    MCacheSys   uint64
    BuckHashSys uint64 // profile桶散列表
    GCSys       uint64 // GC元数据
    OtherSys    uint64 // 其他系统申请
    // 垃圾收集器统计
    NextGC       uint64 // 会在HeapAlloc字段到达该值（字节数）时运行下次GC
    LastGC       uint64 // 上次运行的绝对时间（纳秒）
    PauseTotalNs uint64
    PauseNs      [256]uint64 // 近期GC暂停时间的循环缓冲，最近一次在[(NumGC+255)%256]
    NumGC        uint32
    EnableGC     bool
    DebugGC      bool
    // 每次申请的字节数的统计，61是C代码中的尺寸分级数
    BySize [61]struct {
        Size    uint32
        Mallocs uint64
        Frees   uint64
    }
}
package main

import (
   "runtime"
  "time"
  "fmt"
)
type Student struct {
  name string

}
func main() {
  var list = make([]*Student,0)
  for i:=0;i <100000 ;i++ {
    var s *Student = new(Student)
    list = append(list, s)
  }
  memStatus := runtime.MemStats{}
  runtime.ReadMemStats(&memStatus)
  fmt.Printf("申请的内存:%d\n",memStatus.Mallocs)
  fmt.Printf("释放的内存次数:%d\n",memStatus.Frees)
  time.Sleep(time.Second)
}

image.png
10.查看程序正在使用的字节数
func (r *MemProfileRecord) InUseBytes() int64
InUseBytes返回正在使用的字节数（AllocBytes – FreeBytes）

11.查看程序正在使用的对象数
func (r *MemProfileRecord) InUseObjects() int64
InUseObjects返回正在使用的对象数（AllocObjects - FreeObjects）

12.获取调用堆栈列表
func (r *MemProfileRecord) Stack() []uintptr
Stack返回关联至此记录的调用栈踪迹，即r.Stack0的前缀。

13.获取内存profile记录历史
func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool)
MemProfile返回当前内存profile中的记录数n。若len(p)>=n，MemProfile会将此分析报告复制到p中并返回(n, true)；如果len(p)<n，MemProfile则不会更改p，而只返回(n, false)。

如果inuseZero为真，该profile就会包含无效分配记录（其中r.AllocBytes>0，而r.AllocBytes==r.FreeBytes。这些内存都是被申请后又释放回运行时环境的）。

大多数调用者应当使用runtime/pprof包或testing包的-test.memprofile标记，而非直接调用MemProfile

14.执行一个断点
func Breakpoint()

runtime.Breakpoint()
image.png
15.获取程序调用go协程的栈踪迹历史
func Stack(buf []byte, all bool) int
Stack将调用其的go程的调用栈踪迹格式化后写入到buf中并返回写入的字节数。若all为true，函数会在写入当前go程的踪迹信息后，将其它所有go程的调用栈踪迹都格式化写入到buf中。

package main

import (
     "time"
  "runtime"
  "fmt"
)

func main() {
  go showRecord()
  time.Sleep(time.Second)
  buf := make([]byte,10000)
  runtime.Stack(buf,true)
  fmt.Println(string(buf))
}

func showRecord(){
 tiker := time.Tick(time.Second)
 for t := range  tiker {
   fmt.Println(t)
 }
}
image.png
我们在调用Stack方法后,首先格式化当前go协程的信息，然后把其他正在运行的go协程也格式化后写入buf中

16.获取当前函数或者上层函数的标识号、文件名、调用方法在当前文件中的行号
func Caller(skip int) (pc uintptr, file string, line int, ok bool)

package main
import (
  "runtime"
  "fmt"
)

func main() {
  pc,file,line,ok := runtime.Caller(0)
  fmt.Println(pc)
  fmt.Println(file)
  fmt.Println(line)
  fmt.Println(ok)
}
image.png
pc = 17380971 不是main函数自己的标识 runtime.Caller 方法的标识,line = 13 标识它在main方法中的第13行被调用

package main

import (
  "runtime"
  "fmt"
)

func main() {
  pc,_,line,_ := runtime.Caller(1)
  fmt.Printf("main函数的pc:%d\n",pc)
  fmt.Printf("main函数被调用的行数:%d\n",line)
  show()
}
func show(){
  pc,_,line,_ := runtime.Caller(1)
  fmt.Printf("show函数的pc:%d\n",pc)
  fmt.Printf("show函数被调用的行数:%d\n",line)
  // 这个是main函数的栈
  pc,_,line,_ = runtime.Caller(2)
  fmt.Printf("show的上层函数的pc:%d\n",pc)
  fmt.Printf("show的上层函数被调用的行数:%d\n",line)
  pc,_,_,_ = runtime.Caller(3)
  fmt.Println(pc)
  pc,_,_,_ = runtime.Caller(4)
  fmt.Println(pc)
}
image.png

通过上面的例子我演示了如何追踪一个方法被调用的顺序,以及所有相关函数的信息

17.获取与当前堆栈记录相关链的调用栈踪迹
func Callers(skip int, pc []uintptr) int

函数把当前go程调用栈上的调用栈标识符填入切片pc中，返回写入到pc中的项数。实参skip为开始在pc中记录之前所要跳过的栈帧数，0表示Callers自身的调用栈，1表示Callers所在的调用栈。返回写入p的项数

package main

import (
  "runtime"
  "fmt"
)


func main() {
  pcs := make([]uintptr,10)
  i := runtime.Callers(1,pcs)
  fmt.Println(pcs[:i])
}
image.png

我们获得了三个pc 其中有一个是main方法自身的

18.获取一个标识调用栈标识符pc对应的调用栈
func FuncForPC(pc uintptr) *Func

package main

import (
  "runtime"
  )


func main() {

  pcs := make([]uintptr,10)
  i := runtime.Callers(1,pcs)
  for _,pc := range pcs[:i]{
    println(runtime.FuncForPC(pc))
  }
}
image.png
我们知道这个调用栈有什么用呢?请继续下想看

19.获取调用栈所调用的函数的名字
func (f *Func) Name() string

package main

import (
  "runtime"
  )



func main() {

  pcs := make([]uintptr,10)
  i := runtime.Callers(1,pcs)
  for _,pc := range pcs[:i]{
    funcPC := runtime.FuncForPC(pc)
    println(funcPC.Name())
  }
}
image.png
20.获取调用栈所调用的函数的所在的源文件名和行号
func (f *Func) FileLine(pc uintptr) (file string, line int)

package main
import (
  "runtime"
  )

func main() {
  pcs := make([]uintptr,10)
  i := runtime.Callers(1,pcs)
  for _,pc := range pcs[:i]{
    funcPC := runtime.FuncForPC(pc)
    file,line := funcPC.FileLine(pc)
    println(funcPC.Name(),file,line)
  }
}
image.png
21.获取该调用栈的调用栈标识符
func (f *Func) Entry() uintptr

package main

import (
  "runtime"
  )



func main() {
  pcs := make([]uintptr,10)
  i := runtime.Callers(1,pcs)
  for _,pc := range pcs[:i]{
    funcPC := runtime.FuncForPC(pc)
    println(funcPC.Entry())
  }
}
image.png
22.获取当前进程执行的cgo调用次数
func NumCgoCall() int64
获取当前进程调用c方法的次数
`

package main

import (
  "runtime"
  )
/*
#include <stdio.h>
*/
import "C"




func main() {
 println(runtime.NumCgoCall())
}
image.png

注意我们没有调用c的方法为什么是1呢？因为import c是,会调用了c包中的init方法
下面我们看一个完整例子

import (
  "runtime"
  )
/*
#include <stdio.h>
// 自定义一个c语言的方法
static void myPrint(const char* msg) {
  printf("myPrint: %s", msg);
}
*/
import "C"




func main() {
  // 调用c方法
  C.myPrint(C.CString("Hello,C\n"))
  println(runtime.NumCgoCall())
}
image.png
23.获取当前存在的go协程数
func NumGoroutine() int

package main

import "runtime"



func main() {
 go print()
 print()
 println(runtime.NumGoroutine())
}
func print(){

}
image.png
我们可以看到输出的是2 表示存在2个go协程 一个是main.go 另外一个是go print()

24.终止掉当前的go协程
func Goexit()

package main
import (
  "runtime"
    "fmt"
)

func main() {
 print()  // 1
 fmt.Println("继续执行")
}
func print(){
  fmt.Println("准备结束go协程")
  runtime.Goexit()
  defer fmt.Println("结束了")
}
image.png
Goexit终止调用它的go协程,其他协程不受影响,Goexit会在终止该go协程前执行所有的defer函数，前提是defer必须在它前面定义,如果在main go协程调用本方法,会终止该go协程,但不会让main返回,因为main函数没有返回,程序会继续执行其他go协程,当其他go协程执行完毕后,程序就会崩溃

25.让其他go协程优先执行,等其他协程执行完后,在执行当前的协程
func Gosched()

我们先看一个示例

package main
import (
  "fmt"
  )

func main() {
  go print()  // 1
  fmt.Println("继续执行")
}
func print(){
  fmt.Println("执行打印方法")
}
image.png
我们在1处调用了go print方法,但是还未执行 main函数就执行完毕了,因为两个协程是并发的

那么我们应该怎么才能让每个协程都能够执行完毕呢?方法有很多种,不过就针对这个知识点,我们就使用 runtime.Gosched()来解决

package main
import (
  "fmt"
  "runtime"
)

func main() {
  go print()  // 1
  runtime.Gosched()
  fmt.Println("继续执行")
}
func print(){
  fmt.Println("执行打印方法")
}
image.png
26.获取活跃的go协程的堆栈profile以及记录个数
func GoroutineProfile(p []StackRecord) (n int, ok bool)


27.将调用的go协程绑定到当前所在的操作系统线程，其它go协程不能进入该线程
func LockOSThread()
将调用的go程绑定到它当前所在的操作系统线程。除非调用的go程退出或调用UnlockOSThread，否则它将总是在该线程中执行，而其它go程则不能进入该线程

我们看下面一个例子

package main
import (
  "fmt"
  "runtime"
  "time"
)

func main() {
  go calcSum1()
  go calcSum2()
  time.Sleep(time.Second*100)
}

func calcSum1(){
  runtime.LockOSThread()
  start := time.Now()
  count := 0
  for i := 0; i < 10000000000 ; i++  {
    count += i
  }
  end := time.Now()
  fmt.Println("calcSum1耗时")
  fmt.Println(end.Sub(start))
  defer runtime.UnlockOSThread()
}

func calcSum2(){
  start := time.Now()
  count := 0
  for i := 0; i < 10000000000 ; i++  {
    count += i
  }
  end := time.Now()
  fmt.Println("calcSum2耗时")
  fmt.Println(end.Sub(start))
}
image.png
测试速度没有多大的差别,如果有需要协程,但是有一项重要的功能需要占一个核，就需要

28.解除go协程与操作系统线程的绑定关系
func UnlockOSThread()
将调用的go程解除和它绑定的操作系统线程。若调用的go程未调用LockOSThread，UnlockOSThread不做操作

29.获取线程创建profile中的记录个数
func ThreadCreateProfile(p []StackRecord) (n int, ok bool)
返回线程创建profile中的记录个数。如果len(p)>=n，本函数就会将profile中的记录复制到p中并返回(n, true)。若len(p)<n，则不会更改p，而只返回(n, false)。

绝大多数使用者应当使用runtime/pprof包，而非直接调用ThreadCreateProfile。

30.控制阻塞profile记录go协程阻塞事件的采样率
func SetBlockProfileRate(rate int)

SetBlockProfileRate控制阻塞profile记录go程阻塞事件的采样频率。对于一个阻塞事件，平均每阻塞rate纳秒，阻塞profile记录器就采集一份样本。

要在profile中包括每一个阻塞事件，需传入rate=1；要完全关闭阻塞profile的记录，需传入rate<=0。

31.返回当前阻塞profile中的记录个数
func BlockProfile(p []BlockProfileRecord) (n int, ok bool)

BlockProfile返回当前阻塞profile中的记录个数。如果len(p)>=n，本函数就会将此profile中的记录复制到p中并返回(n, true)。如果len(p)<n，本函数则不会修改p，而只返回(n, false)。

绝大多数使用者应当使用runtime/pprof包或testing包的-test.blockprofile标记， 而非直接调用 BlockProfile
