---
title: lucence  elasticsearch
layout: post
category: elasticsearch
author: 夏泽民
---
lucene，最先进、功能最强大的搜索库，直接基于lucene开发，非常复杂，api复杂（实现一些简单的功能，写大量的java代码），需要深入理解原理（各种索引结构）
elasticsearch，基于lucene，隐藏复杂性，提供简单易用的restful api接口、java api接口（还有其他语言的api接口）
（1）分布式的文档存储引擎
（2）分布式的搜索引擎和分析引擎
（3）分布式，支持PB级数据
开箱即用，优秀的默认参数，不需要任何额外设置，完全开源
<!-- more -->
Lucene概述

Lucene是一款高性能的、可扩展的信息检索（IR）工具库。信息检索是指文档搜索、文档内信息搜索或者文档相关的元数据搜索等操作。

索引过程：

①获取内容

②建立文档 
获取原始内容后，就需要对这些内容进行索引，必须首先将这些内容转换成部件（通常称为文档），以供搜索引擎使用。文档主要包括几个带值的域，比如标题、正文、摘要、作者和链接。

③文档分析 
搜索引擎不能直接对文本进行索引：确切地说，必须将文本分割成一系列被称为语汇单元的独立的原子元素。每一个语汇单元大致与语言中的“单词”对应起来。

④文档索引 
在索引步骤中，文档被加入到索引列表。

搜索组件

搜索处理过程就是从索引中查找单词，从而找到包含该单词的文档。搜索质量主要由查准率和查全率来衡量。查全率用来衡量搜索系统查找相关文档的能力；而查准率用来衡量搜索系统过滤非相关文档的能力。 
①用户搜索界面 
Lucene不提供默认的用户搜索界面，需要自己开发。 
②建立查询 
用户从搜索界面提交一个搜索请求，通常以HTML表单或者Ajax请求的形式由浏览器提交到你的搜索引擎服务器。然后将这个请求转换成搜索引擎使用的查询对象格式，这称为建立查询。 
③搜索查询 
查询检索索引并返回与查询语句匹配的文档，结果返回时按照查询请求来排序。 
④展现结果 
一旦获得匹配查询语句并排好序的文档结果集，接下来就得用直观的、经济的方式为用户展现结果。

索引过程的核心类

·IndexWriter 
·Directory 
·Analyzer 
·Document 
·Field

①IndexWriter 
索引过程的核心组件。这个类负责创建新索引或者打开已有索引，以及向索引中添加、删除或更新被索引文档的信息。可以把IndexWriter看作这样一个对象：它为你提供针对索引文件的写入操作，但不能用于读取或搜索索引。IndexWriter需要开辟一定空间来存储索引，该功能可以由Directory完成。

②Directory 
该类描述了Lucene索引的存放位置。它是一个抽象类，它的子类负责具体指定索引的存储路径。用FSDirectory.open方法来获取真实文件在文件系统的存储路径，然后将它们一次传递给IndexWriter类构造方法。IndexWriter不能直接索引文本，这需要先由Analyzer将文本分割成独立的单词才行。

③Analyzer 
文本文件在被索引之前，需要经过Analyzer（分析器）处理。Analyzer是由IndexWriter的构造方法来指定的，它负责从被索引文本文件中提取语汇单元，并提出剩下的无用信息。如果被索引内容不是纯文本文件，那就需要先将其转换为文本文档。对于要将Lucene集成到应用程序的开发人员来说，选择什么样Analyzer是程序设计中非常关键的一步。分析器的分析对象为文档，该文档包含一些分离的能被索引的域。

④Document 
Document对象代表一些域（Field）的集合。文档的域代表文档或者文档相关的一些元数据。元数据（如作者、标题、主题和修改日期等）都作为文档的不同域单独存储并被索引。Document对象的结构比较简单，为一个包含多个Filed对象容器；Field是指包含能被索引的文本内容的类。

⑤Field 
索引中的每个文档都包含一个或多个不同命名的域，这些域包含在Field类中。每个域都有一个域名和对应的域值，以及一组选项来精确控制Lucene索引操作各个域值。

搜索过程中的核心类

·IndexSearcher 
·Term 
·Query 
·TermQuery 
·TopDocs

这里写图片描述 
①IndexSearcher 
该类用于搜索由IndexWriter类创建的索引，它是连接索引的中心环节。可以将IndexSearcher类看作是一个以只读方式打开索引的类。它需要利用Directory实例来掌控前期创建的索引，然后才能提供大量的搜索方法。 
②Term 
Term对象是搜索功能的基本单元。Term对象包含一对字符串元素：域名和单词（或域名文本值）。 
③Query 
包含了一些非常有用的方法，TermQuery是它的一个子类。 
④TermQuery 
该类提供最基本的查询，用来匹配指定域中包含特定项的文档。 
⑤TopDocs 
该类是一个简单的指针容器，指针一般指向前N个排名的搜索结果，搜索结果即匹配查询条件的文档。

Lucene如何对搜索内容进行建模

①文档和域 
文档是Lucene索引和搜索的原子单位。文档为包含一个或多个域的容器，而域则依次包含“真正的”被搜索内容。每个域都有一个标识名称，该名称为一个文本值或二进制值。如：用户在输入搜索内容“title:lucene”时，搜索结果则为标题域值包含单词“lucene”的所有文档。

Lucene可以针对域进行3种操作： 
·域值可以被索引。如果需要搜索一个域，则必须首先对它进行索引。被索引的域值必须是文本格式的（二进制格式的域值只能被存储而不能被索引）。在索引一个域时，需要首先使用分析过程将域值转换为语汇单元，然后将语汇单元加入到索引中。 
·域被索引后，还可以选择性地存储项向量，后者可以看作该域的一个小型反向索引集合，通过该向量能够检索该域的所有语汇单元。这个机制有助于实现一些高级功能，比如搜索与当前文档相似的文档。 
·域值可以被单独存储，即是说被分析前的域值备份也可以写进索引中，以便后续的检索。这个机制可以使你将原始值展现给用户，比如文档的标题或摘要。

【注意】 
当搜索程序通过索引检索文档时，只有被存储的域才会被作为搜索结果展现。例如，被索引但未被存储于文档的域是不会被作为搜索结果展现的。这种机制通常会使得搜索结果具有不确定性。

②灵活的架构 
与数据库不同的是，Lucene没有一个确定的全局模式；Lucene要求在进行索引操作时简单化或反向规格化原始数据。

理解索引过程

这里写图片描述 
在索引操作期间，文本首先从原始数据中提取出来，并用于创建对应的Document实例，该实例包含多个Field实例，它们都用来保存原始数据信息。随后的分析过程将域文本处理成大量语汇单元。最后将语汇单元加入到段结构中。 
①提取文本和创建文档 
使用Lucene索引数据时，必须先从数据中提取纯文本格式信息，以便Lucene识别该文本并建立对应的Lucene文档。 
②分析文档 
一旦建立其Lucene文档和域，就可以调用IndexWriter对象的addDocument方法将数据传递给Lucene进行索引操作了。在索引操作时，Lucene首先分析文本，将文本数据分割成语汇单元串，然后对它们执行一些可选操作。 
③向索引添加文档 
对输入数据分析完毕后，就可以将分析结果写入索引文件中。Lucene将输入数据以一种倒排索引的数据结构进行存储。在进行关键字快速查找时，这种数据结构能够有效利用磁盘空间。Lucene使用倒排数据结构的原因是：把文档中提取出的语汇单元作为查询关键字，而不是将文档作为中心实体，这种思想很像书籍的索引与页码的对应关系。

–索引段 
Lucene索引都包含一个或多个段。每个段都是一个独立的索引，它包含整个文档索引的一个子集。每当writer刷新缓冲区增加的文档，以及挂起目录删除操作时，索引文件都会建立一个新段。在搜索索引时，每个段都是单独访问的，但搜索结果是合并后返回的。 
每个段文件都包含多个文件，文件格式为_X.，这里X代表段名称，为扩展名，用来标识该文件对应索引的某个部分。如果使用混合文件格式（这是Lucene默认的处理方式，可以通过IndexWriter.setUseCompoundFile方法进行修改），那么上述索引文件都会被压缩成一个单一的文件：_X.cfs。这种方式能在搜索期间减少打开的文件数量。 
还有一个特殊文件，名叫段文件，用段_标识，该文件指向索引激活的段。Lucene会首先打开该文件，然后打开它所指向的其他文件。IndexWriter类会周期性地选择一些段，然后将它们合并到一个新段中，然后删除老的段。被合并段的选取策略由一个独立的MergePolicy类主导。

这里写图片描述

基本索引操作

①向索引添加文档 
添加文档的方法有两个： 
——addDocument(Document)，使用默认分析器添加文档，该分析器在创建IndexWriter对象时指定，用于语汇单元操作 
——addDocument(Document , Analyzer)，使用指定的分析器添加文档和语汇单元操作。

②删除文档 
——deleteDocuments(Term)负责删除包含项的所有文档 
——deleteDocuments(Term[]) 
——deleteDocuments(Query) 
——deleteDocuments(Query[]) 
——deleteAll() 
通过Term类删除单个文档，需要确认在每个文档中都已索引过对应的Field类，还需要确认所有域值都是以唯一的，这样才能将这个文档单独找出来删除。 
注意，如果碰巧指定了一个错误的Term对象（例如，由一个普通的被索引的域文本创建的Term对象，而不是由唯一ID值创建的域），那么Lucene将很容易地快速删除索引中的大量文档。删除操作不会马上执行，而是放入内存缓冲区，Lucene周期性刷新文档目录来执行该操作。不过即使删除操作已完成，存储该文档的磁盘空间也不会马上释放，Lucene只是将该文档标记为“删除”。

③更新文档 
Lucene更新文档通过先删除整个旧文档，然后向索引中添加新文档来完成。 
——updateDocument(Term,Document)首先删除包含Term变量的所有文档，然后使用writer的默认分析器添加新文档。 
——updateDocument(Term,Document，Analyzer)同上，区别是可以指定分析器。由于updateDocument方法要在后台调用deleteDocuments方法，因此注意：要确认被更新文档的Term标识的唯一性。

域选项

①域索引选项（Field.Index.*） 
域索引选项通过倒排索引来控制域文本是否可被搜索。 
——Index.ANALYZED,使用分析器将域值分解成独立的语汇单元流，并使每个语汇单元能被搜索。该选项适用于普通文本域（如正文、标题、摘要等） 
——Index.NOT_ANALYZED，对域进行索引，但不对String值进行分析。该操作实际上将域值作为单一语汇单元并使之能被搜索。该选项适用于索引那些不能被分解的域值，如URL、文件路径、日期、人名等。 
——Index.ANALYZED_NO_NORMS,这是Index.ANALYZED的变体，它不会在索引中存储norms信息。norms记录了索引中的index-time boost信息，但是当你进行搜索时可能会比较耗费内存。 
——Index.NOT_ANALYZED_NO_NORMS。该选项常用语在搜索期间节省索引空间和减少内存耗费，因为single-token域并不需要norms信息，除非它们已被进行加权操作。 
——Index.NO，使对应的域值不被搜索。 
当Lucene建立起倒排索引后，默认情况下它会保存所有必要信息以实施VectorSpace Model。该Model需要计算文档中出现的term数，以及它们出现的位置。但有时候这些域只是在布尔搜索时用到，它们并不为相关评分做贡献，一个常见的例子是，域只是被用作过滤，如权限过滤和日期过滤。这种情况下，可以通过调用Field.setOmitTermFreqAndPositions(true)方法让Lucene跳过该项的出现频率和出现位置的索引。该方法可以节省一些索引在磁盘上的存储空间，还可以加速搜索和过滤过程，但会阻止需要位置信息的搜索，如阻止PhraseQuery和SpanQuery类的运行。

②域存储选项（Field.Store.*） 
用来确定是否需要存储域的真实值，以便后续搜索时能恢复这个值： 
——Store.YES，指定存储域值。原始的字符串全部被保存在索引中，并可以由IndexReader类恢复。该选项对于需要展示搜索结果的一些域很有用（如URL、标题或数据库主键）。如果索引的大小在搜索程序考虑之列的化，不要存储太大的文本域值，因为这些域值会消耗掉索引的存储空间。 
——Store.NO，指定不存储域值 
Lucene包含一个工具类CompressionTools，提供静态方法压缩和解压字节数组。可以在存储域值之前对它进行压缩。注意，尽管该方法可以为索引节省一些空间，但节省的幅度跟域值的可被压缩程度有关，而且该方法会降低索引和搜索速度。这样其实就是通过消耗更多的CPU计算能力来换取更多的磁盘空间。如果域值所占空间很小，建议少使用压缩。

③域的项向量选项 
它是介于索引域和存储域的一个中间结构。

④Reader、TokenStream和byte[]域值 
Field对象还有其他几个初始化方法，允许传入除String以外的其他参数： 
·Field (String name, Reader value, TermVector termVector）方法使用 Reader而不是String对象来表示域值。在这种情况下，域值是不能被存储的。（域存储选项被硬编码成 Store. NO ）， 并且该域会一直用于分析和索引(Index.ANALYZED ）． 如果在内存中保存 String 代价较高或者不太方便时，如存储的域值较大时，使用这个初始化方法则比较有效． 
·Field(String name,Reader value）， 与前述方法类似， 使用 Reader 而不是 String 对象来表示域值，但使用该方法时，默认的 termVector 为 TermVector.NO 
• Field (String name, TokenStream tokenStream, termVector termVector )允许程序对域值进行预分析并生成 TokenStream 对象．此外，这个域不会被存储并将一直用于分析和索引． 
• Field(String name,TokenStream tokenStream）， 与前一个方法类似，允许程序对域值进行预分析并生成 TokenStream 对象，但使用该方法时默认的termVector 为 TermVector.NO. 
• Field (String name, byte [] value, Store store）方法可以用来存储二进制域， 如用不参与索引的域（ Index.NO ）和没有项向量的域（ TermVector. NO ）. 其中 store 参数必须设置为 Store.YES. 
• Field (String name, byte[] value, int offset, int length, Stroe store) 与前一个方法类似，能够对二进制域进行索引，区别在于该方法允许你对这个二进制的部分片段进行引用，该片段的起始位置可以用 offset 参数表示，处理长度可以用参数 length 对应的字节数来表示． 
Field 类是一个非常复杂的类， 它提供了大量的初始化选项，以向 Lucene 传达精确的域值处理指令。
⑤域选项组合
1.2 lucene能做什么 
要回答这个问题，先要了解lucene的本质。实际上lucene的功能很单一，说到底，就是你给它若干个字符串，然后它为你提供一个全文搜索服务，告诉你你要搜索的关键词出现在哪里。知道了这个本质，你就可以发挥想象做任何符合这个条件的事情了。你可以把站内新闻都索引了，做个资料库；你可以把一个数据库表的若干个字段索引起来，那就不用再担心因为“%like%”而锁表了；你也可以写个自己的搜索引擎……

1.3 你该不该选择lucene 
下面给出一些测试数据，如果你觉得可以接受，那么可以选择。 
测试一：250万记录，300M左右文本，生成索引380M左右，800线程下平均处理时间300ms。 
测试二：37000记录，索引数据库中的两个varchar字段，索引文件2.6M，800线程下平均处理时间1.5ms。

2 lucene的工作方式 
lucene提供的服务实际包含两部分：一入一出。所谓入是写入，即将你提供的源（本质是字符串）写入索引或者将其从索引中删除；所谓出是读出，即向用户提供全文搜索服务，让用户可以通过关键词定位源。

2.1写入流程 
源字符串首先经过analyzer处理，包括：分词，分成一个个单词；去除stopword（可选）。 
将源中需要的信息加入Document的各个Field中，并把需要索引的Field索引起来，把需要存储的Field存储起来。 
将索引写入存储器，存储器可以是内存或磁盘。

2.2读出流程 
用户提供搜索关键词，经过analyzer处理。 
对处理后的关键词搜索索引找出对应的Document。 
用户根据需要从找到的Document中提取需要的Field。

3 一些需要知道的概念 
lucene用到一些概念，了解它们的含义，有利于下面的讲解。

3.1 analyzer 
Analyzer是分析器，它的作用是把一个字符串按某种规则划分成一个个词语，并去除其中的无效词语，这里说的无效词语是指英文中的“of”、 “the”，中文中的“的”、“地”等词语，这些词语在文章中大量出现，但是本身不包含什么关键信息，去掉有利于缩小索引文件、提高效率、提高命中率。 
分词的规则千变万化，但目的只有一个：按语义划分。这点在英文中比较容易实现，因为英文本身就是以单词为单位的，已经用空格分开；而中文则必须以某种方法将连成一片的句子划分成一个个词语。具体划分方法下面再详细介绍，这里只需了解分析器的概念即可。

3.2 document 
用户提供的源是一条条记录，它们可以是文本文件、字符串或者数据库表的一条记录等等。一条记录经过索引之后，就是以一个Document的形式存储在索引文件中的。用户进行搜索，也是以Document列表的形式返回。

3.3 field 
一个Document可以包含多个信息域，例如一篇文章可以包含“标题”、“正文”、“最后修改时间”等信息域，这些信息域就是通过Field在Document中存储的。 
Field有两个属性可选：存储和索引。通过存储属性你可以控制是否对这个Field进行存储；通过索引属性你可以控制是否对该Field进行索引。这看起来似乎有些废话，事实上对这两个属性的正确组合很重要，下面举例说明： 
还是以刚才的文章为例子，我们需要对标题和正文进行全文搜索，所以我们要把索引属性设置为真，同时我们希望能直接从搜索结果中提取文章标题，所以我们把标题域的存储属性设置为真，但是由于正文域太大了，我们为了缩小索引文件大小，将正文域的存储属性设置为假，当需要时再直接读取文件；我们只是希望能从搜索解果中提取最后修改时间，不需要对它进行搜索，所以我们把最后修改时间域的存储属性设置为真，索引属性设置为假。上面的三个域涵盖了两个属性的三种组合，还有一种全为假的没有用到，事实上Field不允许你那么设置，因为既不存储又不索引的域是没有意义的。

3.4 term 
term是搜索的最小单位，它表示文档的一个词语，term由两部分组成：它表示的词语和这个词语所出现的field。

3.5 tocken 
tocken是term的一次出现，它包含trem文本和相应的起止偏移，以及一个类型字符串。一句话中可以出现多次相同的词语，它们都用同一个term表示，但是用不同的tocken，每个tocken标记该词语出现的地方。

3.6 segment 
添加索引时并不是每个document都马上添加到同一个索引文件，它们首先被写入到不同的小文件，然后再合并成一个大索引文件，这里每个小文件都是一个segment。

4 lucene的结构 
lucene包括core和sandbox两部分，其中core是lucene稳定的核心部分，sandbox包含了一些附加功能，例如highlighter、各种分析器。 
Lucene core有七个包：analysis，document，index，queryParser，search，store，util。 
4.1 analysis 
Analysis包含一些内建的分析器，例如按空白字符分词的WhitespaceAnalyzer，添加了stopwrod过滤的StopAnalyzer，最常用的StandardAnalyzer。 
4.2 document 
Document包含文档的数据结构，例如Document类定义了存储文档的数据结构，Field类定义了Document的一个域。 
4.3 index 
Index包含了索引的读写类，例如对索引文件的segment进行写、合并、优化的IndexWriter类和对索引进行读取和删除操作的 IndexReader类，这里要注意的是不要被IndexReader这个名字误导，以为它是索引文件的读取类，实际上删除索引也是由它完成， IndexWriter只关心如何将索引写入一个个segment，并将它们合并优化；IndexReader则关注索引文件中各个文档的组织形式。 
4.4 queryParser 
QueryParser包含了解析查询语句的类，lucene的查询语句和sql语句有点类似，有各种保留字，按照一定的语法可以组成各种查询。 Lucene有很多种Query类，它们都继承自Query，执行各种特殊的查询，QueryParser的作用就是解析查询语句，按顺序调用各种 Query类查找出结果。 
4.5 search 
Search包含了从索引中搜索结果的各种类，例如刚才说的各种Query类，包括TermQuery、BooleanQuery等就在这个包里。 
4.6 store 
Store包含了索引的存储类，例如Directory定义了索引文件的存储结构，FSDirectory为存储在文件中的索引，RAMDirectory为存储在内存中的索引，MmapDirectory为使用内存映射的索引。 
4.7 util 
Util包含一些公共工具类，例如时间和字符串之间的转换工具。 
5 如何建索引 
5.1 最简单的能完成索引的代码片断

IndexWriter writer = new IndexWriter(“/data/index/”, new StandardAnalyzer(), true);
Document doc = new Document();
doc.add(new Field("title", "lucene introduction", Field.Store.YES, Field.Index.TOKENIZED));
doc.add(new Field("content", "lucene works well", Field.Store.YES, Field.Index.TOKENIZED));
writer.addDocument(doc);
writer.optimize();
writer.close();
下面我们分析一下这段代码。 
首先我们创建了一个writer，并指定存放索引的目录为“/data/index”，使用的分析器为StandardAnalyzer，第三个参数说明如果已经有索引文件在索引目录下，我们将覆盖它们。 
然后我们新建一个document。 
我们向document添加一个field，名字是“title”，内容是“lucene introduction”，对它进行存储并索引。 
再添加一个名字是“content”的field，内容是“lucene works well”，也是存储并索引。 
然后我们将这个文档添加到索引中，如果有多个文档，可以重复上面的操作，创建document并添加。 
添加完所有document，我们对索引进行优化，优化主要是将多个segment合并到一个，有利于提高索引速度。 
随后将writer关闭，这点很重要。

对，创建索引就这么简单！ 
当然你可能修改上面的代码获得更具个性化的服务。

5.2 将索引直接写在内存 
你需要首先创建一个RAMDirectory，并将其传给writer，代码如下：

Directory dir = new RAMDirectory();
IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true);
Document doc = new Document();
doc.add(new Field("title", "lucene introduction", Field.Store.YES, Field.Index.TOKENIZED));
doc.add(new Field("content", "lucene works well", Field.Store.YES, Field.Index.TOKENIZED));
writer.addDocument(doc);
writer.optimize();
writer.close();
5.3 索引文本文件 
如果你想把纯文本文件索引起来，而不想自己将它们读入字符串创建field，你可以用下面的代码创建field：

Field field = new Field(“content”, new FileReader(file));

这里的file就是该文本文件。该构造函数实际上是读去文件内容，并对其进行索引，但不存储。

6 如何维护索引 
索引的维护操作都是由IndexReader类提供。

6.1 如何删除索引 
lucene提供了两种从索引中删除document的方法，一种是

void deleteDocument(int docNum)
这种方法是根据document在索引中的编号来删除，每个document加进索引后都会有个唯一编号，所以根据编号删除是一种精确删除，但是这个编号是索引的内部结构，一般我们不会知道某个文件的编号到底是几，所以用处不大。另一种是

void deleteDocuments(Term term)
这种方法实际上是首先根据参数term执行一个搜索操作，然后把搜索到的结果批量删除了。我们可以通过这个方法提供一个严格的查询条件，达到删除指定document的目的。 
下面给出一个例子：

Directory dir = FSDirectory.getDirectory(PATH, false);
IndexReader reader = IndexReader.open(dir);
Term term = new Term(field, key);
reader.deleteDocuments(term);
reader.close();
6.2 如何更新索引 
lucene并没有提供专门的索引更新方法，我们需要先将相应的document删除，然后再将新的document加入索引。例如：

Directory dir = FSDirectory.getDirectory(PATH, false);
IndexReader reader = IndexReader.open(dir);
Term term = new Term(“title”, “lucene introduction”);
reader.deleteDocuments(term);
reader.close();

IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true);
Document doc = new Document();
doc.add(new Field("title", "lucene introduction", Field.Store.YES, Field.Index.TOKENIZED));
doc.add(new Field("content", "lucene is funny", Field.Store.YES, Field.Index.TOKENIZED));
writer.addDocument(doc);
writer.optimize();
writer.close();
7 如何搜索 
lucene的搜索相当强大，它提供了很多辅助查询类，每个类都继承自Query类，各自完成一种特殊的查询，你可以像搭积木一样将它们任意组合使用，完成一些复杂操作；另外lucene还提供了Sort类对结果进行排序，提供了Filter类对查询条件进行限制。你或许会不自觉地拿它跟SQL语句进行比较：“lucene能执行and、or、order by、where、like ‘%xx%’操作吗？”回答是：“当然没问题！”

7.1 各种各样的Query 
下面我们看看lucene到底允许我们进行哪些查询操作：

7.1.1 TermQuery 
首先介绍最基本的查询，如果你想执行一个这样的查询：“在content域中包含‘lucene’的document”，那么你可以用TermQuery：

Term t = new Term("content", " lucene";
Query query = new TermQuery(t);
7.1.2 BooleanQuery 
如果你想这么查询：“在content域中包含java或perl的document”，那么你可以建立两个TermQuery并把它们用BooleanQuery连接起来：

TermQuery termQuery1 = new TermQuery(new Term("content", "java");
TermQuery termQuery 2 = new TermQuery(new Term("content", "perl");
BooleanQuery booleanQuery = new BooleanQuery();
booleanQuery.add(termQuery 1, BooleanClause.Occur.SHOULD);
booleanQuery.add(termQuery 2, BooleanClause.Occur.SHOULD);
7.1.3 WildcardQuery 
如果你想对某单词进行通配符查询，你可以用WildcardQuery，通配符包括’?’匹配一个任意字符和’’匹配零个或多个任意字符，例如你搜索’use’，你可能找到’useful’或者’useless’：

Query query = new WildcardQuery(new Term("content", "use*");
1
7.1.4 PhraseQuery 
你可能对中日关系比较感兴趣，想查找‘中’和‘日’挨得比较近（5个字的距离内）的文章，超过这个距离的不予考虑，你可以：

PhraseQuery query = new PhraseQuery();
query.setSlop(5);
query.add(new Term("content ", “中”));
query.add(new Term(“content”, “日”));
那么它可能搜到“中日合作……”、“中方和日方……”，但是搜不到“中国某高层领导说日本欠扁”。

7.1.5 PrefixQuery 
如果你想搜以‘中’开头的词语，你可以用PrefixQuery：

PrefixQuery query = new PrefixQuery(new Term("content ", "中");
1
7.1.6 FuzzyQuery 
FuzzyQuery用来搜索相似的term，使用Levenshtein算法。假设你想搜索跟‘wuzza’相似的词语，你可以：

Query query = new FuzzyQuery(new Term("content", "wuzza");
1
你可能得到‘fuzzy’和‘wuzzy’。

7.1.7 RangeQuery 
另一个常用的Query是RangeQuery，你也许想搜索时间域从20060101到20060130之间的document，你可以用RangeQuery：

RangeQuery query = new RangeQuery(new Term(“time”, “20060101”), new Term(“time”, “20060130”), true);
1
最后的true表示用闭合区间。

7.2 QueryParser 
看了这么多Query，你可能会问：“不会让我自己组合各种Query吧，太麻烦了！”当然不会，lucene提供了一种类似于SQL语句的查询语句，我们姑且叫它lucene语句，通过它，你可以把各种查询一句话搞定，lucene会自动把它们查分成小块交给相应Query执行。下面我们对应每种Query演示一下：

TermQuery可以用“field:key”方式，例如“content:lucene”。
BooleanQuery中‘与’用‘+’，‘或’用‘ ’，例如“content:java contenterl”。
WildcardQuery仍然用‘?’和‘*’，例如“content:use*”。
PhraseQuery用‘~’，例如“content:"中日"~5”。
PrefixQuery用‘*’，例如“中*”。
FuzzyQuery用‘~’，例如“content: wuzza ~”。
RangeQuery用‘[]’或‘{}’，前者表示闭区间，后者表示开区间，例如“time:[20060101 TO 20060130]”，注意TO区分大小写。
你可以任意组合query string，完成复杂操作，例如“标题或正文包括lucene，并且时间在20060101到20060130之间的文章”可以表示为：“+ (title:lucene content:lucene) +time:[20060101 TO 20060130]”。代码如下：

Directory dir = FSDirectory.getDirectory(PATH, false);
IndexSearcher is = new IndexSearcher(dir);
QueryParser parser = new QueryParser("content", new StandardAnalyzer());
Query query = parser.parse("+(title:lucene content:lucene) +time:[20060101 TO 20060130]";
Hits hits = is.search(query);
for (int i = 0; i < hits.length(); i++)
{
Document doc = hits.doc(i);
System.out.println(doc.get("title");
}
is.close();
首先我们创建一个在指定文件目录上的IndexSearcher。 
然后创建一个使用StandardAnalyzer作为分析器的QueryParser，它默认搜索的域是content。 
接着我们用QueryParser来parse查询字串，生成一个Query。 
然后利用这个Query去查找结果，结果以Hits的形式返回。 
这个Hits对象包含一个列表，我们挨个把它的内容显示出来。

7.3 Filter 
filter的作用就是限制只查询索引的某个子集，它的作用有点像SQL语句里的where，但又有区别，它不是正规查询的一部分，只是对数据源进行预处理，然后交给查询语句。注意它执行的是预处理，而不是对查询结果进行过滤，所以使用filter的代价是很大的，它可能会使一次查询耗时提高一百倍。 
最常用的filter是RangeFilter和QueryFilter。RangeFilter是设定只搜索指定范围内的索引；QueryFilter是在上次查询的结果中搜索。 
Filter的使用非常简单，你只需创建一个filter实例，然后把它传给searcher。继续上面的例子，查询“时间在20060101到20060130之间的文章”除了将限制写在query string中，你还可以写在RangeFilter中：

Directory dir = FSDirectory.getDirectory(PATH, false);
IndexSearcher is = new IndexSearcher(dir);
QueryParser parser = new QueryParser("content", new StandardAnalyzer());
Query query = parser.parse("title:lucene content:lucene";
RangeFilter filter = new RangeFilter("time", "20060101", "20060230", true, true);
Hits hits = is.search(query, filter);
for (int i i < hits.length(); i++)
{
Document doc = hits.doc(i);
System.out.println(doc.get("title");
}
is.close();
7.4 Sort 
有时你想要一个排好序的结果集，就像SQL语句的“order by”，lucene能做到：通过Sort。 
Sort sort Sort(“time”); //相当于SQL的“order by time” 
Sort sort = new Sort(“time”, true); // 相当于SQL的“order by time desc” 
下面是一个完整的例子：

Directory dir = FSDirectory.getDirectory(PATH, false);
IndexSearcher is = new IndexSearcher(dir);
QueryParser parser = new QueryParser("content", new StandardAnalyzer());
Query query = parser.parse("title:lucene content:lucene";
RangeFilter filter = new RangeFilter("time", "20060101", "20060230", true, true);
Sort sort = new Sort(“time”);
Hits hits = is.search(query, filter, sort);
for (int i = 0; i < hits.length(); i++)
{
Document doc = hits.doc(i);
System.out.println(doc.get("title");
}
is.close();
8 分析器 
在前面的概念介绍中我们已经知道了分析器的作用，就是把句子按照语义切分成一个个词语。英文切分已经有了很成熟的分析器： StandardAnalyzer，很多情况下StandardAnalyzer是个不错的选择。甚至你会发现StandardAnalyzer也能对中文进行分词。 
但是我们的焦点是中文分词，StandardAnalyzer能支持中文分词吗？实践证明是可以的，但是效果并不好，搜索“如果”会把“牛奶不如果汁好喝”也搜索出来，而且索引文件很大。那么我们手头上还有什么分析器可以使用呢？core里面没有，我们可以在sandbox里面找到两个： ChineseAnalyzer和CJKAnalyzer。但是它们同样都有分词不准的问题。相比之下用StandardAnalyzer和 ChineseAnalyzer建立索引时间差不多，索引文件大小也差不多，CJKAnalyzer表现会差些，索引文件大且耗时比较长。 
要解决问题，首先分析一下这三个分析器的分词方式。StandardAnalyzer和ChineseAnalyzer都是把句子按单个字切分，也就是说 “牛奶不如果汁好喝”会被它们切分成“牛 奶 不 如 果 汁 好 喝”；而CJKAnalyzer则会切分成“牛奶 奶不 不如 如果 果汁 汁好好喝”。这也就解释了为什么搜索“果汁”都能匹配这个句子。 
以上分词的缺点至少有两个：匹配不准确和索引文件大。我们的目标是将上面的句子分解成“牛奶 不如 果汁好喝”。这里的关键就是语义识别，我们如何识别“牛奶”是一个词而“奶不”不是词语？我们很自然会想到基于词库的分词法，也就是我们先得到一个词库，里面列举了大部分词语，我们把句子按某种方式切分，当得到的词语与词库中的项匹配时，我们就认为这种切分是正确的。这样切词的过程就转变成匹配的过程，而匹配的方式最简单的有正向最大匹配和逆向最大匹配两种，说白了就是一个从句子开头向后进行匹配，一个从句子末尾向前进行匹配。基于词库的分词词库非常重要，词库的容量直接影响搜索结果，在相同词库的前提下，据说逆向最大匹配优于正向最大匹配。 
当然还有别的分词方法，这本身就是一个学科，我这里也没有深入研究。回到具体应用，我们的目标是能找到成熟的、现成的分词工具，避免重新发明车轮。经过网上搜索，用的比较多的是中科院的ICTCLAS和一个不开放源码但是免费的JE-Analysis。ICTCLAS有个问题是它是一个动态链接库， java调用需要本地方法调用，不方便也有安全隐患，而且口碑也确实不大好。JE-Analysis效果还不错，当然也会有分词不准的地方，相比比较方便放心。= new = 0; 
9 性能优化 
一直到这里，我们还是在讨论怎么样使lucene跑起来，完成指定任务。利用前面说的也确实能完成大部分功能。但是测试表明lucene的性能并不是很好，在大数据量大并发的条件下甚至会有半分钟返回的情况。另外大数据量的数据初始化建立索引也是一个十分耗时的过程。那么如何提高lucene的性能呢？下面从优化创建索引性能和优化搜索性能两方面介绍。

9.1 优化创建索引性能 
这方面的优化途径比较有限，IndexWriter提供了一些接口可以控制建立索引的操作，另外我们可以先将索引写入RAMDirectory，再批量写入FSDirectory，不管怎样，目的都是尽量少的文件IO，因为创建索引的最大瓶颈在于磁盘IO。另外选择一个较好的分析器也能提高一些性能。

9.1.1 通过设置IndexWriter的参数优化索引建立 
setMaxBufferedDocs(int maxBufferedDocs) 
控制写入一个新的segment前内存中保存的document的数目，设置较大的数目可以加快建索引速度，默认为10。 
setMaxMergeDocs(int maxMergeDocs) 
控制一个segment中可以保存的最大document数目，值较小有利于追加索引的速度，默认Integer.MAX_VALUE，无需修改。 
setMergeFactor(int mergeFactor) 
控制多个segment合并的频率，值较大时建立索引速度较快，默认是10，可以在建立索引时设置为100。

9.1.2 通过RAMDirectory缓写提高性能 
我们可以先把索引写入RAMDirectory，达到一定数量时再批量写进FSDirectory，减少磁盘IO次数。

FSDirectory fsDir = FSDirectory.getDirectory("/data/index", true);
RAMDirectory ramDir = new RAMDirectory();
IndexWriter fsWriter = new IndexWriter(fsDir, new StandardAnalyzer(), true);
IndexWriter ramWriter = new IndexWriter(ramDir, new StandardAnalyzer(), true);
while (there are documents to index)
{
... create Document ...
ramWriter.addDocument(doc);
if (condition for flushing memory to disk has been met)
{
fsWriter.addIndexes(new Directory[] { ramDir });
ramWriter.close();
ramWriter = new IndexWriter(ramDir, new StandardAnalyzer(), true);
}
}
9.1.3 选择较好的分析器 
这个优化主要是对磁盘空间的优化，可以将索引文件减小将近一半，相同测试数据下由600M减少到380M。但是对时间并没有什么帮助，甚至会需要更长时间，因为较好的分析器需要匹配词库，会消耗更多cpu，测试数据用StandardAnalyzer耗时133分钟；用MMAnalyzer耗时150分钟。

9.2 优化搜索性能 
虽然建立索引的操作非常耗时，但是那毕竟只在最初创建时才需要，平时只是少量的维护操作，更何况这些可以放到一个后台进程处理，并不影响用户搜索。我们创建索引的目的就是给用户搜索，所以搜索的性能才是我们最关心的。下面就来探讨一下如何提高搜索性能。

9.2.1 将索引放入内存 
这是一个最直观的想法，因为内存比磁盘快很多。Lucene提供了RAMDirectory可以在内存中容纳索引：

Directory fsDir = FSDirectory.getDirectory(“/data/index/”, false);
Directory ramDir = new RAMDirectory(fsDir);
Searcher searcher = new IndexSearcher(ramDir);
1
2
3
但是实践证明RAMDirectory和FSDirectory速度差不多，当数据量很小时两者都非常快，当数据量较大时（索引文件400M）RAMDirectory甚至比FSDirectory还要慢一点，这确实让人出乎意料。 
而且lucene的搜索非常耗内存，即使将400M的索引文件载入内存，在运行一段时间后都会out of memory，所以个人认为载入内存的作用并不大。

9.2.2 优化时间范围限制 
既然载入内存并不能提高效率，一定有其它瓶颈，经过测试发现最大的瓶颈居然是时间范围限制，那么我们可以怎样使时间范围限制的代价最小呢？ 
当需要搜索指定时间范围内的结果时，可以： 
1、用RangeQuery，设置范围，但是RangeQuery的实现实际上是将时间范围内的时间点展开，组成一个个BooleanClause加入到 BooleanQuery中查询，因此时间范围不可能设置太大，经测试，范围超过一个月就会抛BooleanQuery.TooManyClauses，可以通过设置 BooleanQuery.setMaxClauseCount(int maxClauseCount)扩大，但是扩大也是有限的，并且随着maxClauseCount扩大，占用内存也扩大 
2、用RangeFilter代替RangeQuery，经测试速度不会比RangeQuery慢，但是仍然有性能瓶颈，查询的90%以上时间耗费在 RangeFilter，研究其源码发现RangeFilter实际上是首先遍历所有索引，生成一个BitSet，标记每个document，在时间范围内的标记为true，不在的标记为false，然后将结果传递给Searcher查找，这是十分耗时的。 
3、进一步提高性能，这个又有两个思路： 
a、缓存Filter结果。既然RangeFilter的执行是在搜索之前，那么它的输入都是一定的，就是IndexReader，而 IndexReader是由Directory决定的，所以可以认为RangeFilter的结果是由范围的上下限决定的，也就是由具体的 RangeFilter对象决定，所以我们只要以RangeFilter对象为键，将filter结果BitSet缓存起来即可。lucene API已经提供了一个CachingWrapperFilter类封装了Filter及其结果，所以具体实施起来我们可以cache CachingWrapperFilter对象，需要注意的是，不要被CachingWrapperFilter的名字及其说明误导， CachingWrapperFilter看起来是有缓存功能，但的缓存是针对同一个filter的，也就是在你用同一个filter过滤不同 IndexReader时，它可以帮你缓存不同IndexReader的结果，而我们的需求恰恰相反，我们是用不同filter过滤同一个 IndexReader，所以只能把它作为一个封装类。 
b、降低时间精度。研究Filter的工作原理可以看出，它每次工作都是遍历整个索引的，所以时间粒度越大，对比越快，搜索时间越短，在不影响功能的情况下，时间精度越低越好，有时甚至牺牲一点精度也值得，当然最好的情况是根本不作时间限制。 
下面针对上面的两个思路演示一下优化结果（都采用800线程随机关键词随即时间范围）： 
第一组，时间精度为秒： 
方式 直接用RangeFilter 使用cache 不用filter 
平均每个线程耗时 10s 1s 300ms

第二组，时间精度为天 
方式 直接用RangeFilter 使用cache 不用filter 
平均每个线程耗时 900ms 360ms 300ms

由以上数据可以得出结论： 
1、 尽量降低时间精度，将精度由秒换成天带来的性能提高甚至比使用cache还好，最好不使用filter。 
2、 在不能降低时间精度的情况下，使用cache能带了10倍左右的性能提高。

9.2.3 使用更好的分析器 
这个跟创建索引优化道理差不多，索引文件小了搜索自然会加快。当然这个提高也是有限的。较好的分析器相对于最差的分析器对性能的提升在20%以下。

10 一些经验

10.1关键词区分大小写 
or AND TO等关键词是区分大小写的，lucene只认大写的，小写的当做普通单词。

10.2 读写互斥性 
同一时刻只能有一个对索引的写操作，在写的同时可以进行搜索

10.3 文件锁 
在写索引的过程中强行退出将在tmp目录留下一个lock文件，使以后的写操作无法进行，可以将其手工删除

10.4 时间格式 
lucene只支持一种时间格式yyMMddHHmmss，所以你传一个yy-MM-dd HH:mm:ss的时间给lucene它是不会当作时间来处理的

10.5 设置boost 
有些时候在搜索时某个字段的权重需要大一些，例如你可能认为标题中出现关键词的文章比正文中出现关键词的文章更有价值，你可以把标题的boost设置的更大，那么搜索结果会优先显示标题中出现关键词的文章（没有使用排序的前题下）。使用方法： 
Field. setBoost(float boost);默认值是1.0，也就是说要增加权重的需要设置得比1大。
