---
title: robots
layout: post
category: jekyll
author: 夏泽民
---
robots.txt是用来告诉搜索引擎网站上哪些内容可以被访问、哪些不能被访问。当搜索引擎访问一个网站的时候，它首先会检查网站是否存在robots.txt，如果有则会根据文件命令访问有权限的文件。

为什么要写robots.txt，主要有四点：

1、保护网站安全

2、节省流量

3、禁止搜索引擎收录部分页面

4、引导蜘蛛爬网站地图
<!-- more -->

#robots.txt的写法与步骤

1、定义搜索引擎

用User-agent：来定义搜索引擎，其中*表示所有，Baiduspider表示百度蜘蛛，Googlebot表示谷歌蜘蛛。

也就是说User-agent：*表示定义所有蜘蛛，User-agent：Baiduspider表示定义百度蜘蛛。

2、禁止与允许访问

Disallow: /表示禁止访问，Allow: /表示允许访问。

在写robots.txt时需特别注意的是，/前面有一个英文状态下的空格（必须是英文状态下的空格）。

3、禁止搜索引擎访问网站中的某几个文件夹，以a、b、c为例，写法分别如下：

Disallow: /a/

Disallow: /b/

Disallow: /c/

3、禁止搜索引擎访问文件夹中的某一类文件，以a文件夹中的js文件为例，写法如下：

Disallow: /a/*.js

4、只允许某个搜索引擎访问，以Baiduspider为例，写法如下：

User-agent: Baiduspider

Disallow:

5、禁止访问网站中的动态页面

User-agent: *

Disallow: /*?*

6、只允许搜索引擎访问某类文件，以htm为例，写法如下：

User-agent: *

Allow: .htm$

Disallow: /

7、禁止某个搜索引擎抓取网站上的所有图片，以Baiduspider为例，写法如下：

User-agent: F

Disallow: .jpg$

Disallow: .jpeg$

Disallow: .gif$

Disallow: .png$

Disallow: .bmp$

三、robots.txt文件存放位置

robots.txt文件存放在网站根目录下，并且文件名所有字母都必须小写。

四、特别注意事项

在写robots.txt文件时语法一定要用对，User-agent、Disallow、Allow、Sitemap这些词都必须是第一个字母大写，后面的字母小写，而且在:后面必须带一个英文字符下的空格。

网站上线之前切记写robots.txt文件禁止蜘蛛访问网站，如果不会写就先了解清楚写法之后再写，以免给网站收录带来不必要的麻烦。

robots.txt文件生效时间在几天至一个月之间，站长自身无法控制。但是，站长可以在百度统计中查看网站robots.txt文件是否生效。

#一键部署的shell脚本例子：

{% highlight bash linenos %}#!/bin/bash  
  #网站根目录定义  
root_dir=("/var/www/")  
  #构建爬虫规则  
for dir in ${root_dir[*]}  
do  
    #删除过期的robots.txt文件  
    if [ -f $dir/robots.txt ]; then  
        rm -r $dir/robots.txt  
    fi  
  #增加新的爬虫规则  
    echo "User-agent: *" >$dir/robots.txt  
    echo "Disallow: /" >>$dir/robots.txt  
  #修改权限  
    chown www-data.www-data $dir/robots.txt  
done  
{% endhighlight %}

#在线生成工具：
http://tool.chinaz.com/robots/
